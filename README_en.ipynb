{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0861775c",
   "metadata": {},
   "source": [
    "<div align='center'>\n",
    "    <img src=\"./images/head.jpg\" alt=\"alt text\" width=\"100%\">\n",
    "    <h1>Happy-LLM</h1>\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://img.shields.io/github/stars/datawhalechina/happy-llm?style=flat&logo=github\" alt=\"GitHub stars\"/>\n",
    "  <img src=\"https://img.shields.io/github/forks/datawhalechina/happy-llm?style=flat&logo=github\" alt=\"GitHub forks\"/>\n",
    "  <img src=\"https://img.shields.io/badge/language-Chinese-brightgreen?style=flat\" alt=\"Language\"/>\n",
    "  <a href=\"https://github.com/datawhalechina/happy-llm\"><img src=\"https://img.shields.io/badge/GitHub-Project-blue?style=flat&logo=github\" alt=\"GitHub Project\"></a>\n",
    "  <a href=\"https://swanlab.cn/@kmno4/Happy-LLM/overview\"><img src=\"https://raw.githubusercontent.com/SwanHubX/assets/main/badge1.svg\" alt=\"SwanLab\"></a>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "\n",
    "[‰∏≠Êñá](./README.ipynb) | [English](./README_en.ipynb)\n",
    "\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <p><a href=\"https://datawhalechina.github.io/happy-llm/\">üìö Online Reading</a></p>\n",
    "  <h3>üìö A Comprehensive Tutorial on Large Language Model Principles and Practice from Scratch</h3>\n",
    "  <p><em>Deep understanding of LLM core principles, hands-on implementation of your first large model</em></p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86e5e0",
   "metadata": {},
   "source": [
    "## üéØ Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babcd10",
   "metadata": {},
   "source": [
    "\n",
    "> &emsp;&emsp;*Many friends felt unsatisfied after reading the Datawhale open-source project: [self-llm Open Source Large Model Usage Guide](https://github.com/datawhalechina/self-llm), wanting to deeply understand the principles and training processes of large language models. Therefore, we (Datawhale) decided to launch the \"Happy-LLM\" project, aiming to help everyone deeply understand the principles and training processes of large language models.*\n",
    "\n",
    "&emsp;&emsp;This project is a **systematic LLM learning tutorial** that starts from basic NLP research methods and gradually deepens according to LLM concepts and principles, systematically analyzing the architectural foundations and training processes of LLMs for readers. At the same time, we will combine the most mainstream code frameworks in the current LLM field to demonstrate how to build and train an LLM from scratch, aiming to achieve \"give a man a fish and you feed him for a day; teach a man to fish and you feed him for a lifetime.\" We hope everyone can start their journey into the vast world of LLMs from this book and explore the endless possibilities of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2cc95",
   "metadata": {},
   "source": [
    "### ‚ú® What Will You Gain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c260f8",
   "metadata": {},
   "source": [
    "\n",
    "- üìö **Datawhale Open Source Free** - Completely free access to all content of this project\n",
    "- üîç **Deep Understanding** of Transformer architecture and attention mechanisms\n",
    "- üìö **Master** the basic principles of pre-trained language models\n",
    "- üß† **Understand** the basic structures of existing large models\n",
    "- üèóÔ∏è **Hands-on Implementation** of a complete LLaMA2 model\n",
    "- ‚öôÔ∏è **Master Training** from pre-training to fine-tuning full pipeline\n",
    "- üöÄ **Practical Applications** of cutting-edge technologies like RAG and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6efb66",
   "metadata": {},
   "source": [
    "## üìñ Content Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc949516",
   "metadata": {},
   "source": [
    "\n",
    "| Chapter | Key Content | Status |\n",
    "| --- | --- | --- |\n",
    "| [Preface](./docs/README.ipynb) | Project origin, background, and reader recommendations | ‚úÖ |\n",
    "| [Chapter 1: NLP Basic Concepts](./docs/chapter1/Á¨¨‰∏ÄÁ´†%20NLPÂü∫Á°ÄÊ¶ÇÂøµ.ipynb) | What is NLP, development history, task classification, text representation evolution | ‚úÖ |\n",
    "| [Chapter 2: Transformer Architecture](./docs/chapter2/Á¨¨‰∫åÁ´†%20TransformerÊû∂ÊûÑ.ipynb) | Attention mechanism, Encoder-Decoder, hands-on Transformer building | ‚úÖ |\n",
    "| [Chapter 3: Pre-trained Language Models](./docs/chapter3/Á¨¨‰∏âÁ´†%20È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°Âûã.ipynb) | Comparison of Encoder-only, Encoder-Decoder, Decoder-Only models | ‚úÖ |\n",
    "| [Chapter 4: Large Language Models](./docs/chapter4/Á¨¨ÂõõÁ´†%20Â§ßËØ≠Ë®ÄÊ®°Âûã.ipynb) | LLM definition, training strategies, emergent ability analysis | ‚úÖ |\n",
    "| [Chapter 5: Building Large Models from Scratch](./docs/chapter5/Á¨¨‰∫îÁ´†%20Âä®ÊâãÊê≠Âª∫Â§ßÊ®°Âûã.ipynb) | Implementing LLaMA2, training Tokenizer, pre-training small LLM | ‚úÖ |\n",
    "| [Chapter 6: Large Model Training Practice](./docs/chapter6/Á¨¨ÂÖ≠Á´†%20Â§ßÊ®°ÂûãËÆ≠ÁªÉÊµÅÁ®ãÂÆûË∑µ.ipynb) | Pre-training, supervised fine-tuning, LoRA/QLoRA efficient fine-tuning | üöß |\n",
    "| [Chapter 7: Large Model Applications](./docs/chapter7/Á¨¨‰∏ÉÁ´†%20Â§ßÊ®°ÂûãÂ∫îÁî®.ipynb) | Model evaluation, RAG retrieval enhancement, Agent intelligent agents | ‚úÖ |\n",
    "| [Extra Chapter LLM Blog](./Extra-Chapter/) | Excellent Learning Notes/Blog on LLMs ÔºåWelcome PR ÔºÅ| üöß |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bd5df",
   "metadata": {},
   "source": [
    "### Extra Chapter LLM Blog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26839a",
   "metadata": {},
   "source": [
    "\n",
    "- [With large models becoming so powerful, what‚Äôs the significance of fine-tuning a 0.6B small model?](./Extra-Chapter/why-fine-tune-small-large-language-models/readme.ipynb) @[‰∏çË¶ÅËë±ÂßúËíú](https://github.com/KMnO4-zx) 2025-7-11\n",
    "\n",
    "- [Details of the Transformer modules](./Extra-Chapter/transformer-architecture/) @[ditingdapeng](https://github.com/ditingdapeng) 2025-7-14\n",
    "\n",
    "- [Detailed Explanation of Text Data Processing](./Extra-Chapter/text-data-processing/readme.ipynb) @[Ëî°ÈãÜÊç∑](https://github.com/xinala-781) 2025-7-14\n",
    "\n",
    "- [Qwen3-\"VL\"‚Äî‚ÄîPath to 'Concatenation Fine-tuning' for Ultra-small Chinese Multimodal Models](./Extra-Chapter/vlm-concatenation-finetune/README.ipynb) @[ShaohonChen](https://github.com/ShaohonChen) 2025-7-30\n",
    "\n",
    "- [S1: Thinking Budget with vLLM](./Extra-Chapter/s1-vllm-thinking-budget/readme.ipynb) @[kmno4-zx](https://github.com/kmno4-zx) 2025-8-03\n",
    "\n",
    "- [CDDRS: Key elements guided Enhancement for RAG-based Retrieval Methods](./Extra-Chapter/CDDRS/readme.ipynb) @[Hongru0306](https://github.com/Hongru0306) 2025-8-21\n",
    "\n",
    "\n",
    "> &emsp;&emsp;*‚ÄÉIf anyone has unique insights, knowledge, or practices related to the Happy-LLM project or LLMs in general, you are welcome to submit a PR to the [Extra Chapter LLM Blog](./Extra-Chapter/). Please adhere to the [PR Guidances](./Extra-Chapter/Readme.ipynb). We will decide whether to merge or supplement the content into the main Happy-LLM text based on the quality and value of the PR.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063ad39",
   "metadata": {},
   "source": [
    "### Model Downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86856e",
   "metadata": {},
   "source": [
    "\n",
    "| Model Name | Download Link |\n",
    "| --- | --- |\n",
    "| Happy-LLM-Chapter5-Base-215M | [ü§ñ ModelScope](https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-base) |\n",
    "| Happy-LLM-Chapter5-SFT-215M | [ü§ñ ModelScope](https://www.modelscope.cn/models/kmno4zx/happy-llm-215M-sft) |\n",
    "\n",
    "> *ModelScope Studio Experience: [ü§ñ Studio](https://www.modelscope.cn/studios/kmno4zx/happy_llm_215M_sft)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce702d7",
   "metadata": {},
   "source": [
    "### PDF Version Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d302ca",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;***This Happy-LLM PDF tutorial is completely open source and free. To prevent various marketing accounts from adding watermarks and selling to LLM beginners, we have pre-added Datawhale open source logo watermarks that do not affect reading in the PDF files. Please understand~***\n",
    "\n",
    "> *Happy-LLM PDF : https://github.com/datawhalechina/happy-llm/releases/tag/PDF*  \n",
    "> *Happy-LLM PDF Domestic Download: https://www.datawhale.cn/learn/summary/179*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5995b44",
   "metadata": {},
   "source": [
    "## üí° How to Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dfac41",
   "metadata": {},
   "source": [
    "\n",
    "&emsp;&emsp;This project is suitable for university students, researchers, and LLM enthusiasts. Before learning this project, it is recommended to have some programming experience, especially familiarity with the Python programming language. It's best to have knowledge of deep learning and understand concepts and terminology in the NLP field to learn this project more easily.\n",
    "\n",
    "&emsp;&emsp;This project is divided into two parts - foundational knowledge and practical applications. Chapters 1-4 are the foundational knowledge section, introducing the basic principles of LLMs from shallow to deep. Chapter 1 briefly introduces basic NLP tasks and development, providing reference for non-NLP field researchers; Chapter 2 introduces the basic architecture of LLMs - Transformer, including principle introduction and code implementation, serving as the most important theoretical foundation for LLMs; Chapter 3 comprehensively introduces classic PLMs, including Encoder-Only, Encoder-Decoder, and Decoder-Only architectures, while also introducing the architectures and concepts of some current mainstream LLMs; Chapter 4 formally enters the LLM section, detailing the characteristics, capabilities, and overall training process of LLMs. Chapters 5-7 are the practical application section, gradually leading everyone into the underlying details of LLMs. Chapter 5 will guide readers to build an LLM from scratch based on PyTorch and implement the full pipeline of pre-training and supervised fine-tuning; Chapter 6 will introduce the current industry-mainstream LLM training framework Transformers, guiding learners to quickly and efficiently implement the LLM training process based on this framework; Chapter 7 will introduce various applications based on LLMs, completing learners' understanding of the LLM system, including LLM evaluation, Retrieval-Augmented Generation (RAG), and the concepts and simple implementation of intelligent agents (Agents). You can selectively read relevant chapters based on personal interests and needs.\n",
    "\n",
    "&emsp;&emsp;During the reading process, it is recommended to combine theory with practice. LLM is a rapidly developing, practice-oriented field. We recommend investing more in hands-on practice, reproducing the various codes provided in this book, while actively participating in LLM-related projects and competitions, truly diving into the wave of LLM development. We encourage you to follow Datawhale and other LLM-related open-source communities. When encountering problems, you can ask questions in the issue section of this project at any time.\n",
    "\n",
    "&emsp;&emsp;Finally, we welcome every reader to join the ranks of LLM developers after learning this project. As a domestic AI open-source community, we hope to fully gather co-creators to enrich this open-source LLM world together and create more comprehensive and distinctive LLM tutorials. Sparks gather into an ocean. We hope to become a bridge between LLMs and the general public, embracing a more magnificent and vast LLM world with the spirit of free and equal open source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525ded8",
   "metadata": {},
   "source": [
    "## ü§ù How to Contribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faec94c",
   "metadata": {},
   "source": [
    "\n",
    "We welcome any form of contribution!\n",
    "\n",
    "- üêõ **Report Bugs** - Please submit an Issue if you find problems\n",
    "- üí° **Feature Suggestions** - Tell us if you have good ideas\n",
    "- üìù **Content Improvement** - Help improve tutorial content\n",
    "- üîß **Code Optimization** - Submit Pull Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e9164",
   "metadata": {},
   "source": [
    "## üôè Acknowledgments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f816767",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2875b744",
   "metadata": {},
   "source": [
    "### Core Contributors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db38f6",
   "metadata": {},
   "source": [
    "- [Song Zhixue - Project Leader](https://github.com/KMnO4-zx) (Datawhale Member - China University of Mining and Technology, Beijing)\n",
    "- [Zou Yuheng - Project Leader](https://github.com/logan-zou) (Datawhale Member - University of International Business and Economics)\n",
    "- [Zhu Xinzhong - Expert Advisor](https://xinzhongzhu.github.io/) (Datawhale Chief Scientist - Professor at Hangzhou Institute for Advanced Study, Zhejiang Normal University)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8c2a1",
   "metadata": {},
   "source": [
    "### Special Thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303193a8",
   "metadata": {},
   "source": [
    "- Thanks to [@Sm1les](https://github.com/Sm1les) for help and support on this project\n",
    "- Thanks to all developers who contributed to this project ‚ù§Ô∏è\n",
    "\n",
    "<div align=center style=\"margin-top: 30px;\">\n",
    "  <a href=\"https://github.com/datawhalechina/happy-llm/graphs/contributors\">\n",
    "    <img src=\"https://contrib.rocks/image?repo=datawhalechina/happy-llm\" />\n",
    "  </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc2eaa",
   "metadata": {},
   "source": [
    "## Star History"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8dfda",
   "metadata": {},
   "source": [
    "\n",
    "<div align='center'>\n",
    "    <img src=\"./images/star-history-2025710.png\" alt=\"Datawhale\" width=\"90%\">\n",
    "</div>\n",
    "\n",
    "<div align=\"center\">\n",
    "  <p>‚≠ê If this project helps you, please give us a Star!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1c8ef",
   "metadata": {},
   "source": [
    "## About Datawhale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4442b8",
   "metadata": {},
   "source": [
    "\n",
    "<div align='center'>\n",
    "    <img src=\"./images/datawhale.png\" alt=\"Datawhale\" width=\"30%\">\n",
    "    <p>Scan the QR code to follow Datawhale WeChat Official Account for more quality open source content</p>\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e7d1ac",
   "metadata": {},
   "source": [
    "## üìú Open Source License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808c703",
   "metadata": {},
   "source": [
    "\n",
    "This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbca636",
   "metadata": {},
   "source": [
    "## üìú Open Source License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331d429",
   "metadata": {},
   "source": [
    "\n",
    "This work is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
