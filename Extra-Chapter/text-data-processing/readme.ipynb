{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13a9fe8",
   "metadata": {},
   "source": [
    "# Extra-Chapterï¼štext-data-processing\n",
    "\n",
    "æœ¬è¡¥å……ç« èŠ‚æ—¨åœ¨å¯¹å¤§æ¨¡å‹æ•°æ®å¤„ç†åšä¸€ä¸ªç®€å•çš„æ¢³ç†ä¸å…·ä½“çš„ä»£ç å®ç°ï¼Œå¸®åŠ©å¤§å®¶å¯¹äºå¤§æ¨¡å‹çš„æ•°æ®å¤„ç†æœ‰ä¸€ä¸ªæ›´åŠ æ¸…æ™°çš„è®¤è¯†ã€‚\n",
    "\n",
    "## 1.ç†è§£è¯åµŒå…¥\n",
    "\n",
    "åœ¨æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œç”±äºæ–‡æœ¬æ˜¯åˆ†ç±»æ•°æ®ï¼Œæ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œå› æ­¤éœ€è¦å°†è¯è¯­è¡¨ç¤ºä¸ºè¿ç»­å€¼å‘é‡ä¼ è¾“ç»™æ¨¡å‹ã€‚è¿™ä¸€å¤„ç†è¿‡ç¨‹æˆ‘ä»¬ç§°ä¹‹ä¸ºè¯åµŒå…¥ï¼Œå…¶æœ¬è´¨æ˜¯å°†**ç¦»æ•£å¯¹è±¡**ï¼ˆå¦‚è¯è¯­ï¼‰æ˜ å°„åˆ°**è¿ç»­å‘é‡**ç©ºé—´ä¸­çš„ç‚¹ï¼Œç›®çš„æ˜¯å°†éæ•°å€¼æ•°æ®è½¬æ¢ä¸ºç¥ç»ç½‘ç»œå¯å¤„ç†çš„æ ¼å¼ã€‚\n",
    "\n",
    "é€šè¿‡å°†æ¯ä¸ªè¯æ˜ å°„ä¸ºé«˜ç»´ç©ºé—´ä¸­çš„ç‚¹ï¼ˆå¦‚â€œçŒ«â€=[0.2, -1.7, ...]ï¼‰ï¼Œä½¿è¯­ä¹‰ç›¸ä¼¼çš„è¯ï¼ˆå¦‚â€œçŒ«â€å’Œâ€œç‹—â€ï¼‰å‘é‡è·ç¦»æ›´è¿‘ï¼Œè€Œæ— å…³è¯ï¼ˆå¦‚â€œçŒ«â€å’Œâ€œæ±½è½¦â€ï¼‰è·ç¦»æ›´è¿œã€‚è¿™ç§å‘é‡åŒ–è¡¨ç¤ºæ—¢ä¿ç•™è¯è¯­å…³ç³»ï¼ˆâ€œå›½ç‹-ç”·æ€§+å¥³æ€§â‰ˆå¥³ç‹â€ï¼‰ï¼Œåˆèƒ½ä½œä¸ºç¥ç»ç½‘ç»œçš„è¾“å…¥ï¼Œæ˜¯å¤§æ¨¡å‹ç†è§£è¯­è¨€çš„åŸºç¡€ã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹ä¸‹é¢çš„ä»£ç ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d5685",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "\n",
    "# ç¤ºä¾‹æ–‡æœ¬ï¼ˆå¯è‡ªè¡Œä¿®æ”¹å¹¶ä¸”æµ‹è¯•ï¼‰\n",
    "text = \"Hello, how are you today? I hope you are doing well.\"\n",
    "\n",
    "# ç®€å•åˆ†è¯å‡½æ•°\n",
    "def tokenize(text):\n",
    "    # æŒ‰éå­—æ¯æ•°å­—å­—ç¬¦åˆ†å‰²æ–‡æœ¬ï¼Œå¹¶ä¿ç•™åˆ†éš”ç¬¦\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    return tokens\n",
    "# æ„å»ºè¯æ±‡è¡¨\n",
    "def build_vocab(tokens):\n",
    "    unique_tokens = sorted(set(tokens))\n",
    "    vocab = {token: i for i, token in enumerate(unique_tokens)}\n",
    "    vocab_size = len(vocab)\n",
    "    return vocab, vocab_size\n",
    "# æ–‡æœ¬ç¼–ç \n",
    "def encode_text(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "# åˆ›å»ºè¯åµŒå…¥æ¨¡å‹\n",
    "class WordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordEmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.embedding(inputs)\n",
    "\n",
    "# å¯è§†åŒ–è¯å‘é‡\n",
    "def visualize_embeddings(embeddings, token_ids, vocab):\n",
    "    reverse_vocab = {i: token for token, i in vocab.items()}\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings.detach().numpy())\n",
    "    # ç»˜åˆ¶æ•£ç‚¹å›¾\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, token_id in enumerate(token_ids):\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], marker='o')\n",
    "        plt.annotate(reverse_vocab[token_id], \n",
    "                     (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                     xytext=(5, 2), \n",
    "                     textcoords='offset points',\n",
    "                     ha='right', \n",
    "                     va='bottom')\n",
    "    plt.title('Word Embeddings Visualization')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('word_embeddings.png')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    tokens = tokenize(text)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    tokens_with_special = tokens + ['<pad>', '<unk>']\n",
    "    \n",
    "    # æ„å»ºè¯æ±‡è¡¨\n",
    "    vocab, vocab_size = build_vocab(tokens_with_special)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Vocabulary: {vocab}\")\n",
    "    \n",
    "    # ç¼–ç æ–‡æœ¬\n",
    "    encoded_text = encode_text(tokens, vocab)\n",
    "    print(f\"Encoded text: {encoded_text}\")\n",
    "    input_tensor = torch.tensor(encoded_text, dtype=torch.long)\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹\n",
    "    embedding_dim = 10  # è¯å‘é‡ç»´åº¦ï¼Œå³æåˆ°çš„è¯ç©ºé—´\n",
    "    model = WordEmbeddingModel(vocab_size, embedding_dim)\n",
    "    # è·å–è¯å‘é‡\n",
    "    embeddings = model(input_tensor)\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")  # è¿™é‡Œåº”è¯¥æ˜¯ [åºåˆ—é•¿åº¦, åµŒå…¥ç»´åº¦]\n",
    "    \n",
    "    print(f\"Word vector for '{tokens[0]}': {embeddings[0].detach().numpy()}\")\n",
    "    print(f\"Word vector for '{tokens[1]}': {embeddings[1].detach().numpy()}\")\n",
    "    # å¯è§†åŒ–è¯å‘é‡ï¼ˆä»…ç”¨äºä½ç»´æ¼”ç¤ºï¼Œå®é™…åº”ç”¨ä¸­è¯å‘é‡ç»´åº¦é€šå¸¸è¾ƒé«˜ï¼‰\n",
    "    if embedding_dim >= 2:\n",
    "        visualize_embeddings(embeddings, encoded_text, vocab)\n",
    "    \n",
    "    # è®¡ç®—è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦\n",
    "    print(\"\\nè¯ç›¸ä¼¼åº¦åˆ†æ:\")\n",
    "    for i, token_i in enumerate(tokens[:5]):  # åªåˆ†æå‰5ä¸ªè¯\n",
    "        for j, token_j in enumerate(tokens[:5]):\n",
    "            if i != j:\n",
    "                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "                sim = F.cosine_similarity(embeddings[i].unsqueeze(0), \n",
    "                                         embeddings[j].unsqueeze(0))\n",
    "                print(f\" '{token_i}' å’Œ '{token_j}' çš„ç›¸ä¼¼åº¦: {sim.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c6e51",
   "metadata": {},
   "source": [
    "\n",
    "    Tokens: ['Hello', ',', 'how', 'are', 'you', 'today', '?', 'I', 'hope', 'you', 'are', 'doing', 'well', '.']\n",
    "    Vocabulary size: 14\n",
    "    Vocabulary: {',': 0, '.': 1, '<pad>': 2, '<unk>': 3, '?': 4, 'Hello': 5, 'I': 6, 'are': 7, 'doing': 8, 'hope': 9, 'how': 10, 'today': 11, 'well': 12, 'you': 13}\n",
    "    Encoded text: [5, 0, 10, 7, 13, 11, 4, 6, 9, 13, 7, 8, 12, 1]\n",
    "    Embeddings shape: torch.Size([14, 10])\n",
    "    Word vector for 'Hello': [-0.16787808 -0.46388683 -0.4728546   0.59449345 -0.23820949  0.34212282\n",
    "      0.6591729  -0.10877569  0.60686487 -1.771871  ]\n",
    "    Word vector for ',': [ 0.19194137 -1.2824519   1.1420391  -0.8361696  -0.578317    0.1025617\n",
    "      1.2452478  -0.08552601  0.9869009  -0.04940421]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "![png](./image/output_1_1.png)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    è¯ç›¸ä¼¼åº¦åˆ†æ:\n",
    "     'Hello' å’Œ ',' çš„ç›¸ä¼¼åº¦: 0.2132\n",
    "     'Hello' å’Œ 'how' çš„ç›¸ä¼¼åº¦: -0.0277\n",
    "     'Hello' å’Œ 'are' çš„ç›¸ä¼¼åº¦: 0.1024\n",
    "     'Hello' å’Œ 'you' çš„ç›¸ä¼¼åº¦: 0.1597\n",
    "     ',' å’Œ 'Hello' çš„ç›¸ä¼¼åº¦: 0.2132\n",
    "     ',' å’Œ 'how' çš„ç›¸ä¼¼åº¦: -0.0801\n",
    "     ',' å’Œ 'are' çš„ç›¸ä¼¼åº¦: -0.6096\n",
    "     ',' å’Œ 'you' çš„ç›¸ä¼¼åº¦: 0.2355\n",
    "     'how' å’Œ 'Hello' çš„ç›¸ä¼¼åº¦: -0.0277\n",
    "     'how' å’Œ ',' çš„ç›¸ä¼¼åº¦: -0.0801\n",
    "     'how' å’Œ 'are' çš„ç›¸ä¼¼åº¦: 0.1170\n",
    "     'how' å’Œ 'you' çš„ç›¸ä¼¼åº¦: -0.0412\n",
    "     'are' å’Œ 'Hello' çš„ç›¸ä¼¼åº¦: 0.1024\n",
    "     'are' å’Œ ',' çš„ç›¸ä¼¼åº¦: -0.6096\n",
    "     'are' å’Œ 'how' çš„ç›¸ä¼¼åº¦: 0.1170\n",
    "     'are' å’Œ 'you' çš„ç›¸ä¼¼åº¦: 0.2203\n",
    "     'you' å’Œ 'Hello' çš„ç›¸ä¼¼åº¦: 0.1597\n",
    "     'you' å’Œ ',' çš„ç›¸ä¼¼åº¦: 0.2355\n",
    "     'you' å’Œ 'how' çš„ç›¸ä¼¼åº¦: -0.0412\n",
    "     'you' å’Œ 'are' çš„ç›¸ä¼¼åº¦: 0.2203\n",
    "    \n",
    "\n",
    "è¿™ä¸‹æˆ‘ä»¬å°±èƒ½å¤Ÿå¾ˆæ¸…æ¥šçš„åœ¨æ•£ç‚¹å›¾ä¸­çœ‹åˆ°ä¸åŒè¯è¯­ä¹‹é—´çš„è·ç¦»ã€‚\n",
    "\n",
    "## 2.åˆ†è¯\n",
    "\n",
    "åˆ†è¯æ˜¯å°†è¾“å…¥æ–‡æœ¬æ‹†åˆ†ä¸ºå•ä¸ªæ ‡è®°ï¼ˆå¯ä»¥æ˜¯å•è¯ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼‰çš„è¿‡ç¨‹ï¼Œä¸¾ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå¥å­â€œHello, world!â€æ‹†æˆä¸€å°å—ä¸€å°å—çš„å½¢å¼ï¼Œå¦‚```[\"Hello\", \",\", \"world\", \"!\"]```ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿé€å—å¤„ç†è¯­è¨€ï¼Œè¿›ä¸€æ­¥è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å€¼å½¢å¼ã€‚\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥ä»ç®€å•çš„åˆ†è¯æ–¹æ³•å…¥æ‰‹ï¼Œä¾‹å¦‚ä½¿ç”¨ Python çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆre åº“ï¼‰æŒ‰ç©ºç™½å­—ç¬¦æ‹†åˆ†æ–‡æœ¬ï¼Œä½†è¿™ç§æ–¹æ³•å¯èƒ½å¯¼è‡´æ ‡ç‚¹ç¬¦å·ä¸å•è¯ç²˜è¿ï¼ˆå¦‚ â€œHello,â€ ä¸­çš„é€—å·æœªåˆ†ç¦»ï¼‰ã€‚\n",
    "\n",
    "ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»£ç å½“ä¸­ä¿®æ”¹æ­£åˆ™è¡¨è¾¾å¼ï¼Œåœ¨ç©ºç™½å­—ç¬¦ã€é€—å·ã€å¥å·ç­‰å¤„è¿›è¡Œæ‹†åˆ†ï¼Œä½¿å•è¯ä¸æ ‡ç‚¹ç¬¦å·æˆä¸ºç‹¬ç«‹åˆ—è¡¨é¡¹ã€‚\n",
    "\n",
    "åŒæ—¶ï¼Œåœ¨å¤„ç†çš„è¿‡ç¨‹ä¸­éœ€æ³¨æ„ä¿ç•™ç©ºç™½å­—ç¬¦æˆ–ç§»é™¤å†—ä½™ç©ºç™½ï¼Œä¾‹å¦‚å¤„ç† Python ä»£ç æ—¶éœ€ä¿ç•™ç¼©è¿›ï¼Œè€Œæ™®é€šæ–‡æœ¬å¯ç§»é™¤ã€‚\n",
    "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šé€šå¸¸ä¸å°†æ–‡æœ¬è½¬ä¸ºå°å†™ï¼Œå› ä¸ºå¤§å°å†™æœ‰åŠ©äºæ¨¡å‹åŒºåˆ†ä¸“æœ‰åè¯ã€ç†è§£å¥å­ç»“æ„ã€‚\n",
    "å®ç°ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb43ce",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"ç®€å•åˆ†è¯å™¨å®ç°\"\"\"\n",
    "    def __init__(self, text=None):\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        self.special_tokens = {\n",
    "            '<pad>': 0,\n",
    "            '<unk>': 1,\n",
    "            '<bos>': 2,\n",
    "            '<eos>': 3\n",
    "        }\n",
    "        if text:\n",
    "            self.build_vocab(text)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ ‡è®°åˆ—è¡¨\"\"\"\n",
    "        # ä¿ç•™ç©ºç™½å­—ç¬¦çš„åˆ†è¯\n",
    "        tokens = re.findall(r'\\S+|\\s+', text)\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, text):\n",
    "        \"\"\"æ„å»ºè¯æ±‡è¡¨\"\"\"\n",
    "        self.vocab = self.special_tokens.copy()\n",
    "        self.vocab_size = len(self.special_tokens)\n",
    "        tokens = self.tokenize(text)\n",
    "        token_counts = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            token_counts[token] += 1\n",
    "        sorted_tokens = sorted(token_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "        for token, _ in sorted_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = self.vocab_size\n",
    "                self.vocab_size += 1\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°IDåºåˆ—\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"å°†æ ‡è®°IDåºåˆ—è½¬æ¢å›æ–‡æœ¬\"\"\"\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.reverse_vocab:\n",
    "                tokens.append(self.reverse_vocab[token_id])\n",
    "            else:\n",
    "                tokens.append('<unk>')\n",
    "        text = ''\n",
    "        prev_token = None\n",
    "        for token in tokens:\n",
    "            if prev_token and prev_token.isspace():\n",
    "                text += token\n",
    "            else:\n",
    "                text = text.rstrip() + token\n",
    "            prev_token = token\n",
    "            \n",
    "        return text\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"åŸºäºå­—èŠ‚å¯¹ç¼–ç (BPE)çš„åˆ†è¯å™¨\"\"\"\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–BPEåˆ†è¯å™¨\n",
    "        model_name: æ¨¡å‹åç§°ï¼Œå¦‚\"gpt2\"æˆ–\"cl100k_base\"(OpenAIçš„text-embedding-ada-002ä½¿ç”¨)\n",
    "        \"\"\"\n",
    "        self.encoder = tiktoken.get_encoding(model_name)\n",
    "        self.vocab_size = self.encoder.n_vocab\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"å°†æ–‡æœ¬è½¬æ¢ä¸ºBPEæ ‡è®°åˆ—è¡¨\"\"\"\n",
    "        token_ids = self.encoder.encode(text)\n",
    "        tokens = [self.encoder.decode_single_token_bytes(token_id) for token_id in token_ids]\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"å°†æ–‡æœ¬è½¬æ¢ä¸ºBPEæ ‡è®°IDåºåˆ—\"\"\"\n",
    "        return self.encoder.encode(text)\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"å°†BPEæ ‡è®°IDåºåˆ—è½¬æ¢å›æ–‡æœ¬\"\"\"\n",
    "        return self.encoder.decode(token_ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Hello, how are you today? I hope you are doing well.\"\n",
    "    print(\"=== ç®€å•åˆ†è¯å™¨ ===\")\n",
    "    simple_tokenizer = SimpleTokenizer(sample_text)\n",
    "    tokens = simple_tokenizer.tokenize(sample_text)\n",
    "    print(f\"åˆ†è¯ç»“æœ: {tokens}\")\n",
    "    encoded = simple_tokenizer.encode(sample_text)\n",
    "    print(f\"ç¼–ç ç»“æœ: {encoded}\")\n",
    "    decoded = simple_tokenizer.decode(encoded)\n",
    "    print(f\"è§£ç ç»“æœ: {decoded}\")\n",
    "    print(f\"è¯æ±‡è¡¨å¤§å°: {simple_tokenizer.vocab_size}\")\n",
    "    print(\"\\n=== BPEåˆ†è¯å™¨ (GPT-2) ===\")\n",
    "    bpe_tokenizer = BPETokenizer(\"gpt2\")\n",
    "    bpe_tokens = bpe_tokenizer.tokenize(sample_text)\n",
    "    print(f\"BPEåˆ†è¯ç»“æœ: {bpe_tokens}\")\n",
    "    bpe_encoded = bpe_tokenizer.encode(sample_text)\n",
    "    print(f\"BPEç¼–ç ç»“æœ: {bpe_encoded}\")\n",
    "    bpe_decoded = bpe_tokenizer.decode(bpe_encoded)\n",
    "    print(f\"BPEè§£ç ç»“æœ: {bpe_decoded}\")\n",
    "    print(f\"BPEè¯æ±‡è¡¨å¤§å°: {bpe_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d087930",
   "metadata": {},
   "source": [
    "\n",
    "    === ç®€å•åˆ†è¯å™¨ ===\n",
    "    åˆ†è¯ç»“æœ: ['Hello,', ' ', 'how', ' ', 'are', ' ', 'you', ' ', 'today?', ' ', 'I', ' ', 'hope', ' ', 'you', ' ', 'are', ' ', 'doing', ' ', 'well.']\n",
    "    ç¼–ç ç»“æœ: [7, 4, 11, 4, 5, 4, 6, 4, 12, 4, 8, 4, 10, 4, 6, 4, 5, 4, 9, 4, 13]\n",
    "    è§£ç ç»“æœ: Hello, how are you today? I hope you are doing well.\n",
    "    è¯æ±‡è¡¨å¤§å°: 14\n",
    "    \n",
    "    === BPEåˆ†è¯å™¨ (GPT-2) ===\n",
    "    BPEåˆ†è¯ç»“æœ: [b'Hello', b',', b' how', b' are', b' you', b' today', b'?', b' I', b' hope', b' you', b' are', b' doing', b' well', b'.']\n",
    "    BPEç¼–ç ç»“æœ: [15496, 11, 703, 389, 345, 1909, 30, 314, 2911, 345, 389, 1804, 880, 13]\n",
    "    BPEè§£ç ç»“æœ: Hello, how are you today? I hope you are doing well.\n",
    "    BPEè¯æ±‡è¡¨å¤§å°: 50257\n",
    "    \n",
    "\n",
    "## 3.å°†æ ‡è®°è½¬æ¢ä¸ºæ ‡è®°ID\n",
    "\n",
    "å°†æ ‡è®°è½¬æ¢ä¸ºæ ‡è®° ID æ˜¯è¿æ¥æ–‡æœ¬ä¸æ•°å€¼å‘é‡çš„ä¸­é—´æ­¥éª¤ï¼Œå…¶æ ¸å¿ƒæ˜¯æ„å»º â€œè¯æ±‡è¡¨â€â€”â€” ä¸€ä¸ªä»å”¯ä¸€æ ‡è®°åˆ°å”¯ä¸€æ•´æ•°çš„æ˜ å°„ï¼ˆå¦‚å›¾ 2.6 æ‰€ç¤ºï¼‰ã€‚è¯æ±‡è¡¨çš„æ„å»ºè¿‡ç¨‹æ˜¯ï¼šä»åˆ†è¯åçš„æ–‡æœ¬ä¸­æå–æ‰€æœ‰å”¯ä¸€æ ‡è®°ï¼ŒæŒ‰å­—æ¯é¡ºåºæ’åºåï¼Œä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ä¸€ä¸ªæ•´æ•° IDã€‚ä¾‹å¦‚ï¼Œã€Šåˆ¤å†³ã€‹åˆ†è¯åå¾—åˆ° 1130 ä¸ªå”¯ä¸€æ ‡è®°ï¼Œè¯æ±‡è¡¨ä¾¿ä¼šå°†è¿™äº›æ ‡è®°åˆ†åˆ«æ˜ å°„åˆ° 0 è‡³ 1129 çš„æ•´æ•°ã€‚\n",
    "\n",
    "æœ‰äº†è¯æ±‡è¡¨ï¼Œæˆ‘ä»¬å°±èƒ½é€šè¿‡ â€œç¼–ç â€ å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®° IDï¼ˆä¾‹å¦‚ â€œHelloâ€ å¯¹åº”æŸä¸ªæ•´æ•°ï¼‰ï¼Œä¹Ÿèƒ½é€šè¿‡ â€œè§£ç â€ å°†æ ‡è®° ID è½¬å›æ–‡æœ¬ã€‚è¿™ä¸€è¿‡ç¨‹å¯é€šè¿‡åˆ†è¯å™¨ç±»å®ç°ï¼Œä¾‹å¦‚ SimpleTokenizerV1 åŒ…å« encode å’Œ decode æ–¹æ³•ï¼šencode å…ˆå¯¹æ–‡æœ¬åˆ†è¯ï¼Œå†ç”¨è¯æ±‡è¡¨æ˜ å°„ä¸º IDï¼›decode åˆ™å°† ID é€šè¿‡åå‘æ˜ å°„è½¬å›æ–‡æœ¬ï¼Œå¹¶å¤„ç†æ ‡ç‚¹ç¬¦å·å‰çš„ç©ºæ ¼é—®é¢˜ã€‚\n",
    "\n",
    "ä½†éœ€æ³¨æ„ï¼Œè‹¥æ–‡æœ¬ä¸­å‡ºç°è¯æ±‡è¡¨å¤–çš„æ ‡è®°ï¼ˆå¦‚ â€œHelloâ€ æœªå‡ºç°åœ¨ã€Šåˆ¤å†³ã€‹ä¸­ï¼‰ï¼Œç¼–ç æ—¶ä¼šæŠ¥é”™ï¼Œè¿™è¯´æ˜è®­ç»ƒé›†çš„è§„æ¨¡å’Œå¤šæ ·æ€§å¯¹æ‰©å±•è¯æ±‡è¡¨è‡³å…³é‡è¦ï¼Œä¹Ÿå¼•å‡ºäº†åç»­å¤„ç†æœªçŸ¥è¯æ±‡çš„éœ€æ±‚ã€‚\n",
    "\n",
    "æ•´ä½“å®ç°ä»£ç å¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664eb227",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "class Tokenizer:\n",
    "    \"\"\"å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°å¹¶æ˜ å°„åˆ°IDçš„åˆ†è¯å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åˆ†è¯å™¨\n",
    "        \n",
    "        Args:\n",
    "            special_tokens: ç‰¹æ®Šæ ‡è®°åŠå…¶IDçš„å­—å…¸ï¼Œé»˜è®¤ä¸ºNone\n",
    "        \"\"\"\n",
    "        self.vocab: Dict[str, int] = {}  # è¯æ±‡è¡¨: æ ‡è®° -> ID\n",
    "        self.reverse_vocab: Dict[int, str] = {}  # åå‘è¯æ±‡è¡¨: ID -> æ ‡è®°\n",
    "        self.special_tokens = special_tokens or {\n",
    "            \"<pad>\": 0,  # å¡«å……æ ‡è®°\n",
    "            \"<unk>\": 1,  # æœªçŸ¥æ ‡è®°\n",
    "            \"<bos>\": 2,  # åºåˆ—å¼€å§‹æ ‡è®°\n",
    "            \"<eos>\": 3   # åºåˆ—ç»“æŸæ ‡è®°\n",
    "        }\n",
    "        self.vocab_size: int = len(self.special_tokens)  # è¯æ±‡è¡¨å¤§å°\n",
    "        # åˆå§‹åŒ–è¯æ±‡è¡¨ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨\n",
    "        \n",
    "        Args:\n",
    "            texts: æ–‡æœ¬åˆ—è¡¨\n",
    "            min_freq: æœ€å°è¯é¢‘ï¼Œä½äºæ­¤é¢‘ç‡çš„è¯å°†è¢«å¿½ç•¥\n",
    "        \"\"\"\n",
    "        # ç»Ÿè®¡è¯é¢‘\n",
    "        token_counts = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            token_counts.update(tokens)\n",
    "        # æŒ‰é¢‘ç‡æ’åºï¼Œé¢‘ç‡ç›¸åŒåˆ™æŒ‰å­—æ¯é¡ºåº\n",
    "        sorted_tokens = sorted(\n",
    "            [(token, count) for token, count in token_counts.items() if count >= min_freq],\n",
    "            key=lambda x: (-x[1], x[0])  \n",
    "        )\n",
    "        # ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ID\n",
    "        for token, _ in sorted_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = self.vocab_size\n",
    "                self.reverse_vocab[self.vocab_size] = token\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ ‡è®°åˆ—è¡¨\n",
    "        \n",
    "        Args:\n",
    "            text: è¾“å…¥æ–‡æœ¬\n",
    "            \n",
    "        Returns:\n",
    "            æ ‡è®°åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°IDåˆ—è¡¨\n",
    "        \n",
    "        Args:\n",
    "            text: è¾“å…¥æ–‡æœ¬\n",
    "            add_special_tokens: æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°\n",
    "            \n",
    "        Returns:\n",
    "            æ ‡è®°IDåˆ—è¡¨\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        encoded = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            encoded = [self.vocab[\"<bos>\"]] + encoded + [self.vocab[\"<eos>\"]]\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        å°†æ ‡è®°IDåˆ—è¡¨è½¬æ¢å›æ–‡æœ¬\n",
    "        Args:\n",
    "            ids: æ ‡è®°IDåˆ—è¡¨\n",
    "            remove_special_tokens: æ˜¯å¦ç§»é™¤ç‰¹æ®Šæ ‡è®°\n",
    "        Returns:\n",
    "            æ–‡æœ¬\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            if idx in self.reverse_vocab:\n",
    "                token = self.reverse_vocab[idx]\n",
    "                if remove_special_tokens and token in self.special_tokens:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.append(\"<unk>\")\n",
    "        text = ''.join(tokens)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def test_tokenizer():\n",
    "    \"\"\"æµ‹è¯•Tokenizerç±»çš„åŠŸèƒ½\"\"\"\n",
    "    print(\"===== æµ‹è¯• Tokenizer =====\")\n",
    "    texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"I hope you are doing well.\",\n",
    "        \"This is a test of the tokenizer.\"\n",
    "    ]\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.build_vocab(texts)\n",
    "    print(f\"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}\")\n",
    "    print(f\"å‰10ä¸ªè¯æ±‡é¡¹: {list(tokenizer.vocab.items())[:10]}\")\n",
    "    sample_text = \"Hello, this is a test!\"\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    print(f\"\\nåˆ†è¯ç»“æœ: {tokens}\")\n",
    "    encoded = tokenizer.encode(sample_text)\n",
    "    print(f\"ç¼–ç ç»“æœ: {encoded}\")\n",
    "    encoded_with_special = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "    print(f\"å¸¦ç‰¹æ®Šæ ‡è®°çš„ç¼–ç ç»“æœ: {encoded_with_special}\")\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"è§£ç ç»“æœ: {decoded}\")\n",
    "    decoded_with_special = tokenizer.decode(encoded_with_special, remove_special_tokens=True)\n",
    "    print(f\"ç§»é™¤ç‰¹æ®Šæ ‡è®°çš„è§£ç ç»“æœ: {decoded_with_special}\")\n",
    "    unknown_text = \"This is a unicorn ğŸ¦„ test.\"\n",
    "    encoded_unknown = tokenizer.encode(unknown_text)\n",
    "    decoded_unknown = tokenizer.decode(encoded_unknown)\n",
    "    print(f\"\\nåŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: {unknown_text}\")\n",
    "    print(f\"ç¼–ç ç»“æœ: {encoded_unknown}\")\n",
    "    print(f\"è§£ç ç»“æœ: {decoded_unknown}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a059b5",
   "metadata": {},
   "source": [
    "\n",
    "    ===== æµ‹è¯• Tokenizer =====\n",
    "    è¯æ±‡è¡¨å¤§å°: 24\n",
    "    å‰10ä¸ªè¯æ±‡é¡¹: [('<pad>', 0), ('<unk>', 1), ('<bos>', 2), ('<eos>', 3), (' ', 4), ('.', 5), ('are', 6), ('you', 7), (',', 8), ('?', 9)]\n",
    "    \n",
    "    åˆ†è¯ç»“æœ: ['Hello', ',', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test', '!']\n",
    "    ç¼–ç ç»“æœ: [10, 8, 4, 1, 4, 17, 4, 13, 4, 19, 1]\n",
    "    å¸¦ç‰¹æ®Šæ ‡è®°çš„ç¼–ç ç»“æœ: [2, 10, 8, 4, 1, 4, 17, 4, 13, 4, 19, 1, 3]\n",
    "    è§£ç ç»“æœ: Hello, <unk> is a test<unk>\n",
    "    ç§»é™¤ç‰¹æ®Šæ ‡è®°çš„è§£ç ç»“æœ: Hello, is a test\n",
    "    \n",
    "    åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: This is a unicorn ğŸ¦„ test.\n",
    "    ç¼–ç ç»“æœ: [12, 4, 17, 4, 13, 4, 1, 4, 1, 4, 19, 5]\n",
    "    è§£ç ç»“æœ: This is a <unk> <unk> test.\n",
    "    \n",
    "\n",
    "## 4.æ·»åŠ ç‰¹æ®Šä¸Šä¸‹æ–‡æ ‡è®°\n",
    "\n",
    "ä¸ºè§£å†³æœªçŸ¥è¯æ±‡é—®é¢˜å¹¶å¢å¼ºæ¨¡å‹å¯¹ä¸Šä¸‹æ–‡çš„ç†è§£ï¼Œéœ€å¼•å…¥ç‰¹æ®Šæ ‡è®°ã€‚å¸¸è§çš„ç‰¹æ®Šæ ‡è®°åŒ…æ‹¬ï¼š<|unk|>ï¼ˆè¡¨ç¤ºæœªçŸ¥è¯æ±‡ï¼‰ã€ï¼ˆåˆ†éš”ä¸åŒæ–‡æœ¬æ¥æºï¼‰ã€[BOS]ï¼ˆåºåˆ—å¼€å§‹ï¼‰ã€[EOS]ï¼ˆåºåˆ—ç»“æŸï¼‰ã€[PAD]ï¼ˆå¡«å……çŸ­æ–‡æœ¬è‡³ç»Ÿä¸€é•¿åº¦ï¼‰ç­‰ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œä¿®æ”¹è¯æ±‡è¡¨åŠ å…¥ <|unk|> å’Œåï¼Œåˆ†è¯å™¨ï¼ˆå¦‚ SimpleTokenizerV2ï¼‰åœ¨é‡åˆ°æœªçŸ¥è¯æ—¶ä¼šè‡ªåŠ¨æ›¿æ¢ä¸º <|unk|>ï¼Œå¹¶åœ¨ä¸åŒæ–‡æœ¬é—´æ’å…¥ä½œä¸ºåˆ†éš”ã€‚è¿™ä¸€è°ƒæ•´ä½¿æ¨¡å‹èƒ½å¤„ç†æœªè§è¿‡çš„è¯æ±‡ï¼Œå¹¶åŒºåˆ†ç‹¬ç«‹æ–‡æœ¬æ¥æºã€‚éœ€æ³¨æ„ï¼ŒGPT æ¨¡å‹é€šå¸¸ä»…ä½¿ç”¨ä½œä¸ºåˆ†éš”ç¬¦å’Œå¡«å……ç¬¦ï¼Œè€Œä¸ä¾èµ– <|unk|>ï¼Œå› ä¸ºå…¶é‡‡ç”¨çš„å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰åˆ†è¯å™¨èƒ½é€šè¿‡å­è¯åˆ†è§£å¤„ç†æœªçŸ¥è¯ï¼Œè¿™ä¹Ÿæ˜¯åç»­å°†ä»‹ç»çš„æ›´é«˜æ•ˆæ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9809349f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Optional, Set\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"å¸¦æœ‰ç‰¹æ®Šä¸Šä¸‹æ–‡æ ‡è®°çš„åˆ†è¯å™¨\"\"\"\n",
    "    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åˆ†è¯å™¨\n",
    "        \n",
    "        Args:\n",
    "            special_tokens: ç‰¹æ®Šæ ‡è®°åŠå…¶IDçš„å­—å…¸ï¼Œé»˜è®¤ä¸ºNone\n",
    "        \"\"\"\n",
    "        # è®¾ç½®é»˜è®¤ç‰¹æ®Šæ ‡è®°\n",
    "        self.default_special_tokens = {\n",
    "            \"<pad>\": 0,  # å¡«å……æ ‡è®°\n",
    "            \"<unk>\": 1,  # æœªçŸ¥æ ‡è®°\n",
    "            \"<bos>\": 2,  # åºåˆ—å¼€å§‹æ ‡è®°\n",
    "            \"<eos>\": 3,  # åºåˆ—ç»“æŸæ ‡è®°\n",
    "            \"<sep>\": 4,  # åˆ†éš”æ ‡è®°\n",
    "            \"<cls>\": 5,  # åˆ†ç±»æ ‡è®°\n",
    "        }\n",
    "        # åˆå¹¶ç”¨æˆ·æä¾›çš„ç‰¹æ®Šæ ‡è®°\n",
    "        self.special_tokens = {**self.default_special_tokens, **(special_tokens or {})}\n",
    "        # åˆå§‹åŒ–è¯æ±‡è¡¨\n",
    "        self.vocab: Dict[str, int] = {}\n",
    "        self.reverse_vocab: Dict[int, str] = {}\n",
    "        self.vocab_size: int = len(self.special_tokens)\n",
    "        # æ·»åŠ ç‰¹æ®Šæ ‡è®°åˆ°è¯æ±‡è¡¨\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨\n",
    "        \n",
    "        Args:\n",
    "            texts: æ–‡æœ¬åˆ—è¡¨\n",
    "            min_freq: æœ€å°è¯é¢‘ï¼Œä½äºæ­¤é¢‘ç‡çš„è¯å°†è¢«å¿½ç•¥\n",
    "        \"\"\"\n",
    "        # ç»Ÿè®¡è¯é¢‘\n",
    "        token_counts = {}\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                token_counts[token] = token_counts.get(token, 0) + 1\n",
    "        \n",
    "        # æŒ‰é¢‘ç‡æ’åºï¼Œé¢‘ç‡ç›¸åŒåˆ™æŒ‰å­—æ¯é¡ºåº\n",
    "        sorted_tokens = sorted(\n",
    "            [(token, count) for token, count in token_counts.items() if count >= min_freq],\n",
    "            key=lambda x: (-x[1], x[0])  # æŒ‰é¢‘ç‡é™åºï¼Œå­—æ¯å‡åº\n",
    "        )\n",
    "        \n",
    "        # ä¸ºæ¯ä¸ªæ ‡è®°åˆ†é…ID\n",
    "        for token, _ in sorted_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = self.vocab_size\n",
    "                self.reverse_vocab[self.vocab_size] = token\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        å°†æ–‡æœ¬åˆ†å‰²ä¸ºæ ‡è®°åˆ—è¡¨\n",
    "        \n",
    "        Args:\n",
    "            text: è¾“å…¥æ–‡æœ¬\n",
    "            \n",
    "        Returns:\n",
    "            æ ‡è®°åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        # ç®€å•çš„åˆ†è¯ï¼šæŒ‰éå­—æ¯æ•°å­—å­—ç¬¦åˆ†å‰²ï¼Œä¿ç•™ç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®°IDåˆ—è¡¨\n",
    "        \n",
    "        Args:\n",
    "            text: è¾“å…¥æ–‡æœ¬\n",
    "            add_special_tokens: æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°\n",
    "            \n",
    "        Returns:\n",
    "            æ ‡è®°IDåˆ—è¡¨\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        encoded = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        if add_special_tokens:\n",
    "            encoded = [self.vocab[\"<bos>\"]] + encoded + [self.vocab[\"<eos>\"]]\n",
    "        return encoded\n",
    "    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        å°†æ ‡è®°IDåˆ—è¡¨è½¬æ¢å›æ–‡æœ¬\n",
    "        Args:\n",
    "            ids: æ ‡è®°IDåˆ—è¡¨\n",
    "            remove_special_tokens: æ˜¯å¦ç§»é™¤ç‰¹æ®Šæ ‡è®°\n",
    "        Returns:\n",
    "            æ–‡æœ¬\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            if idx in self.reverse_vocab:\n",
    "                token = self.reverse_vocab[idx]\n",
    "                if remove_special_tokens and token in self.special_tokens:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.append(\"<unk>\")\n",
    "        text = ''.join(tokens)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    def add_special_token(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®°\n",
    "        Args:\n",
    "            token: ç‰¹æ®Šæ ‡è®°å­—ç¬¦ä¸²\n",
    "        Returns:\n",
    "            æ–°æ ‡è®°çš„ID\n",
    "        \"\"\"\n",
    "        if token in self.vocab:\n",
    "            return self.vocab[token]\n",
    "        new_id = self.vocab_size\n",
    "        self.vocab[token] = new_id\n",
    "        self.reverse_vocab[new_id] = token\n",
    "        self.vocab_size += 1\n",
    "        return new_id\n",
    "    \n",
    "def test_special_tokens():\n",
    "    \"\"\"æµ‹è¯•ç‰¹æ®Šæ ‡è®°çš„åŠŸèƒ½\"\"\"\n",
    "    print(\"===== æµ‹è¯•ç‰¹æ®Šæ ‡è®° =====\")\n",
    "    texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"I hope you are doing well.\",\n",
    "        \"This is a test of the tokenizer.\"\n",
    "    ]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.build_vocab(texts)\n",
    "    sample_text = \"Hello, this is a test!\"\n",
    "    encoded = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "    decoded = tokenizer.decode(encoded, remove_special_tokens=True)\n",
    "    \n",
    "    print(f\"åŸå§‹æ–‡æœ¬: {sample_text}\")\n",
    "    print(f\"ç¼–ç ç»“æœ: {encoded}\")\n",
    "    print(f\"è§£ç ç»“æœ: {decoded}\")\n",
    "    print(\"\\n--- æµ‹è¯•ç‰¹æ®Šæ ‡è®° ---\")\n",
    "    print(f\"ç‰¹æ®Šæ ‡è®°: {tokenizer.special_tokens}\")\n",
    "    # æµ‹è¯•<bos>å’Œ<eos>\n",
    "    encoded_with_bos_eos = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "    print(f\"å¸¦<bos>å’Œ<eos>çš„ç¼–ç : {encoded_with_bos_eos}\")\n",
    "    # æµ‹è¯•<SEP>æ ‡è®° - è¿æ¥ä¸¤ä¸ªå¥å­\n",
    "    print(\"\\n--- æµ‹è¯•<SEP>æ ‡è®° ---\")\n",
    "    sentence1 = \"What is your name?\"\n",
    "    sentence2 = \"My name is Doubao.\"\n",
    "    encoded1 = tokenizer.encode(sentence1)\n",
    "    encoded2 = tokenizer.encode(sentence2)\n",
    "    # æ·»åŠ <SEP>æ ‡è®°\n",
    "    encoded_combined = [tokenizer.vocab[\"<bos>\"]] + \\\n",
    "                       encoded1 + \\\n",
    "                       [tokenizer.vocab[\"<sep>\"]] + \\\n",
    "                       encoded2 + \\\n",
    "                       [tokenizer.vocab[\"<eos>\"]]\n",
    "    decoded_combined = tokenizer.decode(encoded_combined, remove_special_tokens=True)\n",
    "    print(f\"å¥å­1: {sentence1}\")\n",
    "    print(f\"å¥å­2: {sentence2}\")\n",
    "    print(f\"åˆå¹¶åçš„ç¼–ç : {encoded_combined}\")\n",
    "    print(f\"åˆå¹¶åçš„è§£ç : {decoded_combined}\")\n",
    "    # æµ‹è¯•æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®°\n",
    "    print(\"\\n--- æµ‹è¯•æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®° ---\")\n",
    "    new_token = \"<mask>\"\n",
    "    new_token_id = tokenizer.add_special_token(new_token)\n",
    "    print(f\"æ·»åŠ æ–°ç‰¹æ®Šæ ‡è®°: {new_token} (ID: {new_token_id})\")\n",
    "    \n",
    "    # æµ‹è¯•ä½¿ç”¨æ–°çš„ç‰¹æ®Šæ ‡è®°\n",
    "    masked_text = \"This is a <mask> sentence.\"\n",
    "    encoded_masked = tokenizer.encode(masked_text, add_special_tokens=True)\n",
    "    decoded_masked = tokenizer.decode(encoded_masked, remove_special_tokens=False)\n",
    "    \n",
    "    print(f\"å¸¦<mask>çš„æ–‡æœ¬: {masked_text}\")\n",
    "    print(f\"ç¼–ç ç»“æœ: {encoded_masked}\")\n",
    "    print(f\"è§£ç ç»“æœ: {decoded_masked}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_special_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cca374",
   "metadata": {},
   "source": [
    "\n",
    "    ===== æµ‹è¯•ç‰¹æ®Šæ ‡è®° =====\n",
    "    åŸå§‹æ–‡æœ¬: Hello, this is a test!\n",
    "    ç¼–ç ç»“æœ: [2, 12, 10, 6, 1, 6, 19, 6, 15, 6, 21, 1, 3]\n",
    "    è§£ç ç»“æœ: Hello, is a test\n",
    "    \n",
    "    --- æµ‹è¯•ç‰¹æ®Šæ ‡è®° ---\n",
    "    ç‰¹æ®Šæ ‡è®°: {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, '<sep>': 4, '<cls>': 5}\n",
    "    å¸¦<bos>å’Œ<eos>çš„ç¼–ç : [2, 12, 10, 6, 1, 6, 19, 6, 15, 6, 21, 1, 3]\n",
    "    \n",
    "    --- æµ‹è¯•<SEP>æ ‡è®° ---\n",
    "    å¥å­1: What is your name?\n",
    "    å¥å­2: My name is Doubao.\n",
    "    åˆå¹¶åçš„ç¼–ç : [2, 1, 6, 19, 6, 1, 6, 1, 11, 4, 1, 6, 1, 6, 19, 6, 1, 7, 3]\n",
    "    åˆå¹¶åçš„è§£ç : is ? is .\n",
    "    \n",
    "    --- æµ‹è¯•æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®° ---\n",
    "    æ·»åŠ æ–°ç‰¹æ®Šæ ‡è®°: <mask> (ID: 26)\n",
    "    å¸¦<mask>çš„æ–‡æœ¬: This is a <mask> sentence.\n",
    "    ç¼–ç ç»“æœ: [2, 14, 6, 19, 6, 15, 6, 1, 1, 1, 6, 1, 7, 3]\n",
    "    è§£ç ç»“æœ: <bos>This is a <unk><unk><unk> <unk>.<eos>\n",
    "    \n",
    "\n",
    "## 5.å­—èŠ‚å¯¹ç¼–ç \n",
    "\n",
    "å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰æ˜¯ä¸€ç§é«˜çº§åˆ†è¯æ–¹æ³•ï¼Œè¢« GPT-2ã€GPT-3 ç­‰ä¸»æµ LLMs é‡‡ç”¨ï¼Œå…¶æ ¸å¿ƒä¼˜åŠ¿æ˜¯èƒ½å¤„ç†æœªçŸ¥è¯æ±‡ â€”â€” å°†æœªè§è¿‡çš„å•è¯åˆ†è§£ä¸ºå­è¯å•å…ƒï¼ˆå¦‚å›¾ 2.11 æ‰€ç¤ºï¼‰ã€‚ä¾‹å¦‚ï¼Œâ€œsomeunknownPlaceâ€ å¯æ‹†åˆ†ä¸ºå·²çŸ¥çš„å­è¯æ ‡è®°ï¼Œæ— éœ€ä¾èµ– <|unk|>ã€‚\n",
    "\n",
    "BPE çš„å®ç°å¯å€ŸåŠ© tiktoken åº“ï¼ˆOpenAI å¼€æºï¼‰ï¼Œå…¶åŸºäº Rust å®ç°ï¼Œé«˜æ•ˆä¸”å…¼å®¹ GPT æ¨¡å‹çš„åˆ†è¯é€»è¾‘ã€‚ä½¿ç”¨æ—¶ï¼Œå…ˆé€šè¿‡ tiktoken.get_encoding (\"gpt2\") å®ä¾‹åŒ–åˆ†è¯å™¨ï¼Œå†ç”¨ encode æ–¹æ³•å°†æ–‡æœ¬è½¬æ¢ä¸ºæ ‡è®° IDï¼Œdecode æ–¹æ³•åˆ™å¯è¿˜åŸæ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œâ€œHello, do you like tea?â€ ç» BPE ç¼–ç åï¼Œä¼šç”Ÿæˆä¸€ç³»åˆ—æ•´æ•° IDï¼Œè§£ç åèƒ½å‡†ç¡®è¿˜åŸåŸå§‹æ–‡æœ¬ï¼Œå³ä½¿åŒ…å« â€œsomeunknownPlaceâ€ è¿™ç±»æœªçŸ¥è¯ï¼Œä¹Ÿèƒ½é€šè¿‡å­è¯æ‹†åˆ†æ­£ç¡®å¤„ç†ã€‚\n",
    "\n",
    "BPE çš„è¯æ±‡è¡¨è§„æ¨¡å›ºå®šï¼ˆå¦‚ GPT-2 ä¸º 50257ï¼‰ï¼Œé€šè¿‡è¿­ä»£åˆå¹¶é«˜é¢‘å­—ç¬¦æˆ–å­è¯æ„å»ºï¼Œæ—¢æ§åˆ¶äº†è¯æ±‡è¡¨å¤§å°ï¼Œåˆèƒ½è¦†ç›–å‡ ä¹æ‰€æœ‰å¯èƒ½çš„æ–‡æœ¬ï¼Œæ˜¯å¹³è¡¡æ•ˆç‡ä¸æ³›åŒ–èƒ½åŠ›çš„ç†æƒ³é€‰æ‹©ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222f8b6",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Set, Union\n",
    "\n",
    "class SimpleBPETokenizer:\n",
    "    \"\"\"ç®€å•å®ç°çš„BPEåˆ†è¯å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 100):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–BPEåˆ†è¯å™¨\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: ç›®æ ‡è¯æ±‡è¡¨å¤§å°\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}  # è¯æ±‡è¡¨: æ ‡è®° -> ID\n",
    "        self.reverse_vocab = {}  # åå‘è¯æ±‡è¡¨: ID -> æ ‡è®°\n",
    "        self.bpe_ranks = {}  # BPEåˆå¹¶è§„åˆ™çš„ä¼˜å…ˆçº§\n",
    "        self.special_tokens = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<unk>\": 1,\n",
    "            \"<bos>\": 2,\n",
    "            \"<eos>\": 3\n",
    "        }\n",
    "        self.vocab_size_actual = len(self.special_tokens)\n",
    "        \n",
    "        # åˆå§‹åŒ–è¯æ±‡è¡¨ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token\n",
    "    \n",
    "    def _get_stats(self, pairs: Dict[Tuple[str, str], int]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡\"\"\"\n",
    "        stats = defaultdict(int)\n",
    "        for word, freq in pairs.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                stats[symbols[i], symbols[i + 1]] += freq\n",
    "        return stats\n",
    "    \n",
    "    def _merge_vocab(self, pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]:\n",
    "        \"\"\"åˆå¹¶æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹\"\"\"\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in v_in:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "    \n",
    "    def _word_to_pairs(self, word: str) -> Set[Tuple[str, str]]:\n",
    "        \"\"\"å°†å•è¯æ‹†åˆ†ä¸ºç›¸é‚»å­—èŠ‚å¯¹\"\"\"\n",
    "        symbols = word.split()\n",
    "        pairs = set()\n",
    "        if len(symbols) < 2:\n",
    "            return pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs.add((symbols[i], symbols[i + 1]))\n",
    "        return pairs\n",
    "    \n",
    "    def build_vocab(self, texts: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        ä»æ–‡æœ¬æ„å»ºBPEè¯æ±‡è¡¨\n",
    "        \n",
    "        Args:\n",
    "            texts: æ–‡æœ¬åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        # åˆå§‹åŒ–è¯æ±‡è¡¨ï¼ŒåŒ…å«æ‰€æœ‰å•ä¸ªå­—ç¬¦\n",
    "        token_counts = Counter()\n",
    "        for text in texts:\n",
    "            # å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå­—ç¬¦ï¼Œç”¨ç©ºæ ¼åˆ†éš”\n",
    "            words = [' '.join(list(text))]\n",
    "            for word in words:\n",
    "                token_counts[word] += 1\n",
    "        \n",
    "        # ç»Ÿè®¡åˆå§‹çš„å­—ç¬¦è¯æ±‡è¡¨\n",
    "        chars = set()\n",
    "        for text in texts:\n",
    "            chars.update(text)\n",
    "        \n",
    "        # æ·»åŠ å­—ç¬¦åˆ°è¯æ±‡è¡¨\n",
    "        for char in sorted(chars):\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = self.vocab_size_actual\n",
    "                self.reverse_vocab[self.vocab_size_actual] = char\n",
    "                self.vocab_size_actual += 1\n",
    "        \n",
    "        # å¼€å§‹BPEåˆå¹¶è¿‡ç¨‹\n",
    "        num_merges = self.vocab_size - self.vocab_size_actual\n",
    "        if num_merges <= 0:\n",
    "            return\n",
    "        \n",
    "        pairs = token_counts.copy()\n",
    "        for i in range(num_merges):\n",
    "            # è®¡ç®—æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡\n",
    "            stats = self._get_stats(pairs)\n",
    "            if not stats:\n",
    "                break\n",
    "            \n",
    "            # é€‰æ‹©æœ€é¢‘ç¹çš„å­—èŠ‚å¯¹\n",
    "            best = max(stats, key=stats.get)\n",
    "            \n",
    "            # è®°å½•åˆå¹¶è§„åˆ™çš„ä¼˜å…ˆçº§\n",
    "            self.bpe_ranks[best] = i\n",
    "            \n",
    "            # åˆå¹¶è¯æ±‡è¡¨ä¸­çš„å­—èŠ‚å¯¹\n",
    "            pairs = self._merge_vocab(best, pairs)\n",
    "            \n",
    "            # å°†æ–°åˆå¹¶çš„æ ‡è®°æ·»åŠ åˆ°è¯æ±‡è¡¨\n",
    "            new_token = ''.join(best)\n",
    "            if new_token not in self.vocab:\n",
    "                self.vocab[new_token] = self.vocab_size_actual\n",
    "                self.reverse_vocab[self.vocab_size_actual] = new_token\n",
    "                self.vocab_size_actual += 1\n",
    "    \n",
    "    def _get_pairs(self, word: List[str]) -> Set[Tuple[str, str]]:\n",
    "        \"\"\"è·å–å•è¯ä¸­æ‰€æœ‰ç›¸é‚»æ ‡è®°å¯¹\"\"\"\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "    \n",
    "    def bpe(self, token: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        å¯¹å•ä¸ªæ ‡è®°åº”ç”¨BPEç®—æ³•\n",
    "        \n",
    "        Args:\n",
    "            token: è¾“å…¥æ ‡è®°\n",
    "            \n",
    "        Returns:\n",
    "            BPEåˆ†è¯åçš„æ ‡è®°åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        if token in self.special_tokens:\n",
    "            return [token]\n",
    "        \n",
    "        word = list(token)\n",
    "        if len(word) == 0:\n",
    "            return []\n",
    "        if len(word) == 1:\n",
    "            return [word[0]]\n",
    "        \n",
    "        pairs = self._get_pairs(word)\n",
    "        \n",
    "        while True:\n",
    "            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜çš„å­—èŠ‚å¯¹\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            \n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "                \n",
    "                if i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            \n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = self._get_pairs(word)\n",
    "        \n",
    "        return word\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        å°†æ–‡æœ¬åˆ†è¯ä¸ºBPEæ ‡è®°\n",
    "        \n",
    "        Args:\n",
    "            text: è¾“å…¥æ–‡æœ¬\n",
    "            \n",
    "        Returns:\n",
    "            BPEæ ‡è®°åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for token in text.split():\n",
    "            if token in self.special_tokens:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(self.bpe(token))\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        å°†æ–‡æœ¬ç¼–ç ä¸ºBPEæ ‡è®°IDåˆ—è¡¨\n",
    "        \n",
    "        Args:\n",
    "            text: è¾“å…¥æ–‡æœ¬\n",
    "            add_special_tokens: æ˜¯å¦æ·»åŠ ç‰¹æ®Šæ ‡è®°\n",
    "            \n",
    "        Returns:\n",
    "            BPEæ ‡è®°IDåˆ—è¡¨\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        encoded = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            encoded = [self.vocab[\"<bos>\"]] + encoded + [self.vocab[\"<eos>\"]]\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        å°†BPEæ ‡è®°IDåˆ—è¡¨è§£ç ä¸ºæ–‡æœ¬\n",
    "        \n",
    "        Args:\n",
    "            ids: BPEæ ‡è®°IDåˆ—è¡¨\n",
    "            remove_special_tokens: æ˜¯å¦ç§»é™¤ç‰¹æ®Šæ ‡è®°\n",
    "            \n",
    "        Returns:\n",
    "            è§£ç åçš„æ–‡æœ¬\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            if idx in self.reverse_vocab:\n",
    "                token = self.reverse_vocab[idx]\n",
    "                if remove_special_tokens and token in self.special_tokens:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.append(\"<unk>\")\n",
    "        \n",
    "        # ç®€å•çš„åå¤„ç†ï¼šåˆå¹¶æ ‡è®°\n",
    "        text = ''.join(tokens)\n",
    "        return text\n",
    "\n",
    "\n",
    "# æµ‹è¯•ä»£ç \n",
    "def test_simple_bpe_tokenizer():\n",
    "    \"\"\"æµ‹è¯•ç®€å•å®ç°çš„BPEåˆ†è¯å™¨\"\"\"\n",
    "    print(\"===== æµ‹è¯•ç®€å•BPEåˆ†è¯å™¨ =====\")\n",
    "    \n",
    "    # ç¤ºä¾‹æ–‡æœ¬\n",
    "    texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"I hope you are doing well.\",\n",
    "        \"This is a test of the BPE tokenizer.\"\n",
    "    ]\n",
    "    \n",
    "    # åˆå§‹åŒ–BPEåˆ†è¯å™¨\n",
    "    tokenizer = SimpleBPETokenizer(vocab_size=50)\n",
    "    \n",
    "    # æ„å»ºè¯æ±‡è¡¨\n",
    "    tokenizer.build_vocab(texts)\n",
    "    \n",
    "    print(f\"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size_actual}\")\n",
    "    print(f\"å‰10ä¸ªè¯æ±‡é¡¹: {list(tokenizer.vocab.items())[:10]}\")\n",
    "    \n",
    "    # æµ‹è¯•åˆ†è¯\n",
    "    sample_text = \"Hello, this is a test!\"\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    print(f\"\\nåˆ†è¯ç»“æœ: {tokens}\")\n",
    "    \n",
    "    # æµ‹è¯•ç¼–ç \n",
    "    encoded = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "    print(f\"ç¼–ç ç»“æœ: {encoded}\")\n",
    "    \n",
    "    # æµ‹è¯•è§£ç \n",
    "    decoded = tokenizer.decode(encoded, remove_special_tokens=True)\n",
    "    print(f\"è§£ç ç»“æœ: {decoded}\")\n",
    "    \n",
    "    # æµ‹è¯•åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬\n",
    "    unknown_text = \"This is a unicorn ğŸ¦„ test.\"\n",
    "    encoded_unknown = tokenizer.encode(unknown_text)\n",
    "    decoded_unknown = tokenizer.decode(encoded_unknown)\n",
    "    print(f\"\\nåŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: {unknown_text}\")\n",
    "    print(f\"ç¼–ç ç»“æœ: {encoded_unknown}\")\n",
    "    print(f\"è§£ç ç»“æœ: {decoded_unknown}\")\n",
    "    \n",
    "    print(\"\\næ‰€æœ‰æµ‹è¯•å®Œæˆ!\")\n",
    "\n",
    "\n",
    "def test_tiktoken_bpe():\n",
    "    \"\"\"æµ‹è¯•tiktokenåº“çš„BPEåˆ†è¯å™¨\"\"\"\n",
    "    print(\"\\n===== æµ‹è¯•tiktoken BPEåˆ†è¯å™¨ =====\")\n",
    "    \n",
    "    # åˆå§‹åŒ–tiktoken BPEåˆ†è¯å™¨\n",
    "    try:\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    except KeyError:\n",
    "        # å¦‚æœæ²¡æœ‰å®‰è£…gpt2ç¼–ç ï¼Œå°è¯•ä½¿ç”¨cl100k_base (ç”¨äºtext-embedding-ada-002)\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    print(f\"è¯æ±‡è¡¨å¤§å°: {tokenizer.n_vocab}\")\n",
    "    \n",
    "    # æµ‹è¯•åˆ†è¯\n",
    "    sample_text = \"Hello, this is a test!\"\n",
    "    tokens = tokenizer.encode(sample_text)\n",
    "    print(f\"\\nç¼–ç ç»“æœ (ID): {tokens}\")\n",
    "    \n",
    "    # è½¬æ¢IDä¸ºå­—èŠ‚\n",
    "    token_bytes = [tokenizer.decode_single_token_bytes(token) for token in tokens]\n",
    "    print(f\"ç¼–ç ç»“æœ (å­—èŠ‚): {token_bytes}\")\n",
    "    \n",
    "    # æµ‹è¯•è§£ç \n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"è§£ç ç»“æœ: {decoded}\")\n",
    "    \n",
    "    # æµ‹è¯•åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬\n",
    "    unknown_text = \"This is a unicorn ğŸ¦„ test.\"\n",
    "    encoded_unknown = tokenizer.encode(unknown_text)\n",
    "    decoded_unknown = tokenizer.decode(encoded_unknown)\n",
    "    print(f\"\\nåŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: {unknown_text}\")\n",
    "    print(f\"ç¼–ç ç»“æœ: {encoded_unknown}\")\n",
    "    print(f\"è§£ç ç»“æœ: {decoded_unknown}\")\n",
    "    \n",
    "    # è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡\n",
    "    print(f\"\\næ–‡æœ¬ '{sample_text}' çš„tokenæ•°é‡: {len(tokens)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_simple_bpe_tokenizer()\n",
    "    test_tiktoken_bpe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c3578",
   "metadata": {},
   "source": [
    "\n",
    "    ===== æµ‹è¯•ç®€å•BPEåˆ†è¯å™¨ =====\n",
    "    è¯æ±‡è¡¨å¤§å°: 46\n",
    "    å‰10ä¸ªè¯æ±‡é¡¹: [('<pad>', 0), ('<unk>', 1), ('<bos>', 2), ('<eos>', 3), (' ', 4), (',', 5), ('.', 6), ('?', 7), ('B', 8), ('E', 9)]\n",
    "    \n",
    "    åˆ†è¯ç»“æœ: ['Hello,', 't', 'h', 'is', 'is', 'a', 't', 'e', 's', 't', '!']\n",
    "    ç¼–ç ç»“æœ: [2, 44, 28, 19, 41, 41, 14, 28, 16, 27, 28, 1, 3]\n",
    "    è§£ç ç»“æœ: Hello,thisisatest\n",
    "    \n",
    "    åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: This is a unicorn ğŸ¦„ test.\n",
    "    ç¼–ç ç»“æœ: [13, 19, 41, 41, 14, 29, 23, 20, 1, 24, 26, 23, 1, 28, 16, 27, 28, 6]\n",
    "    è§£ç ç»“æœ: Thisisauni<unk>orn<unk>test.\n",
    "    \n",
    "    æ‰€æœ‰æµ‹è¯•å®Œæˆ!\n",
    "    \n",
    "    ===== æµ‹è¯•tiktoken BPEåˆ†è¯å™¨ =====\n",
    "    è¯æ±‡è¡¨å¤§å°: 50257\n",
    "    \n",
    "    ç¼–ç ç»“æœ (ID): [15496, 11, 428, 318, 257, 1332, 0]\n",
    "    ç¼–ç ç»“æœ (å­—èŠ‚): [b'Hello', b',', b' this', b' is', b' a', b' test', b'!']\n",
    "    è§£ç ç»“æœ: Hello, this is a test!\n",
    "    \n",
    "    åŒ…å«æœªçŸ¥è¯æ±‡çš„æ–‡æœ¬: This is a unicorn ğŸ¦„ test.\n",
    "    ç¼–ç ç»“æœ: [1212, 318, 257, 44986, 12520, 99, 226, 1332, 13]\n",
    "    è§£ç ç»“æœ: This is a unicorn ğŸ¦„ test.\n",
    "    \n",
    "    æ–‡æœ¬ 'Hello, this is a test!' çš„tokenæ•°é‡: 7\n",
    "    \n",
    "\n",
    "## 6.ä½¿ç”¨æ»‘åŠ¨çª—å£è¿›è¡Œæ•°æ®é‡‡æ ·\n",
    "\n",
    "LLMs é€šè¿‡ â€œä¸‹ä¸€ä¸ªè¯é¢„æµ‹â€ ä»»åŠ¡é¢„è®­ç»ƒï¼Œå³ç»™å®šè¾“å…¥æ–‡æœ¬å—ï¼Œé¢„æµ‹ç´§éšå…¶åçš„ä¸‹ä¸€ä¸ªè¯ï¼ˆå¦‚å›¾ 2.12 æ‰€ç¤ºï¼‰ã€‚ä¸ºç”Ÿæˆè®­ç»ƒæ‰€éœ€çš„è¾“å…¥ - ç›®æ ‡å¯¹ï¼Œéœ€é‡‡ç”¨ â€œæ»‘åŠ¨çª—å£â€ æ–¹æ³•ï¼šåœ¨åˆ†è¯åçš„æ–‡æœ¬ä¸Šæ»‘åŠ¨å›ºå®šå¤§å°çš„çª—å£ï¼Œçª—å£å†…çš„æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼Œçª—å£å³ä¾§ç´§é‚»çš„è¯ä½œä¸ºç›®æ ‡ï¼ˆè¾“å…¥å³ç§»ä¸€ä½å³ä¸ºç›®æ ‡ï¼‰ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œè‹¥çª—å£å¤§å°ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰ä¸º 4ï¼Œè¾“å…¥ä¸º [æ ‡è®° 1, æ ‡è®° 2, æ ‡è®° 3, æ ‡è®° 4]ï¼Œåˆ™ç›®æ ‡ä¸º [æ ‡è®° 2, æ ‡è®° 3, æ ‡è®° 4, æ ‡è®° 5]ã€‚ä¸ºé«˜æ•ˆç”Ÿæˆæ‰¹é‡æ ·æœ¬ï¼Œå¯ç»“åˆ PyTorch çš„ Dataset å’Œ DataLoaderï¼šDataset è´Ÿè´£æŒ‰çª—å£æ»‘åŠ¨ç”Ÿæˆè¾“å…¥ - ç›®æ ‡å¯¹ï¼ŒDataLoader åˆ™å°†è¿™äº›å¯¹æ‰“åŒ…ä¸ºæ‰¹æ¬¡ï¼ˆå¦‚å›¾ 2.13 æ‰€ç¤ºï¼‰ï¼Œæ”¯æŒè®¾ç½®æ‰¹æ¬¡å¤§å°ã€æ­¥é•¿ï¼ˆçª—å£ç§»åŠ¨è·ç¦»ï¼‰ç­‰å‚æ•°ã€‚\n",
    "\n",
    "æ­¥é•¿å†³å®šæ ·æœ¬é‡å ç¨‹åº¦ï¼šæ­¥é•¿ä¸º 1 æ—¶ï¼Œç›¸é‚»æ ·æœ¬é«˜åº¦é‡å ï¼›æ­¥é•¿ç­‰äºçª—å£å¤§å°æ—¶ï¼Œæ ·æœ¬æ— é‡å ã€‚åˆç†è®¾ç½®æ­¥é•¿å¯å¹³è¡¡æ•°æ®åˆ©ç”¨ç‡ä¸è¿‡æ‹Ÿåˆé£é™©ï¼Œæœ€ç»ˆç”Ÿæˆçš„è¾“å…¥ - ç›®æ ‡å¯¹ä»¥å¼ é‡å½¢å¼è¾“å…¥æ¨¡å‹ï¼Œä¸ºè®­ç»ƒæä¾›æ•°æ®æ”¯æŒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d7553",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class TextWindowDataset(Dataset):\n",
    "    \"\"\"ä½¿ç”¨æ»‘åŠ¨çª—å£å¯¹æ–‡æœ¬è¿›è¡Œé‡‡æ ·çš„æ•°æ®é›†\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 text: List[int], \n",
    "                 context_length: int, \n",
    "                 stride: int = 1,\n",
    "                 pad_id: int = 0):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ–‡æœ¬çª—å£æ•°æ®é›†\n",
    "        \n",
    "        Args:\n",
    "            text: å·²ç¼–ç çš„æ–‡æœ¬ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰\n",
    "            context_length: ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆçª—å£å¤§å°ï¼‰\n",
    "            stride: æ»‘åŠ¨çª—å£çš„æ­¥é•¿ï¼Œé»˜è®¤ä¸º1\n",
    "            pad_id: å¡«å……æ ‡è®°çš„ID\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.context_length = context_length\n",
    "        self.stride = stride\n",
    "        self.pad_id = pad_id\n",
    "        \n",
    "        # è®¡ç®—æœ‰æ•ˆæ ·æœ¬æ•°é‡\n",
    "        self.num_samples = max(0, (len(text) - context_length) // stride + 1)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"è¿”å›æ•°æ®é›†çš„æ ·æœ¬æ•°é‡\"\"\"\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        è·å–å•ä¸ªæ ·æœ¬\n",
    "        \n",
    "        Args:\n",
    "            idx: æ ·æœ¬ç´¢å¼•\n",
    "            \n",
    "        Returns:\n",
    "            å…ƒç»„(inputs, targets)ï¼Œå…¶ä¸­inputsæ˜¯è¾“å…¥åºåˆ—ï¼Œtargetsæ˜¯ç›®æ ‡åºåˆ—\n",
    "        \"\"\"\n",
    "        # è®¡ç®—çª—å£èµ·å§‹ä½ç½®\n",
    "        start = idx * self.stride\n",
    "        \n",
    "        # ç¡®ä¿çª—å£ä¸è¶…å‡ºæ–‡æœ¬é•¿åº¦\n",
    "        end = start + self.context_length\n",
    "        if end > len(self.text):\n",
    "            # æˆªå–æœ€åå¯èƒ½çš„æœ‰æ•ˆçª—å£\n",
    "            end = len(self.text)\n",
    "            start = end - self.context_length\n",
    "        \n",
    "        # æå–è¾“å…¥åºåˆ—å’Œç›®æ ‡åºåˆ—\n",
    "        inputs = self.text[start:end]\n",
    "        targets = self.text[start+1:end+1]  # ç›®æ ‡æ˜¯è¾“å…¥çš„ä¸‹ä¸€ä¸ªæ ‡è®°\n",
    "        \n",
    "        # å¦‚æœç›®æ ‡åºåˆ—é•¿åº¦ä¸è¶³ï¼Œç”¨pad_idå¡«å……\n",
    "        if len(targets) < self.context_length:\n",
    "            targets = targets + [self.pad_id] * (self.context_length - len(targets))\n",
    "        \n",
    "        # è½¬æ¢ä¸ºå¼ é‡\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def create_data_loader(text: List[int], \n",
    "                       context_length: int, \n",
    "                       batch_size: int, \n",
    "                       stride: int = 1,\n",
    "                       shuffle: bool = False) -> DataLoader:\n",
    "    \"\"\"\n",
    "    åˆ›å»ºæ–‡æœ¬çª—å£æ•°æ®åŠ è½½å™¨\n",
    "    \n",
    "    Args:\n",
    "        text: å·²ç¼–ç çš„æ–‡æœ¬ï¼ˆæ•´æ•°åˆ—è¡¨ï¼‰\n",
    "        context_length: ä¸Šä¸‹æ–‡é•¿åº¦\n",
    "        batch_size: æ‰¹æ¬¡å¤§å°\n",
    "        stride: æ»‘åŠ¨çª—å£æ­¥é•¿\n",
    "        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®\n",
    "        \n",
    "    Returns:\n",
    "        æ•°æ®åŠ è½½å™¨\n",
    "    \"\"\"\n",
    "    dataset = TextWindowDataset(\n",
    "        text=text,\n",
    "        context_length=context_length,\n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# æµ‹è¯•ä»£ç \n",
    "def test_sliding_window():\n",
    "    \"\"\"æµ‹è¯•æ»‘åŠ¨çª—å£æ•°æ®é‡‡æ ·\"\"\"\n",
    "    print(\"===== æµ‹è¯•æ»‘åŠ¨çª—å£æ•°æ®é‡‡æ · =====\")\n",
    "    \n",
    "    # ç¤ºä¾‹æ–‡æœ¬ï¼ˆå·²ç¼–ç ï¼‰\n",
    "    encoded_text = [101, 102, 103, 104, 105, 106, 107, 108, 109, 110]\n",
    "    print(f\"åŸå§‹æ–‡æœ¬: {encoded_text}\")\n",
    "    \n",
    "    # å‚æ•°è®¾ç½®\n",
    "    context_length = 4\n",
    "    stride = 2\n",
    "    batch_size = 2\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®é›†\n",
    "    dataset = TextWindowDataset(\n",
    "        text=encoded_text,\n",
    "        context_length=context_length,\n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    # æ‰“å°æ•°æ®é›†ä¿¡æ¯\n",
    "    print(f\"\\næ•°æ®é›†å¤§å°: {len(dataset)}\")\n",
    "    \n",
    "    # æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬\n",
    "    print(\"\\n--- æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬ ---\")\n",
    "    for i in range(min(3, len(dataset))):\n",
    "        inputs, targets = dataset[i]\n",
    "        print(f\"æ ·æœ¬ {i}:\")\n",
    "        print(f\"  è¾“å…¥: {inputs}\")\n",
    "        print(f\"  ç›®æ ‡: {targets}\")\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "    data_loader = create_data_loader(\n",
    "        text=encoded_text,\n",
    "        context_length=context_length,\n",
    "        batch_size=batch_size,\n",
    "        stride=stride,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # æµ‹è¯•æ‰¹æ¬¡æ•°æ®\n",
    "    print(\"\\n--- æµ‹è¯•æ‰¹æ¬¡æ•°æ® ---\")\n",
    "    for i, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "        print(f\"æ‰¹æ¬¡ {i}:\")\n",
    "        print(f\"  è¾“å…¥å½¢çŠ¶: {batch_inputs.shape}\")\n",
    "        print(f\"  è¾“å…¥æ•°æ®:\")\n",
    "        print(batch_inputs)\n",
    "        print(f\"  ç›®æ ‡å½¢çŠ¶: {batch_targets.shape}\")\n",
    "        print(f\"  ç›®æ ‡æ•°æ®:\")\n",
    "        print(batch_targets)\n",
    "    \n",
    "    # æµ‹è¯•ä¸åŒæ­¥é•¿\n",
    "    print(\"\\n--- æµ‹è¯•ä¸åŒæ­¥é•¿ ---\")\n",
    "    for stride in [1, 2, 3]:\n",
    "        dataset = TextWindowDataset(\n",
    "            text=encoded_text,\n",
    "            context_length=context_length,\n",
    "            stride=stride\n",
    "        )\n",
    "        print(f\"æ­¥é•¿ä¸º {stride} æ—¶çš„æ ·æœ¬æ•°: {len(dataset)}\")\n",
    "        \n",
    "        # æ‰“å°å‰ä¸¤ä¸ªæ ·æœ¬\n",
    "        if len(dataset) > 0:\n",
    "            inputs, targets = dataset[0]\n",
    "            print(f\"  ç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å…¥: {inputs}\")\n",
    "            print(f\"  ç¬¬ä¸€ä¸ªæ ·æœ¬ç›®æ ‡: {targets}\")\n",
    "        \n",
    "        if len(dataset) > 1:\n",
    "            inputs, targets = dataset[1]\n",
    "            print(f\"  ç¬¬äºŒä¸ªæ ·æœ¬è¾“å…¥: {inputs}\")\n",
    "            print(f\"  ç¬¬äºŒä¸ªæ ·æœ¬ç›®æ ‡: {targets}\")\n",
    "if __name__ == \"__main__\":\n",
    "    test_sliding_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d690f9f9",
   "metadata": {},
   "source": [
    "\n",
    "    ===== æµ‹è¯•æ»‘åŠ¨çª—å£æ•°æ®é‡‡æ · =====\n",
    "    åŸå§‹æ–‡æœ¬: [101, 102, 103, 104, 105, 106, 107, 108, 109, 110]\n",
    "    \n",
    "    æ•°æ®é›†å¤§å°: 4\n",
    "    \n",
    "    --- æµ‹è¯•è·å–å•ä¸ªæ ·æœ¬ ---\n",
    "    æ ·æœ¬ 0:\n",
    "      è¾“å…¥: tensor([101, 102, 103, 104])\n",
    "      ç›®æ ‡: tensor([102, 103, 104, 105])\n",
    "    æ ·æœ¬ 1:\n",
    "      è¾“å…¥: tensor([103, 104, 105, 106])\n",
    "      ç›®æ ‡: tensor([104, 105, 106, 107])\n",
    "    æ ·æœ¬ 2:\n",
    "      è¾“å…¥: tensor([105, 106, 107, 108])\n",
    "      ç›®æ ‡: tensor([106, 107, 108, 109])\n",
    "    \n",
    "    --- æµ‹è¯•æ‰¹æ¬¡æ•°æ® ---\n",
    "    æ‰¹æ¬¡ 0:\n",
    "      è¾“å…¥å½¢çŠ¶: torch.Size([2, 4])\n",
    "      è¾“å…¥æ•°æ®:\n",
    "    tensor([[101, 102, 103, 104],\n",
    "            [103, 104, 105, 106]])\n",
    "      ç›®æ ‡å½¢çŠ¶: torch.Size([2, 4])\n",
    "      ç›®æ ‡æ•°æ®:\n",
    "    tensor([[102, 103, 104, 105],\n",
    "            [104, 105, 106, 107]])\n",
    "    æ‰¹æ¬¡ 1:\n",
    "      è¾“å…¥å½¢çŠ¶: torch.Size([2, 4])\n",
    "      è¾“å…¥æ•°æ®:\n",
    "    tensor([[105, 106, 107, 108],\n",
    "            [107, 108, 109, 110]])\n",
    "      ç›®æ ‡å½¢çŠ¶: torch.Size([2, 4])\n",
    "      ç›®æ ‡æ•°æ®:\n",
    "    tensor([[106, 107, 108, 109],\n",
    "            [108, 109, 110,   0]])\n",
    "    \n",
    "    --- æµ‹è¯•ä¸åŒæ­¥é•¿ ---\n",
    "    æ­¥é•¿ä¸º 1 æ—¶çš„æ ·æœ¬æ•°: 7\n",
    "      ç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å…¥: tensor([101, 102, 103, 104])\n",
    "      ç¬¬ä¸€ä¸ªæ ·æœ¬ç›®æ ‡: tensor([102, 103, 104, 105])\n",
    "      ç¬¬äºŒä¸ªæ ·æœ¬è¾“å…¥: tensor([102, 103, 104, 105])\n",
    "      ç¬¬äºŒä¸ªæ ·æœ¬ç›®æ ‡: tensor([103, 104, 105, 106])\n",
    "    æ­¥é•¿ä¸º 2 æ—¶çš„æ ·æœ¬æ•°: 4\n",
    "      ç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å…¥: tensor([101, 102, 103, 104])\n",
    "      ç¬¬ä¸€ä¸ªæ ·æœ¬ç›®æ ‡: tensor([102, 103, 104, 105])\n",
    "      ç¬¬äºŒä¸ªæ ·æœ¬è¾“å…¥: tensor([103, 104, 105, 106])\n",
    "      ç¬¬äºŒä¸ªæ ·æœ¬ç›®æ ‡: tensor([104, 105, 106, 107])\n",
    "    æ­¥é•¿ä¸º 3 æ—¶çš„æ ·æœ¬æ•°: 3\n",
    "      ç¬¬ä¸€ä¸ªæ ·æœ¬è¾“å…¥: tensor([101, 102, 103, 104])\n",
    "      ç¬¬ä¸€ä¸ªæ ·æœ¬ç›®æ ‡: tensor([102, 103, 104, 105])\n",
    "      ç¬¬äºŒä¸ªæ ·æœ¬è¾“å…¥: tensor([104, 105, 106, 107])\n",
    "      ç¬¬äºŒä¸ªæ ·æœ¬ç›®æ ‡: tensor([105, 106, 107, 108])\n",
    "    \n",
    "\n",
    "## 7.åˆ›å»ºè¯åµŒå…¥\n",
    "\n",
    "è¯åµŒå…¥æ˜¯å°†æ ‡è®° ID è½¬æ¢ä¸ºè¿ç»­å‘é‡çš„è¿‡ç¨‹ï¼Œæ˜¯ LLM å¤„ç†æ–‡æœ¬çš„æœ€ç»ˆæ•°å€¼å½¢å¼ï¼ˆå¦‚å›¾ 2.15 æ‰€ç¤ºï¼‰ã€‚ç¥ç»ç½‘ç»œéœ€è¿ç»­å‘é‡è¿›è¡Œè¿ç®—ï¼Œå› æ­¤éœ€é€šè¿‡ â€œåµŒå…¥å±‚â€ å®ç°è¿™ä¸€è½¬æ¢ã€‚åµŒå…¥å±‚æœ¬è´¨æ˜¯ä¸€ä¸ªæƒé‡çŸ©é˜µï¼šè¡Œæ•°ç­‰äºè¯æ±‡è¡¨å¤§å°ï¼Œåˆ—æ•°ä¸ºåµŒå…¥ç»´åº¦ï¼ˆå¦‚ 3 ç»´ã€256 ç»´ï¼‰ï¼Œæ¯ä¸ªæ ‡è®° ID å¯¹åº”çŸ©é˜µä¸­çš„ä¸€è¡Œï¼Œå³è¯¥æ ‡è®°çš„åµŒå…¥å‘é‡ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œè¯æ±‡è¡¨å¤§å°ä¸º 6ã€åµŒå…¥ç»´åº¦ä¸º 3 çš„åµŒå…¥å±‚ï¼Œå…¶æƒé‡çŸ©é˜µä¸º 6Ã—3 çš„éšæœºåˆå§‹åŒ–çŸ©é˜µï¼ˆè®­ç»ƒä¸­ä¼šä¼˜åŒ–ï¼‰ã€‚å½“è¾“å…¥æ ‡è®° ID ä¸º [2, 3, 5, 1] æ—¶ï¼ŒåµŒå…¥å±‚ä¼šæå–çŸ©é˜µä¸­å¯¹åº”è¡Œï¼Œå¾—åˆ° 4Ã—3 çš„åµŒå…¥å‘é‡çŸ©é˜µã€‚\n",
    "\n",
    "åœ¨ PyTorch ä¸­ï¼Œå¯é€šè¿‡ torch.nn.Embedding å®ç°åµŒå…¥å±‚ï¼Œå…¶æ ¸å¿ƒæ˜¯ â€œæŸ¥æ‰¾æ“ä½œâ€â€”â€” æ ¹æ® ID å¿«é€Ÿæ£€ç´¢å¯¹åº”åµŒå…¥å‘é‡ã€‚åµŒå…¥ç»´åº¦éœ€æƒè¡¡ï¼šæ›´é«˜ç»´åº¦ï¼ˆå¦‚ GPT-3 çš„ 12288 ç»´ï¼‰èƒ½æ•æ‰æ›´å¤šè¯­ä¹‰ï¼Œä½†è®¡ç®—æˆæœ¬æ›´é«˜ï¼›è¾ƒä½ç»´åº¦ï¼ˆå¦‚ 256 ç»´ï¼‰é€‚åˆå®éªŒã€‚åµŒå…¥å±‚çš„æƒé‡ä¼šé€šè¿‡è®­ç»ƒä¸æ–­ä¼˜åŒ–ï¼Œä½¿å‘é‡èƒ½æ›´å¥½åœ°è¡¨ç¤ºæ ‡è®°çš„è¯­ä¹‰å’Œä¸Šä¸‹æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b97b00",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"è¯æ±‡è¡¨ç®¡ç†ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–è¯æ±‡è¡¨\n",
    "        \n",
    "        Args:\n",
    "            special_tokens: ç‰¹æ®Šæ ‡è®°åŠå…¶IDçš„å­—å…¸\n",
    "        \"\"\"\n",
    "        self.token_to_idx = special_tokens or {\n",
    "            \"<pad>\": 0,\n",
    "            \"<unk>\": 1,\n",
    "            \"<bos>\": 2,\n",
    "            \"<eos>\": 3\n",
    "        }\n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        self.vocab_size = len(self.token_to_idx)\n",
    "    \n",
    "    def add_token(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        æ·»åŠ æ ‡è®°åˆ°è¯æ±‡è¡¨\n",
    "        \n",
    "        Args:\n",
    "            token: è¦æ·»åŠ çš„æ ‡è®°\n",
    "            \n",
    "        Returns:\n",
    "            æ ‡è®°çš„ID\n",
    "        \"\"\"\n",
    "        if token not in self.token_to_idx:\n",
    "            self.token_to_idx[token] = self.vocab_size\n",
    "            self.idx_to_token[self.vocab_size] = token\n",
    "            self.vocab_size += 1\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def build_from_texts(self, texts: List[List[str]]) -> None:\n",
    "        \"\"\"\n",
    "        ä»æ–‡æœ¬åˆ—è¡¨æ„å»ºè¯æ±‡è¡¨\n",
    "        \n",
    "        Args:\n",
    "            texts: æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬æ˜¯æ ‡è®°åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                self.add_token(token)\n",
    "    \n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        å°†æ ‡è®°åˆ—è¡¨ç¼–ç ä¸ºIDåˆ—è¡¨\n",
    "        \n",
    "        Args:\n",
    "            tokens: æ ‡è®°åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            IDåˆ—è¡¨\n",
    "        \"\"\"\n",
    "        return [self.token_to_idx.get(token, self.token_to_idx[\"<unk>\"]) for token in tokens]\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        å°†IDåˆ—è¡¨è§£ç ä¸ºæ ‡è®°åˆ—è¡¨\n",
    "        \n",
    "        Args:\n",
    "            ids: IDåˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            æ ‡è®°åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        return [self.idx_to_token.get(idx, \"<unk>\") for idx in ids]\n",
    "\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    \"\"\"è¿ç»­è¯è¢‹æ¨¡å‹(CBOW)çš„æ•°æ®é›†\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[List[str]], vocab: Vocabulary, context_size: int = 2):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–CBOWæ•°æ®é›†\n",
    "        \n",
    "        Args:\n",
    "            texts: æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬æ˜¯æ ‡è®°åˆ—è¡¨\n",
    "            vocab: è¯æ±‡è¡¨\n",
    "            context_size: ä¸Šä¸‹æ–‡å¤§å°(æ¯ä¾§çš„è¯æ•°)\n",
    "        \"\"\"\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "        \n",
    "        # æ„å»ºè®­ç»ƒæ ·æœ¬\n",
    "        for text in texts:\n",
    "            encoded_text = vocab.encode(text)\n",
    "            for i in range(context_size, len(encoded_text) - context_size):\n",
    "                context = []\n",
    "                # æ”¶é›†å·¦å³ä¸Šä¸‹æ–‡\n",
    "                for j in range(-context_size, context_size + 1):\n",
    "                    if j != 0:  # è·³è¿‡ä¸­å¿ƒè¯\n",
    "                        context.append(encoded_text[i + j])\n",
    "                target = encoded_text[i]  # ä¸­å¿ƒè¯\n",
    "                self.data.append((context, target))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"è¿”å›æ•°æ®é›†å¤§å°\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        è·å–å•ä¸ªæ ·æœ¬\n",
    "        \n",
    "        Args:\n",
    "            idx: æ ·æœ¬ç´¢å¼•\n",
    "            \n",
    "        Returns:\n",
    "            å…ƒç»„(context, target)ï¼Œå…¶ä¸­contextæ˜¯ä¸Šä¸‹æ–‡è¯IDï¼Œtargetæ˜¯ç›®æ ‡è¯ID\n",
    "        \"\"\"\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    \"\"\"è¿ç»­è¯è¢‹æ¨¡å‹(CBOW)\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–CBOWæ¨¡å‹\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: è¯æ±‡è¡¨å¤§å°\n",
    "            embedding_dim: åµŒå…¥ç»´åº¦\n",
    "        \"\"\"\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­\n",
    "        \n",
    "        Args:\n",
    "            inputs: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º[batch_size, context_size*2]\n",
    "            \n",
    "        Returns:\n",
    "            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ä¸º[batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        # è·å–ä¸Šä¸‹æ–‡è¯çš„åµŒå…¥\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # å¯¹ä¸Šä¸‹æ–‡åµŒå…¥å–å¹³å‡\n",
    "        context_mean = torch.mean(embeds, dim=1)\n",
    "        # é€šè¿‡çº¿æ€§å±‚é¢„æµ‹ä¸­å¿ƒè¯\n",
    "        output = self.linear(context_mean)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_cbow_model(texts: List[List[str]], embedding_dim: int = 100, \n",
    "                     context_size: int = 2, epochs: int = 10, \n",
    "                     batch_size: int = 32, lr: float = 0.01) -> nn.Embedding:\n",
    "    \"\"\"\n",
    "    è®­ç»ƒCBOWæ¨¡å‹å¹¶è¿”å›è¯åµŒå…¥\n",
    "    \n",
    "    Args:\n",
    "        texts: æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬æ˜¯æ ‡è®°åˆ—è¡¨\n",
    "        embedding_dim: åµŒå…¥ç»´åº¦\n",
    "        context_size: ä¸Šä¸‹æ–‡å¤§å°\n",
    "        epochs: è®­ç»ƒè½®æ•°\n",
    "        batch_size: æ‰¹æ¬¡å¤§å°\n",
    "        lr: å­¦ä¹ ç‡\n",
    "        \n",
    "    Returns:\n",
    "        è®­ç»ƒå¥½çš„è¯åµŒå…¥å±‚\n",
    "    \"\"\"\n",
    "    # æ„å»ºè¯æ±‡è¡¨\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_from_texts(texts)\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨\n",
    "    dataset = CBOWDataset(texts, vocab, context_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨\n",
    "    model = CBOW(vocab.vocab_size, embedding_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # è®­ç»ƒæ¨¡å‹\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in dataloader:\n",
    "            # å‰å‘ä¼ æ’­\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # åå‘ä¼ æ’­å’Œä¼˜åŒ–\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # è¿”å›è®­ç»ƒå¥½çš„è¯åµŒå…¥å±‚\n",
    "    return model.embeddings\n",
    "\n",
    "\n",
    "# æµ‹è¯•ä»£ç \n",
    "def test_word_embedding():\n",
    "    \"\"\"æµ‹è¯•è¯åµŒå…¥åŠŸèƒ½\"\"\"\n",
    "    print(\"===== æµ‹è¯•è¯åµŒå…¥ =====\")\n",
    "    \n",
    "    # ç¤ºä¾‹æ–‡æœ¬\n",
    "    texts = [\n",
    "        [\"I\", \"like\", \"to\", \"play\", \"football\"],\n",
    "        [\"Football\", \"is\", \"a\", \"popular\", \"sport\"],\n",
    "        [\"I\", \"enjoy\", \"watching\", \"football\", \"matches\"],\n",
    "        [\"Do\", \"you\", \"play\", \"any\", \"sports\"],\n",
    "        [\"Sports\", \"are\", \"good\", \"for\", \"health\"]\n",
    "    ]\n",
    "    \n",
    "    # è®­ç»ƒCBOWæ¨¡å‹è·å–è¯åµŒå…¥\n",
    "    embedding_dim = 10\n",
    "    context_size = 2\n",
    "    embeddings = train_cbow_model(\n",
    "        texts=texts,\n",
    "        embedding_dim=embedding_dim,\n",
    "        context_size=context_size,\n",
    "        epochs=50,\n",
    "        batch_size=4,\n",
    "        lr=0.01\n",
    "    )\n",
    "    \n",
    "    # è·å–è¯æ±‡è¡¨\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_from_texts(texts)\n",
    "    \n",
    "    # æµ‹è¯•è¯åµŒå…¥æŸ¥æ‰¾\n",
    "    test_words = [\"I\", \"football\", \"sports\", \"unknown\"]\n",
    "    print(\"\\nè¯åµŒå…¥ç¤ºä¾‹:\")\n",
    "    for word in test_words:\n",
    "        word_id = vocab.encode([word])[0]\n",
    "        word_vector = embeddings(torch.tensor(word_id, dtype=torch.long)).detach().numpy()\n",
    "        print(f\"{word}: {word_vector[:5]}... (shape: {word_vector.shape})\")\n",
    "    \n",
    "    # è®¡ç®—è¯ä¹‹é—´çš„ç›¸ä¼¼åº¦\n",
    "    print(\"\\nè¯ç›¸ä¼¼åº¦åˆ†æ:\")\n",
    "    target_words = [\"football\", \"sports\", \"play\"]\n",
    "    for target in target_words:\n",
    "        target_id = vocab.encode([target])[0]\n",
    "        target_vector = embeddings(torch.tensor(target_id, dtype=torch.long))\n",
    "        \n",
    "        print(f\"\\nä¸ '{target}' æœ€ç›¸ä¼¼çš„è¯:\")\n",
    "        similarities = []\n",
    "        for word, idx in vocab.token_to_idx.items():\n",
    "            if word in [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]:\n",
    "                continue\n",
    "            \n",
    "            word_vector = embeddings(torch.tensor(idx, dtype=torch.long))\n",
    "            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦\n",
    "            sim = torch.cosine_similarity(target_vector.unsqueeze(0), \n",
    "                                         word_vector.unsqueeze(0)).item()\n",
    "            similarities.append((word, sim))\n",
    "        \n",
    "        # æŒ‰ç›¸ä¼¼åº¦æ’åº\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # æ‰“å°å‰3ä¸ªç›¸ä¼¼è¯\n",
    "        for word, sim in similarities[:3]:\n",
    "            print(f\"  {word}: {sim:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_word_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1114d950",
   "metadata": {},
   "source": [
    "\n",
    "    ===== æµ‹è¯•è¯åµŒå…¥ =====\n",
    "    Epoch 1/50, Loss: 3.1725\n",
    "    Epoch 2/50, Loss: 3.2624\n",
    "    Epoch 3/50, Loss: 3.1252\n",
    "    Epoch 4/50, Loss: 2.7059\n",
    "    Epoch 5/50, Loss: 2.4706\n",
    "    Epoch 6/50, Loss: 2.3471\n",
    "    Epoch 7/50, Loss: 2.5184\n",
    "    Epoch 8/50, Loss: 2.4183\n",
    "    Epoch 9/50, Loss: 2.3172\n",
    "    Epoch 10/50, Loss: 2.2042\n",
    "    Epoch 11/50, Loss: 2.2335\n",
    "    Epoch 12/50, Loss: 1.9774\n",
    "    Epoch 13/50, Loss: 1.8652\n",
    "    Epoch 14/50, Loss: 1.7527\n",
    "    Epoch 15/50, Loss: 1.8568\n",
    "    Epoch 16/50, Loss: 1.6976\n",
    "    Epoch 17/50, Loss: 1.4242\n",
    "    Epoch 18/50, Loss: 1.2158\n",
    "    Epoch 19/50, Loss: 1.1381\n",
    "    Epoch 20/50, Loss: 1.0590\n",
    "    Epoch 21/50, Loss: 1.2977\n",
    "    Epoch 22/50, Loss: 1.3026\n",
    "    Epoch 23/50, Loss: 0.9900\n",
    "    Epoch 24/50, Loss: 0.7884\n",
    "    Epoch 25/50, Loss: 1.0041\n",
    "    Epoch 26/50, Loss: 0.8770\n",
    "    Epoch 27/50, Loss: 0.8645\n",
    "    Epoch 28/50, Loss: 0.7021\n",
    "    Epoch 29/50, Loss: 0.8588\n",
    "    Epoch 30/50, Loss: 0.6019\n",
    "    Epoch 31/50, Loss: 0.4728\n",
    "    Epoch 32/50, Loss: 0.5092\n",
    "    Epoch 33/50, Loss: 0.4671\n",
    "    Epoch 34/50, Loss: 0.4910\n",
    "    Epoch 35/50, Loss: 0.5640\n",
    "    Epoch 36/50, Loss: 0.5198\n",
    "    Epoch 37/50, Loss: 0.4731\n",
    "    Epoch 38/50, Loss: 0.4264\n",
    "    Epoch 39/50, Loss: 0.3248\n",
    "    Epoch 40/50, Loss: 0.2492\n",
    "    Epoch 41/50, Loss: 0.2738\n",
    "    Epoch 42/50, Loss: 0.3748\n",
    "    Epoch 43/50, Loss: 0.2021\n",
    "    Epoch 44/50, Loss: 0.2448\n",
    "    Epoch 45/50, Loss: 0.1978\n",
    "    Epoch 46/50, Loss: 0.2853\n",
    "    Epoch 47/50, Loss: 0.1764\n",
    "    Epoch 48/50, Loss: 0.1442\n",
    "    Epoch 49/50, Loss: 0.1716\n",
    "    Epoch 50/50, Loss: 0.2133\n",
    "    \n",
    "    è¯åµŒå…¥ç¤ºä¾‹:\n",
    "    I: [-0.0588957  -0.14068426 -0.7404043  -1.8865429  -2.6835012 ]... (shape: (10,))\n",
    "    football: [ 0.637025    0.14052066 -0.848007    0.2889565  -0.2740498 ]... (shape: (10,))\n",
    "    sports: [-1.045044   -0.70745003 -2.0171206   1.0361644   0.60308105]... (shape: (10,))\n",
    "    unknown: [-0.14042336  0.71746343  0.11502329 -0.5219049   0.10613117]... (shape: (10,))\n",
    "    \n",
    "    è¯ç›¸ä¼¼åº¦åˆ†æ:\n",
    "    \n",
    "    ä¸ 'football' æœ€ç›¸ä¼¼çš„è¯:\n",
    "      football: 1.0000\n",
    "      is: 0.7577\n",
    "      health: 0.6264\n",
    "    \n",
    "    ä¸ 'sports' æœ€ç›¸ä¼¼çš„è¯:\n",
    "      sports: 1.0000\n",
    "      health: 0.5747\n",
    "      is: 0.5725\n",
    "    \n",
    "    ä¸ 'play' æœ€ç›¸ä¼¼çš„è¯:\n",
    "      play: 1.0000\n",
    "      popular: 0.6973\n",
    "      is: 0.6605\n",
    "    \n",
    "\n",
    "## 8.ç¼–ç è¯ä½ç½®\n",
    "\n",
    "LLM çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶æœ¬èº«ä¸æ„ŸçŸ¥è¯çš„ä½ç½®ï¼Œå› æ­¤éœ€åŠ å…¥ â€œä½ç½®åµŒå…¥â€ ä»¥ä¼ è¾¾è¯åœ¨åºåˆ—ä¸­çš„é¡ºåºä¿¡æ¯ï¼ˆå¦‚å›¾ 2.17 æ‰€ç¤ºï¼‰ã€‚ä½ç½®åµŒå…¥æœ‰ä¸¤ç§ç±»å‹ï¼šç»å¯¹ä½ç½®åµŒå…¥ï¼ˆä¸å…·ä½“ä½ç½®ç»‘å®šï¼Œå¦‚ç¬¬ 1 ä¸ªè¯ç”¨ç‰¹å®šå‘é‡ï¼Œç¬¬ 2 ä¸ªè¯ç”¨å¦ä¸€å‘é‡ï¼‰å’Œç›¸å¯¹ä½ç½®åµŒå…¥ï¼ˆå…³æ³¨è¯ä¹‹é—´çš„è·ç¦»ï¼Œå¦‚ â€œç›¸è· 2 ä¸ªä½ç½®â€ï¼‰ã€‚\n",
    "\n",
    "GPT æ¨¡å‹é‡‡ç”¨ç»å¯¹ä½ç½®åµŒå…¥ï¼Œå…¶å®ç°æ–¹å¼æ˜¯ï¼šåˆ›å»ºå¦ä¸€ä¸ªåµŒå…¥å±‚ï¼Œè¾“å…¥ä¸º 0 è‡³ä¸Šä¸‹æ–‡é•¿åº¦ - 1 çš„ä½ç½®ç´¢å¼•ï¼ˆå¦‚çª—å£å¤§å°ä¸º 4 æ—¶ï¼Œè¾“å…¥ä¸º [0, 1, 2, 3]ï¼‰ï¼Œè¾“å‡ºä¸è¯åµŒå…¥ç»´åº¦ç›¸åŒçš„ä½ç½®å‘é‡ã€‚æœ€ç»ˆè¾“å…¥åµŒå…¥æ˜¯è¯åµŒå…¥ä¸ä½ç½®åµŒå…¥çš„æ€»å’Œï¼ˆå¦‚å›¾ 2.19 æ‰€ç¤ºï¼‰ï¼Œä¾‹å¦‚æŸè¯çš„åµŒå…¥å‘é‡ä¸º [1.2, -0.2, -0.1]ï¼Œå…¶ä½ç½®åµŒå…¥ä¸º [0.5, 0.3, 0.1]ï¼Œåˆ™æœ€ç»ˆè¾“å…¥ä¸º [1.7, 0.1, 0.0]ã€‚\n",
    "\n",
    "ä½ç½®åµŒå…¥çš„ç»´åº¦ä¸è¯åµŒå…¥ä¸€è‡´ï¼Œç¡®ä¿ä¸¤è€…å¯ç›´æ¥ç›¸åŠ ï¼Œä¸”ä¼šéšæ¨¡å‹è®­ç»ƒä¼˜åŒ–ï¼Œä½¿æ¨¡å‹èƒ½å­¦ä¹ åˆ°è¯åºå¯¹è¯­ä¹‰çš„å½±å“ï¼ˆå¦‚ â€œæˆ‘çˆ±ä½ â€ ä¸ â€œä½ çˆ±æˆ‘â€ çš„åŒºåˆ«ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee56d1f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "# 1åˆå§‹åŒ–åˆ†è¯å™¨ï¼ˆä½¿ç”¨GPT-2çš„BPEåˆ†è¯å™¨ï¼‰\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# å®šä¹‰åµŒå…¥å‚æ•°\n",
    "vocab_size = 50257  # GPT-2çš„è¯æ±‡è¡¨å¤§å°\n",
    "output_dim = 256    # åµŒå…¥ç»´åº¦ï¼ˆç¤ºä¾‹ç”¨256ï¼Œå®é™…GPT-3ä¸º12288ï¼‰\n",
    "context_length = 4  # ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå³è¾“å…¥åºåˆ—çš„æœ€å¤§é•¿åº¦ï¼‰\n",
    "\n",
    "# åˆ›å»ºè¯åµŒå…¥å±‚å’Œä½ç½®åµŒå…¥å±‚\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# ç”Ÿæˆä½ç½®ç´¢å¼•ï¼ˆ0åˆ°context_length-1ï¼‰\n",
    "pos_indices = torch.arange(context_length)  # å½¢çŠ¶: [4]\n",
    "pos_embeddings = pos_embedding_layer(pos_indices)  # å½¢çŠ¶: [4, 256]\n",
    "\n",
    "# æµ‹è¯•ï¼šå°†è¯IDè½¬æ¢ä¸ºåµŒå…¥å¹¶æ·»åŠ ä½ç½®åµŒå…¥\n",
    "def test_position_embedding():\n",
    "    # ç¤ºä¾‹è¾“å…¥è¯ID\n",
    "    input_ids = torch.tensor([\n",
    "        [40, 367, 2885, 1464],    # ç¬¬ä¸€å¥çš„è¯ID\n",
    "        [1807, 3619, 402, 271],   # ç¬¬äºŒå¥çš„è¯ID\n",
    "        [10899, 2138, 257, 7026]  # ç¬¬ä¸‰å¥çš„è¯ID\n",
    "    ])\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    print(f\"è¾“å…¥è¯IDå½¢çŠ¶: {input_ids.shape}\")  # åº”è¾“å‡º: torch.Size([3, 4])\n",
    "\n",
    "    # ç”Ÿæˆè¯åµŒå…¥\n",
    "    token_embeddings = token_embedding_layer(input_ids)\n",
    "    print(f\"è¯åµŒå…¥å½¢çŠ¶: {token_embeddings.shape}\")  # åº”è¾“å‡º: torch.Size([3, 4, 256])\n",
    "\n",
    "    # æ·»åŠ ä½ç½®åµŒå…¥\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    print(f\"æ·»åŠ ä½ç½®åµŒå…¥åçš„å½¢çŠ¶: {input_embeddings.shape}\")  # åº”è¾“å‡º: torch.Size([3, 4, 256])\n",
    "\n",
    "    # éªŒè¯ä½ç½®åµŒå…¥çš„å”¯ä¸€æ€§\n",
    "    print(\"\\nä½ç½®åµŒå…¥å‘é‡ï¼ˆå‰3ä¸ªä½ç½®çš„å‰5ç»´ï¼‰:\")\n",
    "    for i in range(3):\n",
    "        print(f\"ä½ç½® {i}: {pos_embeddings[i, :5]}\")\n",
    "\n",
    "# æ‰§è¡Œæµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    test_position_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf8e94",
   "metadata": {},
   "source": [
    "\n",
    "    è¾“å…¥è¯IDå½¢çŠ¶: torch.Size([3, 4])\n",
    "    è¯åµŒå…¥å½¢çŠ¶: torch.Size([3, 4, 256])\n",
    "    æ·»åŠ ä½ç½®åµŒå…¥åçš„å½¢çŠ¶: torch.Size([3, 4, 256])\n",
    "    \n",
    "    ä½ç½®åµŒå…¥å‘é‡ï¼ˆå‰3ä¸ªä½ç½®çš„å‰5ç»´ï¼‰:\n",
    "    ä½ç½® 0: tensor([-0.3552, -0.5629, -1.4778,  0.7029, -0.0278], grad_fn=<SliceBackward0>)\n",
    "    ä½ç½® 1: tensor([-0.7520,  0.3258,  0.5109, -1.2897,  0.2495], grad_fn=<SliceBackward0>)\n",
    "    ä½ç½® 2: tensor([-0.6930,  0.9321, -0.9753,  0.5288,  0.8013], grad_fn=<SliceBackward0>)\n",
    "    \n",
    "\n",
    "## è´¡çŒ®è€…ä¸»é¡µ\n",
    "\n",
    "\n",
    "|è´¡çŒ®è€…|å­¦æ ¡  | ç ”ç©¶æ–¹å‘           |   GitHubä¸»é¡µ |\n",
    "|-----------------|------------------------|-----------------------|------------|\n",
    "| è”¡é‹†æ· | ç¦å·å¤§å­¦  |    Computer Visionï¼ˆCVï¼‰ï¼ŒNatural Language Processingï¼ˆNLPï¼‰      |https://github.com/xinala-781|"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
