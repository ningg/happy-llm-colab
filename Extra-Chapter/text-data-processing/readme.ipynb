{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13a9fe8",
   "metadata": {},
   "source": [
    "# Extra-Chapter：text-data-processing\n",
    "\n",
    "本补充章节旨在对大模型数据处理做一个简单的梳理与具体的代码实现，帮助大家对于大模型的数据处理有一个更加清晰的认识。\n",
    "\n",
    "## 1.理解词嵌入\n",
    "\n",
    "在深度神经网络模型中，由于文本是分类数据，模型无法直接处理原始文本，因此需要将词语表示为连续值向量传输给模型。这一处理过程我们称之为词嵌入，其本质是将**离散对象**（如词语）映射到**连续向量**空间中的点，目的是将非数值数据转换为神经网络可处理的格式。\n",
    "\n",
    "通过将每个词映射为高维空间中的点（如“猫”=[0.2, -1.7, ...]），使语义相似的词（如“猫”和“狗”）向量距离更近，而无关词（如“猫”和“汽车”）距离更远。这种向量化表示既保留词语关系（“国王-男性+女性≈女王”），又能作为神经网络的输入，是大模型理解语言的基础。\n",
    "\n",
    "我们可以看一下下面的代码示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d5685",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import re\n",
    "\n",
    "# 示例文本（可自行修改并且测试）\n",
    "text = \"Hello, how are you today? I hope you are doing well.\"\n",
    "\n",
    "# 简单分词函数\n",
    "def tokenize(text):\n",
    "    # 按非字母数字字符分割文本，并保留分隔符\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    return tokens\n",
    "# 构建词汇表\n",
    "def build_vocab(tokens):\n",
    "    unique_tokens = sorted(set(tokens))\n",
    "    vocab = {token: i for i, token in enumerate(unique_tokens)}\n",
    "    vocab_size = len(vocab)\n",
    "    return vocab, vocab_size\n",
    "# 文本编码\n",
    "def encode_text(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "# 创建词嵌入模型\n",
    "class WordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordEmbeddingModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.embedding(inputs)\n",
    "\n",
    "# 可视化词向量\n",
    "def visualize_embeddings(embeddings, token_ids, vocab):\n",
    "    reverse_vocab = {i: token for token, i in vocab.items()}\n",
    "    pca = PCA(n_components=2)\n",
    "    embeddings_2d = pca.fit_transform(embeddings.detach().numpy())\n",
    "    # 绘制散点图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, token_id in enumerate(token_ids):\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], marker='o')\n",
    "        plt.annotate(reverse_vocab[token_id], \n",
    "                     (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                     xytext=(5, 2), \n",
    "                     textcoords='offset points',\n",
    "                     ha='right', \n",
    "                     va='bottom')\n",
    "    plt.title('Word Embeddings Visualization')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('word_embeddings.png')\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    tokens = tokenize(text)\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    tokens_with_special = tokens + ['<pad>', '<unk>']\n",
    "    \n",
    "    # 构建词汇表\n",
    "    vocab, vocab_size = build_vocab(tokens_with_special)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Vocabulary: {vocab}\")\n",
    "    \n",
    "    # 编码文本\n",
    "    encoded_text = encode_text(tokens, vocab)\n",
    "    print(f\"Encoded text: {encoded_text}\")\n",
    "    input_tensor = torch.tensor(encoded_text, dtype=torch.long)\n",
    "    \n",
    "    # 初始化模型\n",
    "    embedding_dim = 10  # 词向量维度，即提到的词空间\n",
    "    model = WordEmbeddingModel(vocab_size, embedding_dim)\n",
    "    # 获取词向量\n",
    "    embeddings = model(input_tensor)\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")  # 这里应该是 [序列长度, 嵌入维度]\n",
    "    \n",
    "    print(f\"Word vector for '{tokens[0]}': {embeddings[0].detach().numpy()}\")\n",
    "    print(f\"Word vector for '{tokens[1]}': {embeddings[1].detach().numpy()}\")\n",
    "    # 可视化词向量（仅用于低维演示，实际应用中词向量维度通常较高）\n",
    "    if embedding_dim >= 2:\n",
    "        visualize_embeddings(embeddings, encoded_text, vocab)\n",
    "    \n",
    "    # 计算词之间的相似度\n",
    "    print(\"\\n词相似度分析:\")\n",
    "    for i, token_i in enumerate(tokens[:5]):  # 只分析前5个词\n",
    "        for j, token_j in enumerate(tokens[:5]):\n",
    "            if i != j:\n",
    "                # 计算余弦相似度\n",
    "                sim = F.cosine_similarity(embeddings[i].unsqueeze(0), \n",
    "                                         embeddings[j].unsqueeze(0))\n",
    "                print(f\" '{token_i}' 和 '{token_j}' 的相似度: {sim.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c6e51",
   "metadata": {},
   "source": [
    "\n",
    "    Tokens: ['Hello', ',', 'how', 'are', 'you', 'today', '?', 'I', 'hope', 'you', 'are', 'doing', 'well', '.']\n",
    "    Vocabulary size: 14\n",
    "    Vocabulary: {',': 0, '.': 1, '<pad>': 2, '<unk>': 3, '?': 4, 'Hello': 5, 'I': 6, 'are': 7, 'doing': 8, 'hope': 9, 'how': 10, 'today': 11, 'well': 12, 'you': 13}\n",
    "    Encoded text: [5, 0, 10, 7, 13, 11, 4, 6, 9, 13, 7, 8, 12, 1]\n",
    "    Embeddings shape: torch.Size([14, 10])\n",
    "    Word vector for 'Hello': [-0.16787808 -0.46388683 -0.4728546   0.59449345 -0.23820949  0.34212282\n",
    "      0.6591729  -0.10877569  0.60686487 -1.771871  ]\n",
    "    Word vector for ',': [ 0.19194137 -1.2824519   1.1420391  -0.8361696  -0.578317    0.1025617\n",
    "      1.2452478  -0.08552601  0.9869009  -0.04940421]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "![png](./image/output_1_1.png)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    词相似度分析:\n",
    "     'Hello' 和 ',' 的相似度: 0.2132\n",
    "     'Hello' 和 'how' 的相似度: -0.0277\n",
    "     'Hello' 和 'are' 的相似度: 0.1024\n",
    "     'Hello' 和 'you' 的相似度: 0.1597\n",
    "     ',' 和 'Hello' 的相似度: 0.2132\n",
    "     ',' 和 'how' 的相似度: -0.0801\n",
    "     ',' 和 'are' 的相似度: -0.6096\n",
    "     ',' 和 'you' 的相似度: 0.2355\n",
    "     'how' 和 'Hello' 的相似度: -0.0277\n",
    "     'how' 和 ',' 的相似度: -0.0801\n",
    "     'how' 和 'are' 的相似度: 0.1170\n",
    "     'how' 和 'you' 的相似度: -0.0412\n",
    "     'are' 和 'Hello' 的相似度: 0.1024\n",
    "     'are' 和 ',' 的相似度: -0.6096\n",
    "     'are' 和 'how' 的相似度: 0.1170\n",
    "     'are' 和 'you' 的相似度: 0.2203\n",
    "     'you' 和 'Hello' 的相似度: 0.1597\n",
    "     'you' 和 ',' 的相似度: 0.2355\n",
    "     'you' 和 'how' 的相似度: -0.0412\n",
    "     'you' 和 'are' 的相似度: 0.2203\n",
    "    \n",
    "\n",
    "这下我们就能够很清楚的在散点图中看到不同词语之间的距离。\n",
    "\n",
    "## 2.分词\n",
    "\n",
    "分词是将输入文本拆分为单个标记（可以是单词、标点符号等）的过程，举个例子，我们可以把句子“Hello, world!”拆成一小块一小块的形式，如```[\"Hello\", \",\", \"world\", \"!\"]```，使计算机能够逐块处理语言，进一步转换为模型可处理的数值形式。\n",
    "\n",
    "我们可以从简单的分词方法入手，例如使用 Python 的正则表达式（re 库）按空白字符拆分文本，但这种方法可能导致标点符号与单词粘连（如 “Hello,” 中的逗号未分离）。\n",
    "\n",
    "为此，我们可以在代码当中修改正则表达式，在空白字符、逗号、句号等处进行拆分，使单词与标点符号成为独立列表项。\n",
    "\n",
    "同时，在处理的过程中需注意保留空白字符或移除冗余空白，例如处理 Python 代码时需保留缩进，而普通文本可移除。\n",
    "值得注意的是：通常不将文本转为小写，因为大小写有助于模型区分专有名词、理解句子结构。\n",
    "实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb43ce",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "from collections import defaultdict\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"简单分词器实现\"\"\"\n",
    "    def __init__(self, text=None):\n",
    "        self.vocab = {}\n",
    "        self.reverse_vocab = {}\n",
    "        self.vocab_size = 0\n",
    "        self.special_tokens = {\n",
    "            '<pad>': 0,\n",
    "            '<unk>': 1,\n",
    "            '<bos>': 2,\n",
    "            '<eos>': 3\n",
    "        }\n",
    "        if text:\n",
    "            self.build_vocab(text)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"将文本分割为标记列表\"\"\"\n",
    "        # 保留空白字符的分词\n",
    "        tokens = re.findall(r'\\S+|\\s+', text)\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocab(self, text):\n",
    "        \"\"\"构建词汇表\"\"\"\n",
    "        self.vocab = self.special_tokens.copy()\n",
    "        self.vocab_size = len(self.special_tokens)\n",
    "        tokens = self.tokenize(text)\n",
    "        token_counts = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            token_counts[token] += 1\n",
    "        sorted_tokens = sorted(token_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "        for token, _ in sorted_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = self.vocab_size\n",
    "                self.vocab_size += 1\n",
    "        self.reverse_vocab = {v: k for k, v in self.vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"将文本转换为标记ID序列\"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocab.get(token, self.vocab['<unk>']) for token in tokens]\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"将标记ID序列转换回文本\"\"\"\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.reverse_vocab:\n",
    "                tokens.append(self.reverse_vocab[token_id])\n",
    "            else:\n",
    "                tokens.append('<unk>')\n",
    "        text = ''\n",
    "        prev_token = None\n",
    "        for token in tokens:\n",
    "            if prev_token and prev_token.isspace():\n",
    "                text += token\n",
    "            else:\n",
    "                text = text.rstrip() + token\n",
    "            prev_token = token\n",
    "            \n",
    "        return text\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"基于字节对编码(BPE)的分词器\"\"\"\n",
    "    def __init__(self, model_name=\"gpt2\"):\n",
    "        \"\"\"\n",
    "        初始化BPE分词器\n",
    "        model_name: 模型名称，如\"gpt2\"或\"cl100k_base\"(OpenAI的text-embedding-ada-002使用)\n",
    "        \"\"\"\n",
    "        self.encoder = tiktoken.get_encoding(model_name)\n",
    "        self.vocab_size = self.encoder.n_vocab\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"将文本转换为BPE标记列表\"\"\"\n",
    "        token_ids = self.encoder.encode(text)\n",
    "        tokens = [self.encoder.decode_single_token_bytes(token_id) for token_id in token_ids]\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"将文本转换为BPE标记ID序列\"\"\"\n",
    "        return self.encoder.encode(text)\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"将BPE标记ID序列转换回文本\"\"\"\n",
    "        return self.encoder.decode(token_ids)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Hello, how are you today? I hope you are doing well.\"\n",
    "    print(\"=== 简单分词器 ===\")\n",
    "    simple_tokenizer = SimpleTokenizer(sample_text)\n",
    "    tokens = simple_tokenizer.tokenize(sample_text)\n",
    "    print(f\"分词结果: {tokens}\")\n",
    "    encoded = simple_tokenizer.encode(sample_text)\n",
    "    print(f\"编码结果: {encoded}\")\n",
    "    decoded = simple_tokenizer.decode(encoded)\n",
    "    print(f\"解码结果: {decoded}\")\n",
    "    print(f\"词汇表大小: {simple_tokenizer.vocab_size}\")\n",
    "    print(\"\\n=== BPE分词器 (GPT-2) ===\")\n",
    "    bpe_tokenizer = BPETokenizer(\"gpt2\")\n",
    "    bpe_tokens = bpe_tokenizer.tokenize(sample_text)\n",
    "    print(f\"BPE分词结果: {bpe_tokens}\")\n",
    "    bpe_encoded = bpe_tokenizer.encode(sample_text)\n",
    "    print(f\"BPE编码结果: {bpe_encoded}\")\n",
    "    bpe_decoded = bpe_tokenizer.decode(bpe_encoded)\n",
    "    print(f\"BPE解码结果: {bpe_decoded}\")\n",
    "    print(f\"BPE词汇表大小: {bpe_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d087930",
   "metadata": {},
   "source": [
    "\n",
    "    === 简单分词器 ===\n",
    "    分词结果: ['Hello,', ' ', 'how', ' ', 'are', ' ', 'you', ' ', 'today?', ' ', 'I', ' ', 'hope', ' ', 'you', ' ', 'are', ' ', 'doing', ' ', 'well.']\n",
    "    编码结果: [7, 4, 11, 4, 5, 4, 6, 4, 12, 4, 8, 4, 10, 4, 6, 4, 5, 4, 9, 4, 13]\n",
    "    解码结果: Hello, how are you today? I hope you are doing well.\n",
    "    词汇表大小: 14\n",
    "    \n",
    "    === BPE分词器 (GPT-2) ===\n",
    "    BPE分词结果: [b'Hello', b',', b' how', b' are', b' you', b' today', b'?', b' I', b' hope', b' you', b' are', b' doing', b' well', b'.']\n",
    "    BPE编码结果: [15496, 11, 703, 389, 345, 1909, 30, 314, 2911, 345, 389, 1804, 880, 13]\n",
    "    BPE解码结果: Hello, how are you today? I hope you are doing well.\n",
    "    BPE词汇表大小: 50257\n",
    "    \n",
    "\n",
    "## 3.将标记转换为标记ID\n",
    "\n",
    "将标记转换为标记 ID 是连接文本与数值向量的中间步骤，其核心是构建 “词汇表”—— 一个从唯一标记到唯一整数的映射（如图 2.6 所示）。词汇表的构建过程是：从分词后的文本中提取所有唯一标记，按字母顺序排序后，为每个标记分配一个整数 ID。例如，《判决》分词后得到 1130 个唯一标记，词汇表便会将这些标记分别映射到 0 至 1129 的整数。\n",
    "\n",
    "有了词汇表，我们就能通过 “编码” 将文本转换为标记 ID（例如 “Hello” 对应某个整数），也能通过 “解码” 将标记 ID 转回文本。这一过程可通过分词器类实现，例如 SimpleTokenizerV1 包含 encode 和 decode 方法：encode 先对文本分词，再用词汇表映射为 ID；decode 则将 ID 通过反向映射转回文本，并处理标点符号前的空格问题。\n",
    "\n",
    "但需注意，若文本中出现词汇表外的标记（如 “Hello” 未出现在《判决》中），编码时会报错，这说明训练集的规模和多样性对扩展词汇表至关重要，也引出了后续处理未知词汇的需求。\n",
    "\n",
    "整体实现代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664eb227",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "class Tokenizer:\n",
    "    \"\"\"将文本转换为标记并映射到ID的分词器\"\"\"\n",
    "    \n",
    "    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):\n",
    "        \"\"\"\n",
    "        初始化分词器\n",
    "        \n",
    "        Args:\n",
    "            special_tokens: 特殊标记及其ID的字典，默认为None\n",
    "        \"\"\"\n",
    "        self.vocab: Dict[str, int] = {}  # 词汇表: 标记 -> ID\n",
    "        self.reverse_vocab: Dict[int, str] = {}  # 反向词汇表: ID -> 标记\n",
    "        self.special_tokens = special_tokens or {\n",
    "            \"<pad>\": 0,  # 填充标记\n",
    "            \"<unk>\": 1,  # 未知标记\n",
    "            \"<bos>\": 2,  # 序列开始标记\n",
    "            \"<eos>\": 3   # 序列结束标记\n",
    "        }\n",
    "        self.vocab_size: int = len(self.special_tokens)  # 词汇表大小\n",
    "        # 初始化词汇表，添加特殊标记\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        从文本构建词汇表\n",
    "        \n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "            min_freq: 最小词频，低于此频率的词将被忽略\n",
    "        \"\"\"\n",
    "        # 统计词频\n",
    "        token_counts = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            token_counts.update(tokens)\n",
    "        # 按频率排序，频率相同则按字母顺序\n",
    "        sorted_tokens = sorted(\n",
    "            [(token, count) for token, count in token_counts.items() if count >= min_freq],\n",
    "            key=lambda x: (-x[1], x[0])  \n",
    "        )\n",
    "        # 为每个标记分配ID\n",
    "        for token, _ in sorted_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = self.vocab_size\n",
    "                self.reverse_vocab[self.vocab_size] = token\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        将文本分割为标记列表\n",
    "        \n",
    "        Args:\n",
    "            text: 输入文本\n",
    "            \n",
    "        Returns:\n",
    "            标记列表\n",
    "        \"\"\"\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        将文本转换为标记ID列表\n",
    "        \n",
    "        Args:\n",
    "            text: 输入文本\n",
    "            add_special_tokens: 是否添加特殊标记\n",
    "            \n",
    "        Returns:\n",
    "            标记ID列表\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        encoded = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            encoded = [self.vocab[\"<bos>\"]] + encoded + [self.vocab[\"<eos>\"]]\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        将标记ID列表转换回文本\n",
    "        Args:\n",
    "            ids: 标记ID列表\n",
    "            remove_special_tokens: 是否移除特殊标记\n",
    "        Returns:\n",
    "            文本\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            if idx in self.reverse_vocab:\n",
    "                token = self.reverse_vocab[idx]\n",
    "                if remove_special_tokens and token in self.special_tokens:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.append(\"<unk>\")\n",
    "        text = ''.join(tokens)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def test_tokenizer():\n",
    "    \"\"\"测试Tokenizer类的功能\"\"\"\n",
    "    print(\"===== 测试 Tokenizer =====\")\n",
    "    texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"I hope you are doing well.\",\n",
    "        \"This is a test of the tokenizer.\"\n",
    "    ]\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.build_vocab(texts)\n",
    "    print(f\"词汇表大小: {tokenizer.vocab_size}\")\n",
    "    print(f\"前10个词汇项: {list(tokenizer.vocab.items())[:10]}\")\n",
    "    sample_text = \"Hello, this is a test!\"\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    print(f\"\\n分词结果: {tokens}\")\n",
    "    encoded = tokenizer.encode(sample_text)\n",
    "    print(f\"编码结果: {encoded}\")\n",
    "    encoded_with_special = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "    print(f\"带特殊标记的编码结果: {encoded_with_special}\")\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"解码结果: {decoded}\")\n",
    "    decoded_with_special = tokenizer.decode(encoded_with_special, remove_special_tokens=True)\n",
    "    print(f\"移除特殊标记的解码结果: {decoded_with_special}\")\n",
    "    unknown_text = \"This is a unicorn 🦄 test.\"\n",
    "    encoded_unknown = tokenizer.encode(unknown_text)\n",
    "    decoded_unknown = tokenizer.decode(encoded_unknown)\n",
    "    print(f\"\\n包含未知词汇的文本: {unknown_text}\")\n",
    "    print(f\"编码结果: {encoded_unknown}\")\n",
    "    print(f\"解码结果: {decoded_unknown}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a059b5",
   "metadata": {},
   "source": [
    "\n",
    "    ===== 测试 Tokenizer =====\n",
    "    词汇表大小: 24\n",
    "    前10个词汇项: [('<pad>', 0), ('<unk>', 1), ('<bos>', 2), ('<eos>', 3), (' ', 4), ('.', 5), ('are', 6), ('you', 7), (',', 8), ('?', 9)]\n",
    "    \n",
    "    分词结果: ['Hello', ',', ' ', 'this', ' ', 'is', ' ', 'a', ' ', 'test', '!']\n",
    "    编码结果: [10, 8, 4, 1, 4, 17, 4, 13, 4, 19, 1]\n",
    "    带特殊标记的编码结果: [2, 10, 8, 4, 1, 4, 17, 4, 13, 4, 19, 1, 3]\n",
    "    解码结果: Hello, <unk> is a test<unk>\n",
    "    移除特殊标记的解码结果: Hello, is a test\n",
    "    \n",
    "    包含未知词汇的文本: This is a unicorn 🦄 test.\n",
    "    编码结果: [12, 4, 17, 4, 13, 4, 1, 4, 1, 4, 19, 5]\n",
    "    解码结果: This is a <unk> <unk> test.\n",
    "    \n",
    "\n",
    "## 4.添加特殊上下文标记\n",
    "\n",
    "为解决未知词汇问题并增强模型对上下文的理解，需引入特殊标记。常见的特殊标记包括：<|unk|>（表示未知词汇）、（分隔不同文本来源）、[BOS]（序列开始）、[EOS]（序列结束）、[PAD]（填充短文本至统一长度）等。\n",
    "\n",
    "例如，修改词汇表加入 <|unk|> 和后，分词器（如 SimpleTokenizerV2）在遇到未知词时会自动替换为 <|unk|>，并在不同文本间插入作为分隔。这一调整使模型能处理未见过的词汇，并区分独立文本来源。需注意，GPT 模型通常仅使用作为分隔符和填充符，而不依赖 <|unk|>，因为其采用的字节对编码（BPE）分词器能通过子词分解处理未知词，这也是后续将介绍的更高效方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9809349f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Dict, Optional, Set\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"带有特殊上下文标记的分词器\"\"\"\n",
    "    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):\n",
    "        \"\"\"\n",
    "        初始化分词器\n",
    "        \n",
    "        Args:\n",
    "            special_tokens: 特殊标记及其ID的字典，默认为None\n",
    "        \"\"\"\n",
    "        # 设置默认特殊标记\n",
    "        self.default_special_tokens = {\n",
    "            \"<pad>\": 0,  # 填充标记\n",
    "            \"<unk>\": 1,  # 未知标记\n",
    "            \"<bos>\": 2,  # 序列开始标记\n",
    "            \"<eos>\": 3,  # 序列结束标记\n",
    "            \"<sep>\": 4,  # 分隔标记\n",
    "            \"<cls>\": 5,  # 分类标记\n",
    "        }\n",
    "        # 合并用户提供的特殊标记\n",
    "        self.special_tokens = {**self.default_special_tokens, **(special_tokens or {})}\n",
    "        # 初始化词汇表\n",
    "        self.vocab: Dict[str, int] = {}\n",
    "        self.reverse_vocab: Dict[int, str] = {}\n",
    "        self.vocab_size: int = len(self.special_tokens)\n",
    "        # 添加特殊标记到词汇表\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token\n",
    "    \n",
    "    def build_vocab(self, texts: List[str], min_freq: int = 1) -> None:\n",
    "        \"\"\"\n",
    "        从文本构建词汇表\n",
    "        \n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "            min_freq: 最小词频，低于此频率的词将被忽略\n",
    "        \"\"\"\n",
    "        # 统计词频\n",
    "        token_counts = {}\n",
    "        for text in texts:\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                token_counts[token] = token_counts.get(token, 0) + 1\n",
    "        \n",
    "        # 按频率排序，频率相同则按字母顺序\n",
    "        sorted_tokens = sorted(\n",
    "            [(token, count) for token, count in token_counts.items() if count >= min_freq],\n",
    "            key=lambda x: (-x[1], x[0])  # 按频率降序，字母升序\n",
    "        )\n",
    "        \n",
    "        # 为每个标记分配ID\n",
    "        for token, _ in sorted_tokens:\n",
    "            if token not in self.vocab:\n",
    "                self.vocab[token] = self.vocab_size\n",
    "                self.reverse_vocab[self.vocab_size] = token\n",
    "                self.vocab_size += 1\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        将文本分割为标记列表\n",
    "        \n",
    "        Args:\n",
    "            text: 输入文本\n",
    "            \n",
    "        Returns:\n",
    "            标记列表\n",
    "        \"\"\"\n",
    "        # 简单的分词：按非字母数字字符分割，保留空格和标点符号\n",
    "        tokens = re.findall(r'\\w+|[^\\w\\s]|\\s+', text)\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        将文本转换为标记ID列表\n",
    "        \n",
    "        Args:\n",
    "            text: 输入文本\n",
    "            add_special_tokens: 是否添加特殊标记\n",
    "            \n",
    "        Returns:\n",
    "            标记ID列表\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        encoded = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        if add_special_tokens:\n",
    "            encoded = [self.vocab[\"<bos>\"]] + encoded + [self.vocab[\"<eos>\"]]\n",
    "        return encoded\n",
    "    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        将标记ID列表转换回文本\n",
    "        Args:\n",
    "            ids: 标记ID列表\n",
    "            remove_special_tokens: 是否移除特殊标记\n",
    "        Returns:\n",
    "            文本\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            if idx in self.reverse_vocab:\n",
    "                token = self.reverse_vocab[idx]\n",
    "                if remove_special_tokens and token in self.special_tokens:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.append(\"<unk>\")\n",
    "        text = ''.join(tokens)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    def add_special_token(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        添加新的特殊标记\n",
    "        Args:\n",
    "            token: 特殊标记字符串\n",
    "        Returns:\n",
    "            新标记的ID\n",
    "        \"\"\"\n",
    "        if token in self.vocab:\n",
    "            return self.vocab[token]\n",
    "        new_id = self.vocab_size\n",
    "        self.vocab[token] = new_id\n",
    "        self.reverse_vocab[new_id] = token\n",
    "        self.vocab_size += 1\n",
    "        return new_id\n",
    "    \n",
    "def test_special_tokens():\n",
    "    \"\"\"测试特殊标记的功能\"\"\"\n",
    "    print(\"===== 测试特殊标记 =====\")\n",
    "    texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"I hope you are doing well.\",\n",
    "        \"This is a test of the tokenizer.\"\n",
    "    ]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.build_vocab(texts)\n",
    "    sample_text = \"Hello, this is a test!\"\n",
    "    encoded = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "    decoded = tokenizer.decode(encoded, remove_special_tokens=True)\n",
    "    \n",
    "    print(f\"原始文本: {sample_text}\")\n",
    "    print(f\"编码结果: {encoded}\")\n",
    "    print(f\"解码结果: {decoded}\")\n",
    "    print(\"\\n--- 测试特殊标记 ---\")\n",
    "    print(f\"特殊标记: {tokenizer.special_tokens}\")\n",
    "    # 测试<bos>和<eos>\n",
    "    encoded_with_bos_eos = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "    print(f\"带<bos>和<eos>的编码: {encoded_with_bos_eos}\")\n",
    "    # 测试<SEP>标记 - 连接两个句子\n",
    "    print(\"\\n--- 测试<SEP>标记 ---\")\n",
    "    sentence1 = \"What is your name?\"\n",
    "    sentence2 = \"My name is Doubao.\"\n",
    "    encoded1 = tokenizer.encode(sentence1)\n",
    "    encoded2 = tokenizer.encode(sentence2)\n",
    "    # 添加<SEP>标记\n",
    "    encoded_combined = [tokenizer.vocab[\"<bos>\"]] + \\\n",
    "                       encoded1 + \\\n",
    "                       [tokenizer.vocab[\"<sep>\"]] + \\\n",
    "                       encoded2 + \\\n",
    "                       [tokenizer.vocab[\"<eos>\"]]\n",
    "    decoded_combined = tokenizer.decode(encoded_combined, remove_special_tokens=True)\n",
    "    print(f\"句子1: {sentence1}\")\n",
    "    print(f\"句子2: {sentence2}\")\n",
    "    print(f\"合并后的编码: {encoded_combined}\")\n",
    "    print(f\"合并后的解码: {decoded_combined}\")\n",
    "    # 测试添加新的特殊标记\n",
    "    print(\"\\n--- 测试添加新的特殊标记 ---\")\n",
    "    new_token = \"<mask>\"\n",
    "    new_token_id = tokenizer.add_special_token(new_token)\n",
    "    print(f\"添加新特殊标记: {new_token} (ID: {new_token_id})\")\n",
    "    \n",
    "    # 测试使用新的特殊标记\n",
    "    masked_text = \"This is a <mask> sentence.\"\n",
    "    encoded_masked = tokenizer.encode(masked_text, add_special_tokens=True)\n",
    "    decoded_masked = tokenizer.decode(encoded_masked, remove_special_tokens=False)\n",
    "    \n",
    "    print(f\"带<mask>的文本: {masked_text}\")\n",
    "    print(f\"编码结果: {encoded_masked}\")\n",
    "    print(f\"解码结果: {decoded_masked}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_special_tokens()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cca374",
   "metadata": {},
   "source": [
    "\n",
    "    ===== 测试特殊标记 =====\n",
    "    原始文本: Hello, this is a test!\n",
    "    编码结果: [2, 12, 10, 6, 1, 6, 19, 6, 15, 6, 21, 1, 3]\n",
    "    解码结果: Hello, is a test\n",
    "    \n",
    "    --- 测试特殊标记 ---\n",
    "    特殊标记: {'<pad>': 0, '<unk>': 1, '<bos>': 2, '<eos>': 3, '<sep>': 4, '<cls>': 5}\n",
    "    带<bos>和<eos>的编码: [2, 12, 10, 6, 1, 6, 19, 6, 15, 6, 21, 1, 3]\n",
    "    \n",
    "    --- 测试<SEP>标记 ---\n",
    "    句子1: What is your name?\n",
    "    句子2: My name is Doubao.\n",
    "    合并后的编码: [2, 1, 6, 19, 6, 1, 6, 1, 11, 4, 1, 6, 1, 6, 19, 6, 1, 7, 3]\n",
    "    合并后的解码: is ? is .\n",
    "    \n",
    "    --- 测试添加新的特殊标记 ---\n",
    "    添加新特殊标记: <mask> (ID: 26)\n",
    "    带<mask>的文本: This is a <mask> sentence.\n",
    "    编码结果: [2, 14, 6, 19, 6, 15, 6, 1, 1, 1, 6, 1, 7, 3]\n",
    "    解码结果: <bos>This is a <unk><unk><unk> <unk>.<eos>\n",
    "    \n",
    "\n",
    "## 5.字节对编码\n",
    "\n",
    "字节对编码（BPE）是一种高级分词方法，被 GPT-2、GPT-3 等主流 LLMs 采用，其核心优势是能处理未知词汇 —— 将未见过的单词分解为子词单元（如图 2.11 所示）。例如，“someunknownPlace” 可拆分为已知的子词标记，无需依赖 <|unk|>。\n",
    "\n",
    "BPE 的实现可借助 tiktoken 库（OpenAI 开源），其基于 Rust 实现，高效且兼容 GPT 模型的分词逻辑。使用时，先通过 tiktoken.get_encoding (\"gpt2\") 实例化分词器，再用 encode 方法将文本转换为标记 ID，decode 方法则可还原文本。例如，“Hello, do you like tea?” 经 BPE 编码后，会生成一系列整数 ID，解码后能准确还原原始文本，即使包含 “someunknownPlace” 这类未知词，也能通过子词拆分正确处理。\n",
    "\n",
    "BPE 的词汇表规模固定（如 GPT-2 为 50257），通过迭代合并高频字符或子词构建，既控制了词汇表大小，又能覆盖几乎所有可能的文本，是平衡效率与泛化能力的理想选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222f8b6",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import tiktoken\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Set, Union\n",
    "\n",
    "class SimpleBPETokenizer:\n",
    "    \"\"\"简单实现的BPE分词器\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 100):\n",
    "        \"\"\"\n",
    "        初始化BPE分词器\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: 目标词汇表大小\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}  # 词汇表: 标记 -> ID\n",
    "        self.reverse_vocab = {}  # 反向词汇表: ID -> 标记\n",
    "        self.bpe_ranks = {}  # BPE合并规则的优先级\n",
    "        self.special_tokens = {\n",
    "            \"<pad>\": 0,\n",
    "            \"<unk>\": 1,\n",
    "            \"<bos>\": 2,\n",
    "            \"<eos>\": 3\n",
    "        }\n",
    "        self.vocab_size_actual = len(self.special_tokens)\n",
    "        \n",
    "        # 初始化词汇表，添加特殊标记\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.vocab[token] = idx\n",
    "            self.reverse_vocab[idx] = token\n",
    "    \n",
    "    def _get_stats(self, pairs: Dict[Tuple[str, str], int]) -> Dict[Tuple[str, str], int]:\n",
    "        \"\"\"计算所有相邻字节对的频率\"\"\"\n",
    "        stats = defaultdict(int)\n",
    "        for word, freq in pairs.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                stats[symbols[i], symbols[i + 1]] += freq\n",
    "        return stats\n",
    "    \n",
    "    def _merge_vocab(self, pair: Tuple[str, str], v_in: Dict[str, int]) -> Dict[str, int]:\n",
    "        \"\"\"合并最频繁的字节对\"\"\"\n",
    "        v_out = {}\n",
    "        bigram = re.escape(' '.join(pair))\n",
    "        p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "        for word in v_in:\n",
    "            w_out = p.sub(''.join(pair), word)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "    \n",
    "    def _word_to_pairs(self, word: str) -> Set[Tuple[str, str]]:\n",
    "        \"\"\"将单词拆分为相邻字节对\"\"\"\n",
    "        symbols = word.split()\n",
    "        pairs = set()\n",
    "        if len(symbols) < 2:\n",
    "            return pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs.add((symbols[i], symbols[i + 1]))\n",
    "        return pairs\n",
    "    \n",
    "    def build_vocab(self, texts: List[str]) -> None:\n",
    "        \"\"\"\n",
    "        从文本构建BPE词汇表\n",
    "        \n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "        \"\"\"\n",
    "        # 初始化词汇表，包含所有单个字符\n",
    "        token_counts = Counter()\n",
    "        for text in texts:\n",
    "            # 将文本拆分为字符，用空格分隔\n",
    "            words = [' '.join(list(text))]\n",
    "            for word in words:\n",
    "                token_counts[word] += 1\n",
    "        \n",
    "        # 统计初始的字符词汇表\n",
    "        chars = set()\n",
    "        for text in texts:\n",
    "            chars.update(text)\n",
    "        \n",
    "        # 添加字符到词汇表\n",
    "        for char in sorted(chars):\n",
    "            if char not in self.vocab:\n",
    "                self.vocab[char] = self.vocab_size_actual\n",
    "                self.reverse_vocab[self.vocab_size_actual] = char\n",
    "                self.vocab_size_actual += 1\n",
    "        \n",
    "        # 开始BPE合并过程\n",
    "        num_merges = self.vocab_size - self.vocab_size_actual\n",
    "        if num_merges <= 0:\n",
    "            return\n",
    "        \n",
    "        pairs = token_counts.copy()\n",
    "        for i in range(num_merges):\n",
    "            # 计算所有相邻字节对的频率\n",
    "            stats = self._get_stats(pairs)\n",
    "            if not stats:\n",
    "                break\n",
    "            \n",
    "            # 选择最频繁的字节对\n",
    "            best = max(stats, key=stats.get)\n",
    "            \n",
    "            # 记录合并规则的优先级\n",
    "            self.bpe_ranks[best] = i\n",
    "            \n",
    "            # 合并词汇表中的字节对\n",
    "            pairs = self._merge_vocab(best, pairs)\n",
    "            \n",
    "            # 将新合并的标记添加到词汇表\n",
    "            new_token = ''.join(best)\n",
    "            if new_token not in self.vocab:\n",
    "                self.vocab[new_token] = self.vocab_size_actual\n",
    "                self.reverse_vocab[self.vocab_size_actual] = new_token\n",
    "                self.vocab_size_actual += 1\n",
    "    \n",
    "    def _get_pairs(self, word: List[str]) -> Set[Tuple[str, str]]:\n",
    "        \"\"\"获取单词中所有相邻标记对\"\"\"\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "    \n",
    "    def bpe(self, token: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        对单个标记应用BPE算法\n",
    "        \n",
    "        Args:\n",
    "            token: 输入标记\n",
    "            \n",
    "        Returns:\n",
    "            BPE分词后的标记列表\n",
    "        \"\"\"\n",
    "        if token in self.special_tokens:\n",
    "            return [token]\n",
    "        \n",
    "        word = list(token)\n",
    "        if len(word) == 0:\n",
    "            return []\n",
    "        if len(word) == 1:\n",
    "            return [word[0]]\n",
    "        \n",
    "        pairs = self._get_pairs(word)\n",
    "        \n",
    "        while True:\n",
    "            # 找到优先级最高的字节对\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            \n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "                \n",
    "                if i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            \n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = self._get_pairs(word)\n",
    "        \n",
    "        return word\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        将文本分词为BPE标记\n",
    "        \n",
    "        Args:\n",
    "            text: 输入文本\n",
    "            \n",
    "        Returns:\n",
    "            BPE标记列表\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for token in text.split():\n",
    "            if token in self.special_tokens:\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.extend(self.bpe(token))\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:\n",
    "        \"\"\"\n",
    "        将文本编码为BPE标记ID列表\n",
    "        \n",
    "        Args:\n",
    "            text: 输入文本\n",
    "            add_special_tokens: 是否添加特殊标记\n",
    "            \n",
    "        Returns:\n",
    "            BPE标记ID列表\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        encoded = [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in tokens]\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            encoded = [self.vocab[\"<bos>\"]] + encoded + [self.vocab[\"<eos>\"]]\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def decode(self, ids: List[int], remove_special_tokens: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        将BPE标记ID列表解码为文本\n",
    "        \n",
    "        Args:\n",
    "            ids: BPE标记ID列表\n",
    "            remove_special_tokens: 是否移除特殊标记\n",
    "            \n",
    "        Returns:\n",
    "            解码后的文本\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            if idx in self.reverse_vocab:\n",
    "                token = self.reverse_vocab[idx]\n",
    "                if remove_special_tokens and token in self.special_tokens:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                tokens.append(\"<unk>\")\n",
    "        \n",
    "        # 简单的后处理：合并标记\n",
    "        text = ''.join(tokens)\n",
    "        return text\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "def test_simple_bpe_tokenizer():\n",
    "    \"\"\"测试简单实现的BPE分词器\"\"\"\n",
    "    print(\"===== 测试简单BPE分词器 =====\")\n",
    "    \n",
    "    # 示例文本\n",
    "    texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"I hope you are doing well.\",\n",
    "        \"This is a test of the BPE tokenizer.\"\n",
    "    ]\n",
    "    \n",
    "    # 初始化BPE分词器\n",
    "    tokenizer = SimpleBPETokenizer(vocab_size=50)\n",
    "    \n",
    "    # 构建词汇表\n",
    "    tokenizer.build_vocab(texts)\n",
    "    \n",
    "    print(f\"词汇表大小: {tokenizer.vocab_size_actual}\")\n",
    "    print(f\"前10个词汇项: {list(tokenizer.vocab.items())[:10]}\")\n",
    "    \n",
    "    # 测试分词\n",
    "    sample_text = \"Hello, this is a test!\"\n",
    "    tokens = tokenizer.tokenize(sample_text)\n",
    "    print(f\"\\n分词结果: {tokens}\")\n",
    "    \n",
    "    # 测试编码\n",
    "    encoded = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "    print(f\"编码结果: {encoded}\")\n",
    "    \n",
    "    # 测试解码\n",
    "    decoded = tokenizer.decode(encoded, remove_special_tokens=True)\n",
    "    print(f\"解码结果: {decoded}\")\n",
    "    \n",
    "    # 测试包含未知词汇的文本\n",
    "    unknown_text = \"This is a unicorn 🦄 test.\"\n",
    "    encoded_unknown = tokenizer.encode(unknown_text)\n",
    "    decoded_unknown = tokenizer.decode(encoded_unknown)\n",
    "    print(f\"\\n包含未知词汇的文本: {unknown_text}\")\n",
    "    print(f\"编码结果: {encoded_unknown}\")\n",
    "    print(f\"解码结果: {decoded_unknown}\")\n",
    "    \n",
    "    print(\"\\n所有测试完成!\")\n",
    "\n",
    "\n",
    "def test_tiktoken_bpe():\n",
    "    \"\"\"测试tiktoken库的BPE分词器\"\"\"\n",
    "    print(\"\\n===== 测试tiktoken BPE分词器 =====\")\n",
    "    \n",
    "    # 初始化tiktoken BPE分词器\n",
    "    try:\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    except KeyError:\n",
    "        # 如果没有安装gpt2编码，尝试使用cl100k_base (用于text-embedding-ada-002)\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    print(f\"词汇表大小: {tokenizer.n_vocab}\")\n",
    "    \n",
    "    # 测试分词\n",
    "    sample_text = \"Hello, this is a test!\"\n",
    "    tokens = tokenizer.encode(sample_text)\n",
    "    print(f\"\\n编码结果 (ID): {tokens}\")\n",
    "    \n",
    "    # 转换ID为字节\n",
    "    token_bytes = [tokenizer.decode_single_token_bytes(token) for token in tokens]\n",
    "    print(f\"编码结果 (字节): {token_bytes}\")\n",
    "    \n",
    "    # 测试解码\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(f\"解码结果: {decoded}\")\n",
    "    \n",
    "    # 测试包含未知词汇的文本\n",
    "    unknown_text = \"This is a unicorn 🦄 test.\"\n",
    "    encoded_unknown = tokenizer.encode(unknown_text)\n",
    "    decoded_unknown = tokenizer.decode(encoded_unknown)\n",
    "    print(f\"\\n包含未知词汇的文本: {unknown_text}\")\n",
    "    print(f\"编码结果: {encoded_unknown}\")\n",
    "    print(f\"解码结果: {decoded_unknown}\")\n",
    "    \n",
    "    # 计算文本的token数量\n",
    "    print(f\"\\n文本 '{sample_text}' 的token数量: {len(tokens)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_simple_bpe_tokenizer()\n",
    "    test_tiktoken_bpe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c3578",
   "metadata": {},
   "source": [
    "\n",
    "    ===== 测试简单BPE分词器 =====\n",
    "    词汇表大小: 46\n",
    "    前10个词汇项: [('<pad>', 0), ('<unk>', 1), ('<bos>', 2), ('<eos>', 3), (' ', 4), (',', 5), ('.', 6), ('?', 7), ('B', 8), ('E', 9)]\n",
    "    \n",
    "    分词结果: ['Hello,', 't', 'h', 'is', 'is', 'a', 't', 'e', 's', 't', '!']\n",
    "    编码结果: [2, 44, 28, 19, 41, 41, 14, 28, 16, 27, 28, 1, 3]\n",
    "    解码结果: Hello,thisisatest\n",
    "    \n",
    "    包含未知词汇的文本: This is a unicorn 🦄 test.\n",
    "    编码结果: [13, 19, 41, 41, 14, 29, 23, 20, 1, 24, 26, 23, 1, 28, 16, 27, 28, 6]\n",
    "    解码结果: Thisisauni<unk>orn<unk>test.\n",
    "    \n",
    "    所有测试完成!\n",
    "    \n",
    "    ===== 测试tiktoken BPE分词器 =====\n",
    "    词汇表大小: 50257\n",
    "    \n",
    "    编码结果 (ID): [15496, 11, 428, 318, 257, 1332, 0]\n",
    "    编码结果 (字节): [b'Hello', b',', b' this', b' is', b' a', b' test', b'!']\n",
    "    解码结果: Hello, this is a test!\n",
    "    \n",
    "    包含未知词汇的文本: This is a unicorn 🦄 test.\n",
    "    编码结果: [1212, 318, 257, 44986, 12520, 99, 226, 1332, 13]\n",
    "    解码结果: This is a unicorn 🦄 test.\n",
    "    \n",
    "    文本 'Hello, this is a test!' 的token数量: 7\n",
    "    \n",
    "\n",
    "## 6.使用滑动窗口进行数据采样\n",
    "\n",
    "LLMs 通过 “下一个词预测” 任务预训练，即给定输入文本块，预测紧随其后的下一个词（如图 2.12 所示）。为生成训练所需的输入 - 目标对，需采用 “滑动窗口” 方法：在分词后的文本上滑动固定大小的窗口，窗口内的文本作为输入，窗口右侧紧邻的词作为目标（输入右移一位即为目标）。\n",
    "\n",
    "例如，若窗口大小（上下文长度）为 4，输入为 [标记 1, 标记 2, 标记 3, 标记 4]，则目标为 [标记 2, 标记 3, 标记 4, 标记 5]。为高效生成批量样本，可结合 PyTorch 的 Dataset 和 DataLoader：Dataset 负责按窗口滑动生成输入 - 目标对，DataLoader 则将这些对打包为批次（如图 2.13 所示），支持设置批次大小、步长（窗口移动距离）等参数。\n",
    "\n",
    "步长决定样本重叠程度：步长为 1 时，相邻样本高度重叠；步长等于窗口大小时，样本无重叠。合理设置步长可平衡数据利用率与过拟合风险，最终生成的输入 - 目标对以张量形式输入模型，为训练提供数据支持。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926d7553",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "class TextWindowDataset(Dataset):\n",
    "    \"\"\"使用滑动窗口对文本进行采样的数据集\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 text: List[int], \n",
    "                 context_length: int, \n",
    "                 stride: int = 1,\n",
    "                 pad_id: int = 0):\n",
    "        \"\"\"\n",
    "        初始化文本窗口数据集\n",
    "        \n",
    "        Args:\n",
    "            text: 已编码的文本（整数列表）\n",
    "            context_length: 上下文长度（窗口大小）\n",
    "            stride: 滑动窗口的步长，默认为1\n",
    "            pad_id: 填充标记的ID\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.context_length = context_length\n",
    "        self.stride = stride\n",
    "        self.pad_id = pad_id\n",
    "        \n",
    "        # 计算有效样本数量\n",
    "        self.num_samples = max(0, (len(text) - context_length) // stride + 1)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"返回数据集的样本数量\"\"\"\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        获取单个样本\n",
    "        \n",
    "        Args:\n",
    "            idx: 样本索引\n",
    "            \n",
    "        Returns:\n",
    "            元组(inputs, targets)，其中inputs是输入序列，targets是目标序列\n",
    "        \"\"\"\n",
    "        # 计算窗口起始位置\n",
    "        start = idx * self.stride\n",
    "        \n",
    "        # 确保窗口不超出文本长度\n",
    "        end = start + self.context_length\n",
    "        if end > len(self.text):\n",
    "            # 截取最后可能的有效窗口\n",
    "            end = len(self.text)\n",
    "            start = end - self.context_length\n",
    "        \n",
    "        # 提取输入序列和目标序列\n",
    "        inputs = self.text[start:end]\n",
    "        targets = self.text[start+1:end+1]  # 目标是输入的下一个标记\n",
    "        \n",
    "        # 如果目标序列长度不足，用pad_id填充\n",
    "        if len(targets) < self.context_length:\n",
    "            targets = targets + [self.pad_id] * (self.context_length - len(targets))\n",
    "        \n",
    "        # 转换为张量\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        \n",
    "        return inputs, targets\n",
    "\n",
    "\n",
    "def create_data_loader(text: List[int], \n",
    "                       context_length: int, \n",
    "                       batch_size: int, \n",
    "                       stride: int = 1,\n",
    "                       shuffle: bool = False) -> DataLoader:\n",
    "    \"\"\"\n",
    "    创建文本窗口数据加载器\n",
    "    \n",
    "    Args:\n",
    "        text: 已编码的文本（整数列表）\n",
    "        context_length: 上下文长度\n",
    "        batch_size: 批次大小\n",
    "        stride: 滑动窗口步长\n",
    "        shuffle: 是否打乱数据\n",
    "        \n",
    "    Returns:\n",
    "        数据加载器\n",
    "    \"\"\"\n",
    "    dataset = TextWindowDataset(\n",
    "        text=text,\n",
    "        context_length=context_length,\n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "def test_sliding_window():\n",
    "    \"\"\"测试滑动窗口数据采样\"\"\"\n",
    "    print(\"===== 测试滑动窗口数据采样 =====\")\n",
    "    \n",
    "    # 示例文本（已编码）\n",
    "    encoded_text = [101, 102, 103, 104, 105, 106, 107, 108, 109, 110]\n",
    "    print(f\"原始文本: {encoded_text}\")\n",
    "    \n",
    "    # 参数设置\n",
    "    context_length = 4\n",
    "    stride = 2\n",
    "    batch_size = 2\n",
    "    \n",
    "    # 创建数据集\n",
    "    dataset = TextWindowDataset(\n",
    "        text=encoded_text,\n",
    "        context_length=context_length,\n",
    "        stride=stride\n",
    "    )\n",
    "    \n",
    "    # 打印数据集信息\n",
    "    print(f\"\\n数据集大小: {len(dataset)}\")\n",
    "    \n",
    "    # 测试获取单个样本\n",
    "    print(\"\\n--- 测试获取单个样本 ---\")\n",
    "    for i in range(min(3, len(dataset))):\n",
    "        inputs, targets = dataset[i]\n",
    "        print(f\"样本 {i}:\")\n",
    "        print(f\"  输入: {inputs}\")\n",
    "        print(f\"  目标: {targets}\")\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    data_loader = create_data_loader(\n",
    "        text=encoded_text,\n",
    "        context_length=context_length,\n",
    "        batch_size=batch_size,\n",
    "        stride=stride,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 测试批次数据\n",
    "    print(\"\\n--- 测试批次数据 ---\")\n",
    "    for i, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "        print(f\"批次 {i}:\")\n",
    "        print(f\"  输入形状: {batch_inputs.shape}\")\n",
    "        print(f\"  输入数据:\")\n",
    "        print(batch_inputs)\n",
    "        print(f\"  目标形状: {batch_targets.shape}\")\n",
    "        print(f\"  目标数据:\")\n",
    "        print(batch_targets)\n",
    "    \n",
    "    # 测试不同步长\n",
    "    print(\"\\n--- 测试不同步长 ---\")\n",
    "    for stride in [1, 2, 3]:\n",
    "        dataset = TextWindowDataset(\n",
    "            text=encoded_text,\n",
    "            context_length=context_length,\n",
    "            stride=stride\n",
    "        )\n",
    "        print(f\"步长为 {stride} 时的样本数: {len(dataset)}\")\n",
    "        \n",
    "        # 打印前两个样本\n",
    "        if len(dataset) > 0:\n",
    "            inputs, targets = dataset[0]\n",
    "            print(f\"  第一个样本输入: {inputs}\")\n",
    "            print(f\"  第一个样本目标: {targets}\")\n",
    "        \n",
    "        if len(dataset) > 1:\n",
    "            inputs, targets = dataset[1]\n",
    "            print(f\"  第二个样本输入: {inputs}\")\n",
    "            print(f\"  第二个样本目标: {targets}\")\n",
    "if __name__ == \"__main__\":\n",
    "    test_sliding_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d690f9f9",
   "metadata": {},
   "source": [
    "\n",
    "    ===== 测试滑动窗口数据采样 =====\n",
    "    原始文本: [101, 102, 103, 104, 105, 106, 107, 108, 109, 110]\n",
    "    \n",
    "    数据集大小: 4\n",
    "    \n",
    "    --- 测试获取单个样本 ---\n",
    "    样本 0:\n",
    "      输入: tensor([101, 102, 103, 104])\n",
    "      目标: tensor([102, 103, 104, 105])\n",
    "    样本 1:\n",
    "      输入: tensor([103, 104, 105, 106])\n",
    "      目标: tensor([104, 105, 106, 107])\n",
    "    样本 2:\n",
    "      输入: tensor([105, 106, 107, 108])\n",
    "      目标: tensor([106, 107, 108, 109])\n",
    "    \n",
    "    --- 测试批次数据 ---\n",
    "    批次 0:\n",
    "      输入形状: torch.Size([2, 4])\n",
    "      输入数据:\n",
    "    tensor([[101, 102, 103, 104],\n",
    "            [103, 104, 105, 106]])\n",
    "      目标形状: torch.Size([2, 4])\n",
    "      目标数据:\n",
    "    tensor([[102, 103, 104, 105],\n",
    "            [104, 105, 106, 107]])\n",
    "    批次 1:\n",
    "      输入形状: torch.Size([2, 4])\n",
    "      输入数据:\n",
    "    tensor([[105, 106, 107, 108],\n",
    "            [107, 108, 109, 110]])\n",
    "      目标形状: torch.Size([2, 4])\n",
    "      目标数据:\n",
    "    tensor([[106, 107, 108, 109],\n",
    "            [108, 109, 110,   0]])\n",
    "    \n",
    "    --- 测试不同步长 ---\n",
    "    步长为 1 时的样本数: 7\n",
    "      第一个样本输入: tensor([101, 102, 103, 104])\n",
    "      第一个样本目标: tensor([102, 103, 104, 105])\n",
    "      第二个样本输入: tensor([102, 103, 104, 105])\n",
    "      第二个样本目标: tensor([103, 104, 105, 106])\n",
    "    步长为 2 时的样本数: 4\n",
    "      第一个样本输入: tensor([101, 102, 103, 104])\n",
    "      第一个样本目标: tensor([102, 103, 104, 105])\n",
    "      第二个样本输入: tensor([103, 104, 105, 106])\n",
    "      第二个样本目标: tensor([104, 105, 106, 107])\n",
    "    步长为 3 时的样本数: 3\n",
    "      第一个样本输入: tensor([101, 102, 103, 104])\n",
    "      第一个样本目标: tensor([102, 103, 104, 105])\n",
    "      第二个样本输入: tensor([104, 105, 106, 107])\n",
    "      第二个样本目标: tensor([105, 106, 107, 108])\n",
    "    \n",
    "\n",
    "## 7.创建词嵌入\n",
    "\n",
    "词嵌入是将标记 ID 转换为连续向量的过程，是 LLM 处理文本的最终数值形式（如图 2.15 所示）。神经网络需连续向量进行运算，因此需通过 “嵌入层” 实现这一转换。嵌入层本质是一个权重矩阵：行数等于词汇表大小，列数为嵌入维度（如 3 维、256 维），每个标记 ID 对应矩阵中的一行，即该标记的嵌入向量。\n",
    "\n",
    "例如，词汇表大小为 6、嵌入维度为 3 的嵌入层，其权重矩阵为 6×3 的随机初始化矩阵（训练中会优化）。当输入标记 ID 为 [2, 3, 5, 1] 时，嵌入层会提取矩阵中对应行，得到 4×3 的嵌入向量矩阵。\n",
    "\n",
    "在 PyTorch 中，可通过 torch.nn.Embedding 实现嵌入层，其核心是 “查找操作”—— 根据 ID 快速检索对应嵌入向量。嵌入维度需权衡：更高维度（如 GPT-3 的 12288 维）能捕捉更多语义，但计算成本更高；较低维度（如 256 维）适合实验。嵌入层的权重会通过训练不断优化，使向量能更好地表示标记的语义和上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b97b00",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"词汇表管理类\"\"\"\n",
    "    \n",
    "    def __init__(self, special_tokens: Optional[Dict[str, int]] = None):\n",
    "        \"\"\"\n",
    "        初始化词汇表\n",
    "        \n",
    "        Args:\n",
    "            special_tokens: 特殊标记及其ID的字典\n",
    "        \"\"\"\n",
    "        self.token_to_idx = special_tokens or {\n",
    "            \"<pad>\": 0,\n",
    "            \"<unk>\": 1,\n",
    "            \"<bos>\": 2,\n",
    "            \"<eos>\": 3\n",
    "        }\n",
    "        self.idx_to_token = {v: k for k, v in self.token_to_idx.items()}\n",
    "        self.vocab_size = len(self.token_to_idx)\n",
    "    \n",
    "    def add_token(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        添加标记到词汇表\n",
    "        \n",
    "        Args:\n",
    "            token: 要添加的标记\n",
    "            \n",
    "        Returns:\n",
    "            标记的ID\n",
    "        \"\"\"\n",
    "        if token not in self.token_to_idx:\n",
    "            self.token_to_idx[token] = self.vocab_size\n",
    "            self.idx_to_token[self.vocab_size] = token\n",
    "            self.vocab_size += 1\n",
    "        return self.token_to_idx[token]\n",
    "    \n",
    "    def build_from_texts(self, texts: List[List[str]]) -> None:\n",
    "        \"\"\"\n",
    "        从文本列表构建词汇表\n",
    "        \n",
    "        Args:\n",
    "            texts: 文本列表，每个文本是标记列表\n",
    "        \"\"\"\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                self.add_token(token)\n",
    "    \n",
    "    def encode(self, tokens: List[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        将标记列表编码为ID列表\n",
    "        \n",
    "        Args:\n",
    "            tokens: 标记列表\n",
    "            \n",
    "        Returns:\n",
    "            ID列表\n",
    "        \"\"\"\n",
    "        return [self.token_to_idx.get(token, self.token_to_idx[\"<unk>\"]) for token in tokens]\n",
    "    \n",
    "    def decode(self, ids: List[int]) -> List[str]:\n",
    "        \"\"\"\n",
    "        将ID列表解码为标记列表\n",
    "        \n",
    "        Args:\n",
    "            ids: ID列表\n",
    "            \n",
    "        Returns:\n",
    "            标记列表\n",
    "        \"\"\"\n",
    "        return [self.idx_to_token.get(idx, \"<unk>\") for idx in ids]\n",
    "\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    \"\"\"连续词袋模型(CBOW)的数据集\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[List[str]], vocab: Vocabulary, context_size: int = 2):\n",
    "        \"\"\"\n",
    "        初始化CBOW数据集\n",
    "        \n",
    "        Args:\n",
    "            texts: 文本列表，每个文本是标记列表\n",
    "            vocab: 词汇表\n",
    "            context_size: 上下文大小(每侧的词数)\n",
    "        \"\"\"\n",
    "        self.context_size = context_size\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "        \n",
    "        # 构建训练样本\n",
    "        for text in texts:\n",
    "            encoded_text = vocab.encode(text)\n",
    "            for i in range(context_size, len(encoded_text) - context_size):\n",
    "                context = []\n",
    "                # 收集左右上下文\n",
    "                for j in range(-context_size, context_size + 1):\n",
    "                    if j != 0:  # 跳过中心词\n",
    "                        context.append(encoded_text[i + j])\n",
    "                target = encoded_text[i]  # 中心词\n",
    "                self.data.append((context, target))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"返回数据集大小\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        获取单个样本\n",
    "        \n",
    "        Args:\n",
    "            idx: 样本索引\n",
    "            \n",
    "        Returns:\n",
    "            元组(context, target)，其中context是上下文词ID，target是目标词ID\n",
    "        \"\"\"\n",
    "        context, target = self.data[idx]\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    \"\"\"连续词袋模型(CBOW)\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        初始化CBOW模型\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: 词汇表大小\n",
    "            embedding_dim: 嵌入维度\n",
    "        \"\"\"\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            inputs: 输入张量，形状为[batch_size, context_size*2]\n",
    "            \n",
    "        Returns:\n",
    "            输出张量，形状为[batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        # 获取上下文词的嵌入\n",
    "        embeds = self.embeddings(inputs)\n",
    "        # 对上下文嵌入取平均\n",
    "        context_mean = torch.mean(embeds, dim=1)\n",
    "        # 通过线性层预测中心词\n",
    "        output = self.linear(context_mean)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train_cbow_model(texts: List[List[str]], embedding_dim: int = 100, \n",
    "                     context_size: int = 2, epochs: int = 10, \n",
    "                     batch_size: int = 32, lr: float = 0.01) -> nn.Embedding:\n",
    "    \"\"\"\n",
    "    训练CBOW模型并返回词嵌入\n",
    "    \n",
    "    Args:\n",
    "        texts: 文本列表，每个文本是标记列表\n",
    "        embedding_dim: 嵌入维度\n",
    "        context_size: 上下文大小\n",
    "        epochs: 训练轮数\n",
    "        batch_size: 批次大小\n",
    "        lr: 学习率\n",
    "        \n",
    "    Returns:\n",
    "        训练好的词嵌入层\n",
    "    \"\"\"\n",
    "    # 构建词汇表\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_from_texts(texts)\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    dataset = CBOWDataset(texts, vocab, context_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 初始化模型、损失函数和优化器\n",
    "    model = CBOW(vocab.vocab_size, embedding_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 训练模型\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for context, target in dataloader:\n",
    "            # 前向传播\n",
    "            output = model(context)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    # 返回训练好的词嵌入层\n",
    "    return model.embeddings\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "def test_word_embedding():\n",
    "    \"\"\"测试词嵌入功能\"\"\"\n",
    "    print(\"===== 测试词嵌入 =====\")\n",
    "    \n",
    "    # 示例文本\n",
    "    texts = [\n",
    "        [\"I\", \"like\", \"to\", \"play\", \"football\"],\n",
    "        [\"Football\", \"is\", \"a\", \"popular\", \"sport\"],\n",
    "        [\"I\", \"enjoy\", \"watching\", \"football\", \"matches\"],\n",
    "        [\"Do\", \"you\", \"play\", \"any\", \"sports\"],\n",
    "        [\"Sports\", \"are\", \"good\", \"for\", \"health\"]\n",
    "    ]\n",
    "    \n",
    "    # 训练CBOW模型获取词嵌入\n",
    "    embedding_dim = 10\n",
    "    context_size = 2\n",
    "    embeddings = train_cbow_model(\n",
    "        texts=texts,\n",
    "        embedding_dim=embedding_dim,\n",
    "        context_size=context_size,\n",
    "        epochs=50,\n",
    "        batch_size=4,\n",
    "        lr=0.01\n",
    "    )\n",
    "    \n",
    "    # 获取词汇表\n",
    "    vocab = Vocabulary()\n",
    "    vocab.build_from_texts(texts)\n",
    "    \n",
    "    # 测试词嵌入查找\n",
    "    test_words = [\"I\", \"football\", \"sports\", \"unknown\"]\n",
    "    print(\"\\n词嵌入示例:\")\n",
    "    for word in test_words:\n",
    "        word_id = vocab.encode([word])[0]\n",
    "        word_vector = embeddings(torch.tensor(word_id, dtype=torch.long)).detach().numpy()\n",
    "        print(f\"{word}: {word_vector[:5]}... (shape: {word_vector.shape})\")\n",
    "    \n",
    "    # 计算词之间的相似度\n",
    "    print(\"\\n词相似度分析:\")\n",
    "    target_words = [\"football\", \"sports\", \"play\"]\n",
    "    for target in target_words:\n",
    "        target_id = vocab.encode([target])[0]\n",
    "        target_vector = embeddings(torch.tensor(target_id, dtype=torch.long))\n",
    "        \n",
    "        print(f\"\\n与 '{target}' 最相似的词:\")\n",
    "        similarities = []\n",
    "        for word, idx in vocab.token_to_idx.items():\n",
    "            if word in [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]:\n",
    "                continue\n",
    "            \n",
    "            word_vector = embeddings(torch.tensor(idx, dtype=torch.long))\n",
    "            # 计算余弦相似度\n",
    "            sim = torch.cosine_similarity(target_vector.unsqueeze(0), \n",
    "                                         word_vector.unsqueeze(0)).item()\n",
    "            similarities.append((word, sim))\n",
    "        \n",
    "        # 按相似度排序\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 打印前3个相似词\n",
    "        for word, sim in similarities[:3]:\n",
    "            print(f\"  {word}: {sim:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_word_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1114d950",
   "metadata": {},
   "source": [
    "\n",
    "    ===== 测试词嵌入 =====\n",
    "    Epoch 1/50, Loss: 3.1725\n",
    "    Epoch 2/50, Loss: 3.2624\n",
    "    Epoch 3/50, Loss: 3.1252\n",
    "    Epoch 4/50, Loss: 2.7059\n",
    "    Epoch 5/50, Loss: 2.4706\n",
    "    Epoch 6/50, Loss: 2.3471\n",
    "    Epoch 7/50, Loss: 2.5184\n",
    "    Epoch 8/50, Loss: 2.4183\n",
    "    Epoch 9/50, Loss: 2.3172\n",
    "    Epoch 10/50, Loss: 2.2042\n",
    "    Epoch 11/50, Loss: 2.2335\n",
    "    Epoch 12/50, Loss: 1.9774\n",
    "    Epoch 13/50, Loss: 1.8652\n",
    "    Epoch 14/50, Loss: 1.7527\n",
    "    Epoch 15/50, Loss: 1.8568\n",
    "    Epoch 16/50, Loss: 1.6976\n",
    "    Epoch 17/50, Loss: 1.4242\n",
    "    Epoch 18/50, Loss: 1.2158\n",
    "    Epoch 19/50, Loss: 1.1381\n",
    "    Epoch 20/50, Loss: 1.0590\n",
    "    Epoch 21/50, Loss: 1.2977\n",
    "    Epoch 22/50, Loss: 1.3026\n",
    "    Epoch 23/50, Loss: 0.9900\n",
    "    Epoch 24/50, Loss: 0.7884\n",
    "    Epoch 25/50, Loss: 1.0041\n",
    "    Epoch 26/50, Loss: 0.8770\n",
    "    Epoch 27/50, Loss: 0.8645\n",
    "    Epoch 28/50, Loss: 0.7021\n",
    "    Epoch 29/50, Loss: 0.8588\n",
    "    Epoch 30/50, Loss: 0.6019\n",
    "    Epoch 31/50, Loss: 0.4728\n",
    "    Epoch 32/50, Loss: 0.5092\n",
    "    Epoch 33/50, Loss: 0.4671\n",
    "    Epoch 34/50, Loss: 0.4910\n",
    "    Epoch 35/50, Loss: 0.5640\n",
    "    Epoch 36/50, Loss: 0.5198\n",
    "    Epoch 37/50, Loss: 0.4731\n",
    "    Epoch 38/50, Loss: 0.4264\n",
    "    Epoch 39/50, Loss: 0.3248\n",
    "    Epoch 40/50, Loss: 0.2492\n",
    "    Epoch 41/50, Loss: 0.2738\n",
    "    Epoch 42/50, Loss: 0.3748\n",
    "    Epoch 43/50, Loss: 0.2021\n",
    "    Epoch 44/50, Loss: 0.2448\n",
    "    Epoch 45/50, Loss: 0.1978\n",
    "    Epoch 46/50, Loss: 0.2853\n",
    "    Epoch 47/50, Loss: 0.1764\n",
    "    Epoch 48/50, Loss: 0.1442\n",
    "    Epoch 49/50, Loss: 0.1716\n",
    "    Epoch 50/50, Loss: 0.2133\n",
    "    \n",
    "    词嵌入示例:\n",
    "    I: [-0.0588957  -0.14068426 -0.7404043  -1.8865429  -2.6835012 ]... (shape: (10,))\n",
    "    football: [ 0.637025    0.14052066 -0.848007    0.2889565  -0.2740498 ]... (shape: (10,))\n",
    "    sports: [-1.045044   -0.70745003 -2.0171206   1.0361644   0.60308105]... (shape: (10,))\n",
    "    unknown: [-0.14042336  0.71746343  0.11502329 -0.5219049   0.10613117]... (shape: (10,))\n",
    "    \n",
    "    词相似度分析:\n",
    "    \n",
    "    与 'football' 最相似的词:\n",
    "      football: 1.0000\n",
    "      is: 0.7577\n",
    "      health: 0.6264\n",
    "    \n",
    "    与 'sports' 最相似的词:\n",
    "      sports: 1.0000\n",
    "      health: 0.5747\n",
    "      is: 0.5725\n",
    "    \n",
    "    与 'play' 最相似的词:\n",
    "      play: 1.0000\n",
    "      popular: 0.6973\n",
    "      is: 0.6605\n",
    "    \n",
    "\n",
    "## 8.编码词位置\n",
    "\n",
    "LLM 的自注意力机制本身不感知词的位置，因此需加入 “位置嵌入” 以传达词在序列中的顺序信息（如图 2.17 所示）。位置嵌入有两种类型：绝对位置嵌入（与具体位置绑定，如第 1 个词用特定向量，第 2 个词用另一向量）和相对位置嵌入（关注词之间的距离，如 “相距 2 个位置”）。\n",
    "\n",
    "GPT 模型采用绝对位置嵌入，其实现方式是：创建另一个嵌入层，输入为 0 至上下文长度 - 1 的位置索引（如窗口大小为 4 时，输入为 [0, 1, 2, 3]），输出与词嵌入维度相同的位置向量。最终输入嵌入是词嵌入与位置嵌入的总和（如图 2.19 所示），例如某词的嵌入向量为 [1.2, -0.2, -0.1]，其位置嵌入为 [0.5, 0.3, 0.1]，则最终输入为 [1.7, 0.1, 0.0]。\n",
    "\n",
    "位置嵌入的维度与词嵌入一致，确保两者可直接相加，且会随模型训练优化，使模型能学习到词序对语义的影响（如 “我爱你” 与 “你爱我” 的区别）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee56d1f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "# 1初始化分词器（使用GPT-2的BPE分词器）\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# 定义嵌入参数\n",
    "vocab_size = 50257  # GPT-2的词汇表大小\n",
    "output_dim = 256    # 嵌入维度（示例用256，实际GPT-3为12288）\n",
    "context_length = 4  # 上下文长度（即输入序列的最大长度）\n",
    "\n",
    "# 创建词嵌入层和位置嵌入层\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# 生成位置索引（0到context_length-1）\n",
    "pos_indices = torch.arange(context_length)  # 形状: [4]\n",
    "pos_embeddings = pos_embedding_layer(pos_indices)  # 形状: [4, 256]\n",
    "\n",
    "# 测试：将词ID转换为嵌入并添加位置嵌入\n",
    "def test_position_embedding():\n",
    "    # 示例输入词ID\n",
    "    input_ids = torch.tensor([\n",
    "        [40, 367, 2885, 1464],    # 第一句的词ID\n",
    "        [1807, 3619, 402, 271],   # 第二句的词ID\n",
    "        [10899, 2138, 257, 7026]  # 第三句的词ID\n",
    "    ])\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    print(f\"输入词ID形状: {input_ids.shape}\")  # 应输出: torch.Size([3, 4])\n",
    "\n",
    "    # 生成词嵌入\n",
    "    token_embeddings = token_embedding_layer(input_ids)\n",
    "    print(f\"词嵌入形状: {token_embeddings.shape}\")  # 应输出: torch.Size([3, 4, 256])\n",
    "\n",
    "    # 添加位置嵌入\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    print(f\"添加位置嵌入后的形状: {input_embeddings.shape}\")  # 应输出: torch.Size([3, 4, 256])\n",
    "\n",
    "    # 验证位置嵌入的唯一性\n",
    "    print(\"\\n位置嵌入向量（前3个位置的前5维）:\")\n",
    "    for i in range(3):\n",
    "        print(f\"位置 {i}: {pos_embeddings[i, :5]}\")\n",
    "\n",
    "# 执行测试\n",
    "if __name__ == \"__main__\":\n",
    "    test_position_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf8e94",
   "metadata": {},
   "source": [
    "\n",
    "    输入词ID形状: torch.Size([3, 4])\n",
    "    词嵌入形状: torch.Size([3, 4, 256])\n",
    "    添加位置嵌入后的形状: torch.Size([3, 4, 256])\n",
    "    \n",
    "    位置嵌入向量（前3个位置的前5维）:\n",
    "    位置 0: tensor([-0.3552, -0.5629, -1.4778,  0.7029, -0.0278], grad_fn=<SliceBackward0>)\n",
    "    位置 1: tensor([-0.7520,  0.3258,  0.5109, -1.2897,  0.2495], grad_fn=<SliceBackward0>)\n",
    "    位置 2: tensor([-0.6930,  0.9321, -0.9753,  0.5288,  0.8013], grad_fn=<SliceBackward0>)\n",
    "    \n",
    "\n",
    "## 贡献者主页\n",
    "\n",
    "\n",
    "|贡献者|学校  | 研究方向           |   GitHub主页 |\n",
    "|-----------------|------------------------|-----------------------|------------|\n",
    "| 蔡鋆捷 | 福州大学  |    Computer Vision（CV），Natural Language Processing（NLP）      |https://github.com/xinala-781|"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
