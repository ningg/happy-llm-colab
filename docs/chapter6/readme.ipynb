{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec72817",
   "metadata": {},
   "source": [
    "# 第六章 基于 transformers 的 LLM 训练\n",
    "\n",
    "注：本章的核心内容是，基于 transformers 框架实现 LLM 预训练和微调\n",
    "\n",
    "1. 框架简述：\n",
    "   1. transformers\n",
    "   2. deepspeed\n",
    "   3. peft\n",
    "   4. wandb\n",
    "   5. tokenizers\n",
    "2. 基于 transformers 的 LLM 预训练\n",
    "   1. 分词器训练\n",
    "   2. 数据集构建\n",
    "   3. 模型搭建/继承预训练模型\n",
    "   4. 构造 Trainer 进行训练\n",
    "3. 基于 transformers 的 LLM SFT/下游任务微调\n",
    "   1. 分词器训练\n",
    "   2. 数据集构建\n",
    "   3. LoRA 配置\n",
    "   4. 继承预训练模型\n",
    "   5. 构造 Trainer 进行训练"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
