{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9726c84",
   "metadata": {},
   "source": [
    "# 第五章 动手搭建大模型\n",
    "\n",
    "## 5.1 动手实现一个 LLaMA2 大模型\n",
    "\n",
    "Meta（原Facebook）于2023年2月发布第一款基于Transformer结构的大型语言模型LLaMA，并于同年7月发布同系列模型LLaMA2。我们在第四章已经学习和了解了LLM，以及如何训练LLM等内容。本小节我们就来学习如何动手实现一个LLaMA2模型。\n",
    "\n",
    "LLaMA2 模型结构如下图5.1所示：\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/5-images/LLama2.png\" alt=\"alt text\" width=\"100%\">\n",
    "    <p>图 5.1 LLaMA2结构</p>\n",
    "</div>\n",
    "\n",
    "### 5.1.1 定义超参数\n",
    "\n",
    "首先我们需要定义一些超参数，这些超参数包括模型的大小、层数、头数、词嵌入维度、隐藏层维度等等。这些超参数可以根据实际情况进行调整。\n",
    "\n",
    "这里我们自定义一个`ModelConfig`类，来存储和记录我们的超参数，这里我们继承了`PretrainedConfig`类，这是`transformers`库中的参数类，我们可以通过继承这个类来方便的使用`transformers`库中的一些功能，也方便在后续导出Hugging Face模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e32d04",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from transformers import PretrainedConfig\n",
    "\n",
    "class ModelConfig(PretrainedConfig):\n",
    "    model_type = \"Tiny-K\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int = 768, # 模型维度\n",
    "            n_layers: int = 12, # Transformer的层数\n",
    "            n_heads: int = 16, # 注意力机制的头数\n",
    "            n_kv_heads: int = 8, # 键值头的数量\n",
    "            vocab_size: int = 6144, # 词汇表大小\n",
    "            hidden_dim: int = None, # 隐藏层维度\n",
    "            multiple_of: int = 64, \n",
    "            norm_eps: float = 1e-5, # 归一化层的eps\n",
    "            max_seq_len: int = 512, # 最大序列长度\n",
    "            dropout: float = 0.0, # dropout概率\n",
    "            flash_attn: bool = True, # 是否使用Flash Attention\n",
    "            **kwargs,\n",
    "    ):\n",
    "        self.dim = dim\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.multiple_of = multiple_of\n",
    "        self.norm_eps = norm_eps\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = dropout\n",
    "        self.flash_attn = flash_attn\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a7982",
   "metadata": {},
   "source": [
    "\n",
    "> 在以下代码中出现 `args` 时，即默认为以上 `ModelConfig` 参数配置。\n",
    "\n",
    "我们来看一下其中的一些超参数的含义，比如`dim`是模型维度，`n_layers`是Transformer的层数，`n_heads`是注意力机制的头数，`vocab_size`是词汇表大小，`max_seq_len`是输入的最大序列长度等等。上面的代码中也对每一个参数做了详细的注释，在后面的代码中我们会根据这些超参数来构建我们的模型。\n",
    "\n",
    "### 5.1.2 构建 RMSNorm\n",
    "\n",
    "`RMSNorm`可以用如下的数学公式表示：\n",
    "\n",
    "$$\n",
    "\\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}x_i^2 + \\epsilon}} \\cdot \\gamma\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $x_i$ 是输入向量的第 $i$ 个元素\n",
    "- $\\gamma$ 是可学习的缩放参数（对应代码中的 `self.weight`）\n",
    "- $n$ 是输入向量的维度数量\n",
    "- $\\epsilon$ 是一个小常数，用于数值稳定性（以避免除以零的情况）\n",
    "\n",
    "这种归一化有助于通过确保权重的规模不会变得过大或过小来稳定学习过程，这在具有许多层的深度学习模型中特别有用。\n",
    "\n",
    "我们可以通过如下代码实现`RMSNorm`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24e968",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float):\n",
    "        super().__init__()\n",
    "        # eps是为了防止除以0的情况\n",
    "        self.eps = eps\n",
    "        # weight是一个可学习的参数，全部初始化为1\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # 计算RMSNorm的核心部分\n",
    "        # x.pow(2).mean(-1, keepdim=True)计算了输入x的平方的均值\n",
    "        # torch.rsqrt是平方根的倒数，这样就得到了RMSNorm的分母部分，再加上eps防止分母为0\n",
    "        # 最后乘以x，得到RMSNorm的结果\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward函数是模型的前向传播\n",
    "        # 首先将输入x转为float类型，然后进行RMSNorm，最后再转回原来的数据类型\n",
    "        # 最后乘以weight，这是RMSNorm的一个可学习的缩放因子\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac3fbb",
   "metadata": {},
   "source": [
    "\n",
    "并且，我们可以用下面的代码来对`RMSNorm`模块进行测试，可以看到代码最终输出的形状为`torch.Size([1, 50, 288])`，与我们输入的形状一致，说明模块的实现是正确的，归一化并不会改变输入的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4ac2a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "norm = RMSNorm(args.dim, args.norm_eps)\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "output = norm(x)\n",
    "print(output.shape)\n",
    "\n",
    "out:\n",
    "torch.Size([1, 50, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3467a1fe",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1.3 构建 LLaMA2 Attention\n",
    "\n",
    "在 LLaMA2 模型中，虽然只有 LLaMA2-70B模型使用了分组查询注意力机制（Grouped-Query Attention，GQA），但我们依然选择使用 GQA 来构建我们的 LLaMA Attention 模块，它可以提高模型的效率，并节省一些显存占用。\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/5-images/llama2-attention.png\" alt=\"alt text\" width=\"50%\">\n",
    "    <p>图 5.2 LLaMA2 Attention 结构</p>\n",
    "</div>\n",
    "\n",
    "#### 5.1.3.1 repeat_kv\n",
    "\n",
    "在 LLaMA2 模型中，我们需要将键和值的维度扩展到和查询的维度一样，这样才能进行注意力计算。我们可以通过如下代码实现`repeat_kv`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55b69fa",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    # 获取输入张量的形状：批量大小、序列长度、键/值对头的数量、每个头的维度大小\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    \n",
    "    # 如果重复次数为1，则不需要重复，直接返回原始张量\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    \n",
    "    # 对张量进行扩展和重塑操作以重复键值对\n",
    "    return (\n",
    "        x[:, :, :, None, :]  # 在第四个维度（头的维度前）添加一个新的维度\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)  # 将新添加的维度扩展到n_rep大小，实现重复的效果\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)  # 重新塑形，合并键/值对头的数量和重复次数的维度\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1d163",
   "metadata": {},
   "source": [
    "\n",
    "在上述代码中：\n",
    "\n",
    "- 首先，获取输入张量的形状：首先，代码通过 x.shape 获取输入张量的形状，包括批量大小（bs）、序列长度（slen）、键/值对头的数量（n_kv_heads）以及每个头的维度大小（head_dim）。\n",
    "\n",
    "- 然后，检查重复次数：接着，代码检查重复次数 n_rep 是否为1。如果是1，则说明不需要对键和值进行重复，直接返回原始张量 x。\n",
    "\n",
    "- 最后，扩展和重塑张量：\n",
    "  - 在第三个维度（即键/值对头的维度）之后添加一个新的维度，形成 `x[:, :, :, None, :]`。\n",
    "  - 使用 `expand` 方法将新添加的维度扩展到 `n_rep` 大小，实现键/值对的重复效果。\n",
    "  - 最后，通过 reshape 方法重新塑形，将扩展后的维度合并回键/值对头的数量中，即 `x.reshape(bs, slen, n_kv_heads * n_rep, head_dim)`，这样最终的张量形状就达到了与查询维度一致的效果。\n",
    "\n",
    "#### 5.1.3.2 旋转嵌入\n",
    "\n",
    "接着我们来实现旋转嵌入，旋转嵌入是 LLaMA2 模型中的一个重要组件，它可以为注意力机制提供更强的上下文信息，从而提高模型的性能。\n",
    "\n",
    "首先，我们要构造获得旋转嵌入的实部和虚部的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b43211",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 注意：此处的dim应为 dim//n_head，因为我们是对每个head进行旋转嵌入\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    # torch.arange(0, dim, 2)[: (dim // 2)].float()生成了一个从0开始，步长为2的序列，长度为dim的一半\n",
    "    # 然后每个元素除以dim，再取theta的倒数，得到频率\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    # 生成一个从0到end的序列，长度为end\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    # 计算外积，得到一个二维矩阵，每一行是t的元素乘以freqs的元素\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    # 计算频率的余弦值，得到实部\n",
    "    freqs_cos = torch.cos(freqs)\n",
    "    # 计算频率的正弦值，得到虚部\n",
    "    freqs_sin = torch.sin(freqs)\n",
    "    return freqs_cos, freqs_sin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c484ed",
   "metadata": {},
   "source": [
    "\n",
    "- 计算频率序列：\n",
    "  - `torch.arange(0, dim, 2)[: (dim // 2)].float()` 生成了一个从0开始，步长为2的序列，其长度为`dim`的一半。\n",
    "  - 每个元素除以`dim`后取`theta`的倒数，得到一个频率序列 `freqs`。这一步是为了生成适合旋转嵌入的频率。\n",
    "- 生成时间序列：\n",
    "  - `t = torch.arange(end, device=freqs.device)` 生成一个从`0`到`end`的序列，长度为`end`。`end`通常是序列的最大长度。\n",
    "- 计算频率的外积\n",
    "  - `freqs = torch.outer(t, freqs).float()` 计算时间序列 `t` 和频率序列 `freqs` 的外积，得到一个二维矩阵 `freqs`。每一行是时间序列 `t` 的元素乘以频率序列 `freqs` 的元素。\n",
    "- 计算实部和虚部\n",
    "  - `freqs_cos = torch.cos(freqs)` 计算频率矩阵 `freqs` 的余弦值，得到旋转嵌入的实部。\n",
    "  - `freqs_sin = torch.sin(freqs)` 计算频率矩阵 `freqs` 的正弦值，得到旋转嵌入的虚部。\n",
    "\n",
    "最终，该函数返回两个矩阵 `freqs_cos` 和 `freqs_sin`，分别表示旋转嵌入的实部和虚部，用于后续的计算。\n",
    "\n",
    "接着，我们来构造调整张量形状的`reshape_for_broadcast`函数，这个函数的主要目的是调整 `freqs_cis` 的形状，使其在进行广播操作时与 `x` 的维度对齐，从而能够进行正确的张量运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3b94ff",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    # 获取x的维度数\n",
    "    ndim = x.ndim\n",
    "    \n",
    "    # 断言，确保1在x的维度范围内\n",
    "    assert 0 <= 1 < ndim\n",
    "    \n",
    "    # 断言，确保freqs_cis的形状与x的第二维和最后一维相同\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    \n",
    "    # 构造一个新的形状，除了第二维和最后一维，其他维度都为1，这样做是为了能够将freqs_cis与x进行广播操作\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    \n",
    "    # 将freqs_cis调整为新的形状，并返回\n",
    "    return freqs_cis.view(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ec182",
   "metadata": {},
   "source": [
    "\n",
    "最后，我们可以通过如下代码实现旋转嵌入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95701cd",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cos: torch.Tensor,\n",
    "    freqs_sin: torch.Tensor\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    # 将查询和键张量转换为浮点数，并重塑形状以分离实部和虚部\n",
    "    xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "    xk_r, xk_i = xk.float().reshape(xk.shape[:-1] + (-1, 2)).unbind(-1)\n",
    "\n",
    "    # 重新塑形频率张量以进行广播\n",
    "    freqs_cos = reshape_for_broadcast(freqs_cos, xq_r)\n",
    "    freqs_sin = reshape_for_broadcast(freqs_sin, xq_r)\n",
    "\n",
    "    # 应用旋转，分别计算旋转后的实部和虚部\n",
    "    xq_out_r = xq_r * freqs_cos - xq_i * freqs_sin\n",
    "    xq_out_i = xq_r * freqs_sin + xq_i * freqs_cos\n",
    "    xk_out_r = xk_r * freqs_cos - xk_i * freqs_sin\n",
    "    xk_out_i = xk_r * freqs_sin + xk_i * freqs_cos\n",
    "\n",
    "    # 将最后两个维度合并，并还原为原始张量的形状\n",
    "    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(3)\n",
    "    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(3)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa66000",
   "metadata": {},
   "source": [
    "\n",
    "这里我们给出可以测试`apply_rotary_emb`函数的代码，大家也可以尝试在代码中添加断点，来查看每一步的计算结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcdd75c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "xq = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "xk = torch.randn(1, 50, 6, 48) # bs, seq_len, dim//n_head, n_head_dim\n",
    "\n",
    "# 使用 precompute_freqs_cis 函数获取 sin和cos\n",
    "cos, sin = precompute_freqs_cis(288//6, 50)\n",
    "print(cos.shape, sin.shape)\n",
    "xq_out, xk_out = apply_rotary_emb(xq, xk, cos, sin)\n",
    "\n",
    "xq_out.shape, xk_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9947644",
   "metadata": {},
   "source": [
    "\n",
    "OUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e976b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([50, 24]) torch.Size([50, 24])\n",
    "\n",
    "(torch.Size([1, 50, 6, 48]), torch.Size([1, 50, 6, 48]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ee638",
   "metadata": {},
   "source": [
    "\n",
    "#### 5.1.3.3 组装 LLaMA2 Attention\n",
    "\n",
    "在上面我们已经完成了旋转嵌入的实现，接下来我们就可以构建 LLaMA2 Attention 模块了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e5e60",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, args: ModelConfig):\n",
    "        super().__init__()\n",
    "        # 根据是否指定n_kv_heads，确定用于键（key）和值（value）的头的数量。\n",
    "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
    "        # 确保总头数可以被键值头数整除。\n",
    "        assert args.n_heads % self.n_kv_heads == 0\n",
    "\n",
    "        # 模型并行处理大小，默认为1。\n",
    "        model_parallel_size = 1\n",
    "        # 本地计算头数，等于总头数除以模型并行处理大小。\n",
    "        self.n_local_heads = args.n_heads // model_parallel_size\n",
    "        # 本地键值头数，等于键值头数除以模型并行处理大小。\n",
    "        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n",
    "        # 重复次数，用于扩展键和值的尺寸。\n",
    "        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n",
    "        # 每个头的维度，等于模型维度除以头的总数。\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "\n",
    "        # 定义权重矩阵。\n",
    "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
    "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
    "        # 输出权重矩阵。\n",
    "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
    "\n",
    "        # 定义dropout。\n",
    "        self.attn_dropout = nn.Dropout(args.dropout)\n",
    "        self.resid_dropout = nn.Dropout(args.dropout)\n",
    "        # 保存dropout概率。\n",
    "        self.dropout = args.dropout\n",
    "\n",
    "        # 检查是否使用Flash Attention（需要PyTorch >= 2.0）。\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            # 若不支持Flash Attention，则使用手动实现的注意力机制，并设置mask。\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # 创建一个上三角矩阵，用于遮蔽未来信息。\n",
    "            mask = torch.full((1, 1, args.max_seq_len, args.max_seq_len), float(\"-inf\"))\n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "            # 注册为模型的缓冲区\n",
    "            self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor):\n",
    "        # 获取批次大小和序列长度，[batch_size, seq_len, dim]\n",
    "        bsz, seqlen, _ = x.shape\n",
    "\n",
    "        # 计算查询（Q）、键（K）、值（V）。\n",
    "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
    "        # 调整形状以适应头的维度。\n",
    "        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)\n",
    "        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)\n",
    "\n",
    "        # 应用旋转位置嵌入（RoPE）。\n",
    "        xq, xk = apply_rotary_emb(xq, xk, freqs_cos, freqs_sin)\n",
    "\n",
    "        # 对键和值进行扩展以适应重复次数。\n",
    "        xk = repeat_kv(xk, self.n_rep)\n",
    "        xv = repeat_kv(xv, self.n_rep)\n",
    "\n",
    "        # 将头作为批次维度处理。\n",
    "        xq = xq.transpose(1, 2)\n",
    "        xk = xk.transpose(1, 2)\n",
    "        xv = xv.transpose(1, 2)\n",
    "\n",
    "        # 根据是否支持Flash Attention，选择实现方式。\n",
    "        if self.flash:\n",
    "            # 使用Flash Attention。\n",
    "            output = torch.nn.functional.scaled_dot_product_attention(xq, xk, xv, attn_mask=None, dropout_p=self.dropout if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            # 使用手动实现的注意力机制。\n",
    "            scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "            assert hasattr(self, 'mask')\n",
    "            scores = scores + self.mask[:, :, :seqlen, :seqlen]\n",
    "            scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            scores = self.attn_dropout(scores)\n",
    "            output = torch.matmul(scores, xv)\n",
    "\n",
    "        # 恢复时间维度并合并头。\n",
    "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
    "\n",
    "        # 最终投影回残差流。\n",
    "        output = self.wo(output)\n",
    "        output = self.resid_dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d029a593",
   "metadata": {},
   "source": [
    "\n",
    "同样大家可以使用下面的代码来对注意力模块进行测试，可以看到代码最终输出的形状为`torch.Size([1, 50, 768])`，与我们输入的形状一致，说明模块的实现是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7541be88",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 创建Attention实例\n",
    "attention_model = Attention(args)\n",
    "\n",
    "# 模拟输入数据\n",
    "batch_size = 1\n",
    "seq_len = 50  # 假设实际使用的序列长度为50\n",
    "dim = args.dim\n",
    "x = torch.rand(batch_size, seq_len, dim)  # 随机生成输入张量\n",
    "# freqs_cos = torch.rand(seq_len, dim // 2)  # 模拟cos频率，用于RoPE\n",
    "# freqs_sin = torch.rand(seq_len, dim // 2)  # 模拟sin频率，用于RoPE\n",
    "\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)\n",
    "\n",
    "# 运行Attention模型\n",
    "output = attention_model(x, freqs_cos, freqs_sin)\n",
    "\n",
    "# attention出来之后的形状 依然是[batch_size, seq_len, dim]\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fcb755",
   "metadata": {},
   "source": [
    "\n",
    "OUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eca581",
   "metadata": {},
   "outputs": [],
   "source": [
    "Output shape: torch.Size([1, 50, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22018572",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1.4 构建 LLaMA2 MLP模块\n",
    "\n",
    "相对于前面我们实现的LLaMA2 Attention模块，LLaMA2 MLP模块的实现要简单一些。我们可以通过如下代码实现`MLP`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72daa9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # 如果没有指定隐藏层的维度，我们将其设置为输入维度的4倍\n",
    "        # 然后将其减少到2/3，最后确保它是multiple_of的倍数\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * dim\n",
    "            hidden_dim = int(2 * hidden_dim / 3)\n",
    "            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        # 定义第一层线性变换，从输入维度到隐藏维度\n",
    "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        # 定义第二层线性变换，从隐藏维度到输入维度\n",
    "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "        # 定义第三层线性变换，从输入维度到隐藏维度\n",
    "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        # 定义dropout层，用于防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向传播函数\n",
    "        # 首先，输入x通过第一层线性变换和SILU激活函数\n",
    "        # 然后，结果乘以输入x通过第三层线性变换的结果\n",
    "        # 最后，通过第二层线性变换和dropout层\n",
    "        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3554b6c",
   "metadata": {},
   "source": [
    "\n",
    "我们着重观察一下`forward`函数的实现，首先，输入 `x` 通过第一层线性变换 `self.w1` 和 `SILU` 激活函数，然后，结果乘以输入 `x` 通过第三层线性变换 `self.w3` 的结果，最后，通过第二层线性变换 `self.w2` 和 `dropout` 层，得到最终输出。\n",
    "\n",
    "同样大家可以使用下面的代码来对`LLaMAMLP`模块进行测试，可以看到代码最终输出的形状为`torch.Size([1, 50, 768])`，与我们输入的形状一致，说明模块的实现是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba8224",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 创建MLP实例\n",
    "mlp = MLP(args.dim, args.hidden_dim, args.multiple_of, args.dropout)\n",
    "# 随机生成数据\n",
    "x = torch.randn(1, 50, args.dim)\n",
    "# 运行MLP模型\n",
    "output = mlp(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa67ab",
   "metadata": {},
   "source": [
    "\n",
    "OUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b17a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([1, 50, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa89ff",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1.5 LLaMA2 Decoder Layer\n",
    "\n",
    "到这里，我们已经实现了`LLaMA2`模型的`Attention`模块和`MLP`模块，接下来我们就可以构建`LLaMA2`的`Decoder Layer`了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2962d4",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, layer_id: int, args: ModelConfig):\n",
    "        super().__init__()\n",
    "        # 定义多头注意力的头数\n",
    "        self.n_heads = args.n_heads\n",
    "        # 定义输入维度\n",
    "        self.dim = args.dim\n",
    "        # 定义每个头的维度，等于输入维度除以头数\n",
    "        self.head_dim = args.dim // args.n_heads\n",
    "        # 定义LLaMA2Attention对象，用于进行多头注意力计算\n",
    "        self.attention = Attention(args)\n",
    "        # 定义LLaMAMLP对象，用于进行前馈神经网络计算\n",
    "        self.feed_forward = MLP(\n",
    "            dim=args.dim,\n",
    "            hidden_dim=args.hidden_dim,\n",
    "            multiple_of=args.multiple_of,\n",
    "            dropout=args.dropout,\n",
    "        )\n",
    "        # 定义层的ID\n",
    "        self.layer_id = layer_id\n",
    "        # 定义注意力计算的归一化层\n",
    "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        # 定义前馈神经网络计算的归一化层\n",
    "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "\n",
    "    def forward(self, x, freqs_cos, freqs_sin):\n",
    "        # 前向传播函数\n",
    "        # 首先，输入x经过注意力归一化层，然后进行注意力计算，结果与输入x相加得到h\n",
    "        # 然后，h经过前馈神经网络归一化层，然后进行前馈神经网络计算，结果与h相加得到输出\n",
    "        h = x + self.attention.forward(self.attention_norm(x), freqs_cos, freqs_sin)\n",
    "        out = h + self.feed_forward.forward(self.ffn_norm(h))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb3128",
   "metadata": {},
   "source": [
    "\n",
    "`DecoderLayer`就是把我们上面完成的`Attention`模块和`MLP`模块组合在一起，实现了一个完整的`Transformer`模块。\n",
    "\n",
    "同样大家可以使用下面的代码来对`DecoderLayer`模块进行测试，可以看到代码最终输出的形状为`torch.Size([1, 50, 768])`，与我们输入的形状一致，说明模块的实现是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec9fbc",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 创建LLaMADecoderLayer实例\n",
    "decoderlayer = DecoderLayer(0, args)\n",
    "\n",
    "# 模拟输入数据\n",
    "dim = args.dim\n",
    "seq_len = 50\n",
    "\n",
    "x = torch.randn(1, seq_len, dim) # [bs, seq_len, dim]\n",
    "\n",
    "freqs_cos, freqs_sin = precompute_freqs_cis(dim//args.n_heads, seq_len)\n",
    "\n",
    "out = decoderlayer(x, freqs_cos, freqs_sin)\n",
    "\n",
    "print(out.shape) # 形状和输入的x一样 [batch_size, seq_len, dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5ae99",
   "metadata": {},
   "source": [
    "\n",
    "OUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7116fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([1, 50, 768])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58caadae",
   "metadata": {},
   "source": [
    "\n",
    "### 5.1.6 构建 LLaMA2 模型\n",
    "\n",
    "好了，我们已经完了上述所有的模块的实现，接下来就是激动人心的时刻，我们可以构建`LLaMA2`模型了。，`LLaMA2`模型就是将`DecoderLayer`模块堆叠起来，构成一个完整的`Transformer`模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34030669",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Transformer(PreTrainedModel):\n",
    "    config_class = ModelConfig  # 配置类\n",
    "    last_loss: Optional[torch.Tensor] # 记录最后一次计算的损失\n",
    "\n",
    "    def __init__(self, args: ModelConfig = None):\n",
    "        super().__init__(args)\n",
    "        # 初始化模型参数\n",
    "        self.args = args\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = args.vocab_size\n",
    "        # 层数\n",
    "        self.n_layers = args.n_layers\n",
    "\n",
    "        # 词嵌入层\n",
    "        self.tok_embeddings = nn.Embedding(args.vocab_size, args.dim)\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        # Decoder层\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for layer_id in range(args.n_layers):\n",
    "            self.layers.append(DecoderLayer(layer_id, args))\n",
    "        # 归一化层\n",
    "        self.norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
    "        # 输出层\n",
    "        self.output = nn.Linear(args.dim, args.vocab_size, bias=False)\n",
    "\n",
    "        # 将词嵌入层的权重与输出层的权重共享\n",
    "        self.tok_embeddings.weight = self.output.weight \n",
    "\n",
    "        # 预计算相对位置嵌入的频率\n",
    "        freqs_cos, freqs_sin = precompute_freqs_cis(self.args.dim // self.args.n_heads, self.args.max_seq_len)\n",
    "        self.register_buffer(\"freqs_cos\", freqs_cos, persistent=False)\n",
    "        self.register_buffer(\"freqs_sin\", freqs_sin, persistent=False)\n",
    "\n",
    "        # 初始化所有权重\n",
    "        self.apply(self._init_weights)\n",
    "        # 对残差投影进行特殊的缩放初始化\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('w3.weight') or pn.endswith('wo.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * args.n_layers))\n",
    "\n",
    "        # 初始化最后一次前向传播的损失属性\n",
    "        self.last_loss = None\n",
    "        self.OUT = CausalLMOutputWithPast()  # 输出容器\n",
    "        self._no_split_modules = [name for name, _ in self.named_modules()]  # 不分割的模块列表\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # 初始化权重的函数\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None, **kwargs) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        - tokens: Optional[torch.Tensor], 输入 token 张量。\n",
    "        - targets: Optional[torch.Tensor], 目标 token 张量。\n",
    "        - kv_cache: bool, 是否使用键值缓存。\n",
    "        - kwargs: 其他关键字参数。\n",
    "\n",
    "        - self.OUT: CausalLMOutputWithPast, 包含 logits 和损失。\n",
    "        \"\"\"\n",
    "\n",
    "        if 'input_ids' in kwargs:\n",
    "            tokens = kwargs['input_ids']\n",
    "        if 'attention_mask' in kwargs:\n",
    "            targets = kwargs['attention_mask']\n",
    "\n",
    "        # 前向传播函数\n",
    "        _bsz, seqlen = tokens.shape\n",
    "        # 通过词嵌入层和Dropout层\n",
    "        h = self.tok_embeddings(tokens)\n",
    "        h = self.dropout(h)\n",
    "        # 获取相对位置嵌入的频率\n",
    "        freqs_cos = self.freqs_cos[:seqlen]\n",
    "        freqs_sin = self.freqs_sin[:seqlen]\n",
    "\n",
    "        # 通过Decoder层\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, freqs_cos, freqs_sin)\n",
    "        # 通过归一化层\n",
    "        h = self.norm(h)\n",
    "\n",
    "        if targets is not None:\n",
    "            # 如果给定了目标，计算损失\n",
    "            logits = self.output(h)\n",
    "            self.last_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=0, reduction='none')\n",
    "        else:\n",
    "            # 推理时的小优化：只对最后一个位置的输出进行前向传播\n",
    "            logits = self.output(h[:, [-1], :]) \n",
    "            self.last_loss = None\n",
    "\n",
    "        # 设置输出\n",
    "        self.OUT.__setitem__('logits', logits)\n",
    "        self.OUT.__setitem__('last_loss', self.last_loss)\n",
    "        return self.OUT\n",
    "\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def generate(self, idx, stop_id=None, max_new_tokens=256, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        给定输入序列 idx（形状为 (bz,seq_len) 的长整型张量），通过多次生成新 token 来完成序列。\n",
    "        在 model.eval() 模式下运行。效率较低的采样版本，没有使用键k/v cache。\n",
    "        \"\"\"\n",
    "        index = idx.shape[1]\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列上下文过长，截断它到最大长度\n",
    "            idx_cond = idx if idx.size(1) <= self.args.max_seq_len else idx[:, -self.args.max_seq_len:]\n",
    "            \n",
    "            # 前向传播获取序列中最后一个位置的 logits\n",
    "            logits = self(idx_cond).logits\n",
    "            logits = logits[:, -1, :] # 只保留最后一个时间步的输出\n",
    "            \n",
    "            if temperature == 0.0:\n",
    "                # 选择最有可能的索引\n",
    "                _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                # 缩放 logits 并应用 softmax\n",
    "                logits = logits / temperature\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "\n",
    "            if idx_next == stop_id:\n",
    "                break\n",
    "\n",
    "            # 将采样的索引添加到序列中并继续\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx[:, index:] # 只返回生成的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d016779",
   "metadata": {},
   "source": [
    "\n",
    "同样大家可以使用下面的代码来对`Transformer`模块进行测试，可以看到代码最终输出的形状为`torch.Size([1, 1, 6144])`，与我们输入的形状一致，说明模块的实现是正确的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67004e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# LLaMA2Model.forward 接受两个参数，tokens和targets，其中tokens是输入的张量, 应为int类型\n",
    "x = torch.randint(0, 6144, (1, 50)) # [bs, seq_len]\n",
    "# 实例化LLaMA2Model\n",
    "model = Transformer(args=args)\n",
    "# 计算model的全部参数\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print('Number of parameters:', num_params)\n",
    "\n",
    "out = model(x)\n",
    "print(out.logits.shape) # [batch_size, 1, vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690ab5e",
   "metadata": {},
   "source": [
    "\n",
    "OUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of parameters: 82594560\n",
    "torch.Size([1, 1, 6144])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb7c85",
   "metadata": {},
   "source": [
    "\n",
    "## 5.2 训练 Tokenizer\n",
    "\n",
    "在自然语言处理 (NLP) 中，Tokenizer 是一种将文本分解为较小单位（称为 token）的工具。这些 token 可以是词、子词、字符，甚至是特定的符号。Tokenization 是 NLP 中的第一步，直接影响后续处理和分析的效果。不同类型的 tokenizer 适用于不同的应用场景，以下是几种常见的 tokenizer 及其特点。\n",
    "\n",
    "### 5.2.1 Word-based Tokenizer\n",
    "\n",
    "**Word-based Tokenizer** 是最简单和直观的一种分词方法。它将文本按空格和标点符号分割成单词。这种方法的优点在于其简单和直接，易于实现，且与人类对语言的直觉相符。然而，它也存在一些明显的缺点，如无法处理未登录词（OOV，out-of-vocabulary）和罕见词，对复合词（如“New York”）或缩略词（如“don't”）的处理也不够精细。此外，Word-based Tokenizer 在处理不同语言时也会遇到挑战，因为一些语言（如中文、日文）没有显式的单词分隔符。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f186f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input: \"Hello, world! There is Datawhale.\"\n",
    "Output: [\"Hello\", \",\", \"world\", \"!\", \"There\", \"is\", \"Datawhale\", \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4246d7f8",
   "metadata": {},
   "source": [
    "\n",
    "在这个例子中，输入的句子被分割成一系列单词和标点符号，每个单词或标点符号都作为一个独立的 token。\n",
    "\n",
    "### 5.2.2 Character-based Tokenizer\n",
    "\n",
    "**Character-based Tokenizer** 将文本中的每个字符视为一个独立的 token。这种方法能非常精细地处理文本，适用于处理拼写错误、未登录词或新词。由于每个字符都是一个独立的 token，因此这种方法可以捕捉到非常细微的语言特征。这对于一些特定的应用场景，如生成式任务或需要处理大量未登录词的任务，特别有用。但是，这种方法也会导致 token 序列变得非常长，增加了模型的计算复杂度和训练时间。此外，字符级的分割可能会丢失一些词级别的语义信息，使得模型难以理解上下文。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cc50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input: \"Hello\"\n",
    "Output: [\"H\", \"e\", \"l\", \"l\", \"o\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e5620",
   "metadata": {},
   "source": [
    "\n",
    "在这个例子中，单词“Hello”被分割成单个字符，每个字符作为一个独立的 token。这种方法能够处理任何语言和字符集，具有极大的灵活性。\n",
    "\n",
    "### 5.2.3 Subword Tokenizer\n",
    "\n",
    "**Subword Tokenizer** 介于词和字符之间，能够更好地平衡分词的细粒度和处理未登录词的能力。Subword Tokenizer 的关键思想是将文本分割成比单词更小的单位，但又比字符更大，这样既能处理未知词，又能保持一定的语义信息。常见的子词分词方法包括 BPE、WordPiece 和 Unigram。\n",
    "\n",
    "#### （1）Byte Pair Encoding (BPE)\n",
    "\n",
    "**BPE** 是一种基于统计方法，通过反复合并频率最高的字符或字符序列对来生成子词词典。这种方法的优点在于其简单和高效，能够有效地处理未知词和罕见词，同时保持较低的词典大小。BPE 的合并过程是自底向上的，逐步将频率最高的字符对合并成新的子词，直到达到预定的词典大小或不再有高频的字符对。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dd066",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input: \"lower\"\n",
    "Output: [\"low\", \"er\"]\n",
    "\n",
    "Input: \"newest\"\n",
    "Output: [\"new\", \"est\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90322e22",
   "metadata": {},
   "source": [
    "\n",
    "在这个例子中，单词“lower”被分割成子词“low”和“er”，而“newest”被分割成“new”和“est”。这种方法有效地处理了词干和词缀，保持了单词的基本语义结构。\n",
    "\n",
    "#### （2）WordPiece\n",
    "\n",
    "**WordPiece** 是另一种基于子词的分词方法，最初用于谷歌的 BERT 模型。与 BPE 类似，WordPiece 通过最大化子词序列的似然函数来生成词典，但在合并子词时更注重语言模型的优化。WordPiece 会优先选择能够最大化整体句子概率的子词，使得分词结果在语言模型中具有更高的概率。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3fe368",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input: \"unhappiness\"\n",
    "Output: [\"un\", \"##happiness\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45066bd7",
   "metadata": {},
   "source": [
    "\n",
    "在这个例子中，单词“unhappiness”被分割成子词“un”和“##happiness”，其中“##”表示这是一个后缀子词。通过这种方式，WordPiece 能够更好地处理复合词和派生词，保留更多的语义信息。\n",
    "\n",
    "#### （3）Unigram\n",
    "\n",
    "**Unigram** 分词方法基于概率模型，通过选择具有最高概率的子词来分割文本。Unigram 词典是通过训练语言模型生成的，可以处理多种语言和不同类型的文本。Unigram 模型会为每个子词分配一个概率，然后根据这些概率进行最优分割。\n",
    "\n",
    "示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ff804",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input: \"unhappiness\"\n",
    "Output: [\"un\", \"happiness\"]\n",
    "\n",
    "Input: \"newest\"\n",
    "Output: [\"new\", \"est\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254ac24",
   "metadata": {},
   "source": [
    "\n",
    "在这个例子中，单词“unhappiness”被分割成子词“un”和“happiness”，而“newest”被分割成“new”和“est”。这种方法通过概率模型有效地处理了子词分割，使得分割结果更符合语言使用习惯。\n",
    "\n",
    "每种 Tokenizer 方法都有其特定的应用场景和优缺点，选择适合的 Tokenizer 对于自然语言处理任务的成功至关重要。\n",
    "\n",
    "### 5.2.4 训练一个 Tokenizer\n",
    "\n",
    "这里我们选择使用 BPE 算法来训练一个 Subword Tokenizer。BPE 是一种简单而有效的分词方法，能够处理未登录词和罕见词，同时保持较小的词典大小。我们将使用 Hugging Face 的 `tokenizers` 库来训练一个 BPE Tokenizer。\n",
    "\n",
    "\n",
    "#### Step 1: 安装和导入依赖库\n",
    "\n",
    "首先，我们需要安装 `tokenizers` 库，除此之外还需要安装 `datasets` 和 `transformers` 库，用于加载训练数据和加载训练完成后的 Tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a262b179",
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "pip install tokenizers datasets transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e8337",
   "metadata": {},
   "source": [
    "\n",
    "然后，导入所需的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff618d9b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tokenizers.normalizers import NFKC\n",
    "from typing import Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4f0d6",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: 加载训练数据\n",
    "\n",
    "这里我们使用与预训练相同的数据集（出门问问序列猴子开源数据集）训练tokenizer，可使用`code/download_dataset.sh` 和 `code/deal_dataset.py` 下载和预处理数据集。\n",
    "\n",
    "> 注：由于数据集过大，可能会导致在训练过程中内存不足。因为本项目为学习目的，建议学习者手动分割小部分数据集用于训练验证，笔者也在 Github 仓库中存放了训练好的 tokenizer，可以直接使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b77f84e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def read_texts_from_jsonl(file_path: str) -> Generator[str, None, None]:\n",
    "    \"\"\"读取JSONL文件并安全提取文本数据\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if 'text' not in data:\n",
    "                    raise KeyError(f\"Missing 'text' field in line {line_num}\")\n",
    "                yield data['text']\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON in line {line_num}\")\n",
    "                continue\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46644d7",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 3: 创建配置文件\n",
    "\n",
    "在训练 BPE Tokenizer 之前，我们需要创建一个完整的 `Tokenizer` 配置文件，包括 `tokenizer_config.json` 和 `special_tokens_map.json`。这些配置文件定义了 `Tokenizer` 的参数和特殊标记，用于训练和加载 `Tokenizer`。此处的`chat_template`我们与`Qwen2.5`模型保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def2c6e3",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def create_tokenizer_config(save_dir: str) -> None:\n",
    "    \"\"\"创建完整的tokenizer配置文件\"\"\"\n",
    "    config = {\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": False,\n",
    "        \"bos_token\": \"<|im_start|>\",\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        \"pad_token\": \"<|im_end|>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"model_max_length\": 1000000000000000019884624838656,\n",
    "        \"clean_up_tokenization_spaces\": False,\n",
    "        \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "        \"chat_template\": (\n",
    "            \"{% for message in messages %}\"\n",
    "            \"{% if message['role'] == 'system' %}\"\n",
    "            \"<|im_start|>system\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% elif message['role'] == 'user' %}\"\n",
    "            \"<|im_start|>user\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"<|im_start|>assistant\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% endif %}\"\n",
    "            \"{% endfor %}\"\n",
    "            \"{% if add_generation_prompt %}\"\n",
    "            \"{{ '<|im_start|>assistant\\n' }}\"\n",
    "            \"{% endif %}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # 保存主配置文件\n",
    "    with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 创建special_tokens_map.json\n",
    "    special_tokens_map = {\n",
    "        \"bos_token\": \"<|im_start|>\",\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<|im_end|>\",\n",
    "        \"additional_special_tokens\": [\"<s>\", \"</s>\"]\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(special_tokens_map, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bca34",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 4: 训练 BPE Tokenizer\n",
    "\n",
    "在训练 BPE Tokenizer 之前，我们需要定义一个训练函数，用于训练 Tokenizer 并保存训练好的 Tokenizer 文件。这里我们使用 `tokenizers` 库中的 `Tokenizer` 类来训练 BPE Tokenizer。\n",
    "\n",
    "可以看到我们在训练 Tokenizer 时，配置了一些特殊的 token，如 `<unk>`、`<s>`、`</s>`、`<|im_start|>` 和 `<|im_end|>`。这些 token 用于标记未知词、句子的开始和结束，以及对话的开始和结束。这些特殊 token 可以帮助模型更好地理解文本数据，提高模型的泛化能力和效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d0c580",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def train_tokenizer(data_path: str, save_dir: str, vocab_size: int = 8192) -> None:\n",
    "    \"\"\"训练并保存自定义tokenizer\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()  # 添加文本规范化\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # 配置特殊token\n",
    "    special_tokens = [\n",
    "        \"<unk>\", \n",
    "        \"<s>\", \n",
    "        \"</s>\", \n",
    "        \"<|im_start|>\", \n",
    "        \"<|im_end|>\"\n",
    "    ]\n",
    "\n",
    "    # 配置训练器\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        min_frequency=2,  # 提高低频词过滤\n",
    "        show_progress=True,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    "    )\n",
    "\n",
    "    # 训练tokenizer\n",
    "    print(f\"Training tokenizer with data from {data_path}\")\n",
    "    texts = read_texts_from_jsonl(data_path)\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer, length=os.path.getsize(data_path))\n",
    "\n",
    "    # 验证特殊token映射\n",
    "    try:\n",
    "        assert tokenizer.token_to_id(\"<unk>\") == 0\n",
    "        assert tokenizer.token_to_id(\"<s>\") == 1\n",
    "        assert tokenizer.token_to_id(\"</s>\") == 2\n",
    "        assert tokenizer.token_to_id(\"<|im_start|>\") == 3\n",
    "        assert tokenizer.token_to_id(\"<|im_end|>\") == 4\n",
    "    except AssertionError as e:\n",
    "        print(\"Special tokens mapping error:\", e)\n",
    "        raise\n",
    "\n",
    "    # 保存tokenizer文件\n",
    "    tokenizer.save(os.path.join(save_dir, \"tokenizer.json\"))\n",
    "    \n",
    "    # 创建配置文件\n",
    "    create_tokenizer_config(save_dir)\n",
    "    print(f\"Tokenizer saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cf44d9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Step 5: 使用训练好的 Tokenizer\n",
    "\n",
    "我们可以使用训练好的 Tokenizer 来处理文本数据，如编码、解码、生成对话等。下面是一个简单的示例，展示了如何使用训练好的 Tokenizer 来处理文本数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c4571",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def eval_tokenizer(tokenizer_path: str) -> None:\n",
    "    \"\"\"评估tokenizer功能\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    # 测试基本属性\n",
    "    print(\"\\n=== Tokenizer基本信息 ===\")\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "    print(f\"Special tokens: {tokenizer.all_special_tokens}\")\n",
    "    print(f\"Special token IDs: {tokenizer.all_special_ids}\")\n",
    "\n",
    "    # 测试聊天模板\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个AI助手。\"},\n",
    "        {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm fine, thank you. and you?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'm good too.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"That's great to hear!\"},\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== 聊天模板测试 ===\")\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        # add_generation_prompt=True\n",
    "    )\n",
    "    print(\"Generated prompt:\\n\", prompt, sep=\"\")\n",
    "\n",
    "    # 测试编码解码\n",
    "    print(\"\\n=== 编码解码测试 ===\")\n",
    "    encoded = tokenizer(prompt, truncation=True, max_length=256)\n",
    "    decoded = tokenizer.decode(encoded[\"input_ids\"], skip_special_tokens=False)\n",
    "    print(\"Decoded text matches original:\", decoded == prompt)\n",
    "\n",
    "    # 测试特殊token处理\n",
    "    print(\"\\n=== 特殊token处理 ===\")\n",
    "    test_text = \"<|im_start|>user\\nHello<|im_end|>\"\n",
    "    encoded = tokenizer(test_text).input_ids\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"Original: {test_text}\")\n",
    "    print(f\"Decoded:  {decoded}\")\n",
    "    print(\"Special tokens preserved:\", decoded == test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445db31d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb882e8b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "eval_tokenizer('your tokenizer path')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d61dc2",
   "metadata": {},
   "source": [
    "\n",
    "OUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3713e0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "=== Tokenizer基本信息 ===\n",
    "Vocab size: 6144\n",
    "Special tokens: ['<|im_start|>', '<|im_end|>', '<unk>', '<s>', '</s>']\n",
    "Special token IDs: [3, 4, 0, 1, 2]\n",
    "\n",
    "=== 聊天模板测试 ===\n",
    "Generated prompt:\n",
    "<|im_start|>system\n",
    "你是一个AI助手。<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I'm fine, thank you. and you?<|im_end|>\n",
    "<|im_start|>user\n",
    "I'm good too.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "That's great to hear!<|im_end|>\n",
    "\n",
    "\n",
    "=== 编码解码测试 ===\n",
    "Decoded text matches original: False\n",
    "\n",
    "=== 特殊token处理 ===\n",
    "Original: <|im_start|>user\n",
    "Hello<|im_end|>\n",
    "Decoded:  <|im_start|> user\n",
    "Hello<|im_end|>\n",
    "Special tokens preserved: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bcf457",
   "metadata": {},
   "source": [
    "\n",
    "## 5.3 预训练一个小型LLM\n",
    "\n",
    "在前面的章节中，我们熟悉了各种大模型的模型结构，以及如如何训练Tokenizer。在本节中，我们将动手训练一个八千万参数的LLM。\n",
    "\n",
    "### 5.3.1 数据下载\n",
    "\n",
    "首先，我们需要下载预训练数据集。在这里，我们使用两个开源的数据集，包含了大量的中文对话数据，可以用于训练对话生成模型。\n",
    "\n",
    "- 出门问问序列猴子开源数据集：出门问问序列猴子通用文本数据集由来自网页、百科、博客、问答、开源代码、书籍、报刊、专利、教材、考题等多种公开可获取的数据进行汇总清洗之后而形成的大语言模型预训练语料。总量大概在 10B Token。\n",
    "\n",
    "- BelleGroup：350万条中文对话数据集，包含了人机对话、人人对话、人物对话等多种对话数据，可以用于训练对话生成模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a537902",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# 下载预训练数据集\n",
    "os.system(\"modelscope download --dataset ddzhu123/seq-monkey mobvoi_seq_monkey_general_open_corpus.jsonl.tar.bz2 --local_dir your_local_dir\")\n",
    "# 解压预训练数据集\n",
    "os.system(\"tar -xvf your_local_dir/mobvoi_seq_monkey_general_open_corpus.jsonl.tar.bz2\")\n",
    "\n",
    "# 下载SFT数据集\n",
    "os.system(f'huggingface-cli download --repo-type dataset --resume-download BelleGroup/train_3.5M_CN --local-dir BelleGroup')\n",
    "\n",
    "\n",
    "\n",
    "# 1 处理预训练数据\n",
    "def split_text(text, chunk_size=512):\n",
    "    \"\"\"将文本按指定长度切分成块\"\"\"\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "input_file = 'mobvoi_seq_monkey_general_open_corpus.jsonl'\n",
    "\n",
    "with open('seq_monkey_datawhale.jsonl', 'a', encoding='utf-8') as pretrain:\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for line in tqdm(data, desc=f\"Processing lines in {input_file}\", leave=False):  # 添加行级别的进度条\n",
    "            line = json.loads(line)\n",
    "            text = line['text']\n",
    "            chunks = split_text(text)\n",
    "            for chunk in chunks:\n",
    "                pretrain.write(json.dumps({'text': chunk}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 2 处理SFT数据\n",
    "\n",
    "def convert_message(data):\n",
    "    \"\"\"\n",
    "    将原始数据转换为标准格式\n",
    "    \"\"\"\n",
    "    message = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个AI助手\"},\n",
    "    ]\n",
    "    for item in data:\n",
    "        if item['from'] == 'human':\n",
    "            message.append({'role': 'user', 'content': item['value']})\n",
    "        elif item['from'] == 'assistant':\n",
    "            message.append({'role': 'assistant', 'content': item['value']})\n",
    "    return message\n",
    "\n",
    "with open('BelleGroup_sft.jsonl', 'a', encoding='utf-8') as sft:\n",
    "    with open('BelleGroup/train_3.5M_CN.json', 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for item in tqdm(data, desc=\"Processing\", unit=\"lines\"):\n",
    "            item = json.loads(item)\n",
    "            message = convert_message(item['conversations'])\n",
    "            sft.write(json.dumps(message, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0755aa",
   "metadata": {},
   "source": [
    "\n",
    "### 5.3.2 训练 Tokenizer\n",
    "\n",
    "首先，我们需要为文本处理训练一个Tokenizer。Tokenizer的作用是将文本转换为数字序列，以便模型能够理解和处理。我们使用的数据集是 [出门问问序列猴子开源数据集](https://www.modelscope.cn/datasets/ddzhu123/seq-monkey/files) ，这个数据集包含了大量的中文文本数据，可以用于训练Tokenizer。\n",
    "\n",
    "> 注：由于数据集较大，如果大家在自己本地电脑训练的话进度比较慢，所以在这里我们提供了一个已经训练好的Tokenizer，大家可以直接使用。如果大家想要自己训练的话，可以参考下面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e77a5",
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "python code/train_tokenizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654c17a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bba2362",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tokenizers.normalizers import NFKC\n",
    "from typing import Generator\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def read_texts_from_jsonl(file_path: str) -> Generator[str, None, None]:\n",
    "    \"\"\"读取JSONL文件并安全提取文本数据\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if 'text' not in data:\n",
    "                    raise KeyError(f\"Missing 'text' field in line {line_num}\")\n",
    "                yield data['text']\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Error decoding JSON in line {line_num}\")\n",
    "                continue\n",
    "            except KeyError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "def create_tokenizer_config(save_dir: str) -> None:\n",
    "    \"\"\"创建完整的tokenizer配置文件\"\"\"\n",
    "    config = {\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": True,\n",
    "        \"bos_token\": \"<|im_start|>\",\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        \"pad_token\": \"<|im_end|>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"model_max_length\": 1000000000000000019884624838656,\n",
    "        \"clean_up_tokenization_spaces\": False,\n",
    "        \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "        \"chat_template\": (\n",
    "            \"{% for message in messages %}\"\n",
    "            \"{% if message['role'] == 'system' %}\"\n",
    "            \"<|im_start|>system\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% elif message['role'] == 'user' %}\"\n",
    "            \"<|im_start|>user\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% elif message['role'] == 'assistant' %}\"\n",
    "            \"<|im_start|>assistant\\n{{ message['content'] }}<|im_end|>\\n\"\n",
    "            \"{% endif %}\"\n",
    "            \"{% endfor %}\"\n",
    "            \"{% if add_generation_prompt %}\"\n",
    "            \"{{ '<|im_start|>assistant\\n' }}\"\n",
    "            \"{% endif %}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # 保存主配置文件\n",
    "    with open(os.path.join(save_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    # 创建special_tokens_map.json\n",
    "    special_tokens_map = {\n",
    "        \"bos_token\": \"<|im_start|>\",\n",
    "        \"eos_token\": \"<|im_end|>\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"pad_token\": \"<|im_end|>\",\n",
    "        \"additional_special_tokens\": [\"<s>\", \"</s>\"]\n",
    "    }\n",
    "    with open(os.path.join(save_dir, \"special_tokens_map.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(special_tokens_map, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def train_tokenizer(data_path: str, save_dir: str, vocab_size: int = 8192) -> None:\n",
    "    \"\"\"训练并保存自定义tokenizer\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 初始化tokenizer\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "    tokenizer.normalizer = NFKC()  # 添加文本规范化\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # 配置特殊token\n",
    "    special_tokens = [\n",
    "        \"<unk>\", \n",
    "        \"<s>\", \n",
    "        \"</s>\", \n",
    "        \"<|im_start|>\", \n",
    "        \"<|im_end|>\"\n",
    "    ]\n",
    "\n",
    "    # 配置训练器\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens,\n",
    "        min_frequency=2,  # 提高低频词过滤\n",
    "        show_progress=True,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    "    )\n",
    "\n",
    "    # 训练tokenizer\n",
    "    print(f\"Training tokenizer with data from {data_path}\")\n",
    "    texts = read_texts_from_jsonl(data_path)\n",
    "    tokenizer.train_from_iterator(texts, trainer=trainer, length=os.path.getsize(data_path))\n",
    "\n",
    "    # 验证特殊token映射\n",
    "    try:\n",
    "        assert tokenizer.token_to_id(\"<unk>\") == 0\n",
    "        assert tokenizer.token_to_id(\"<s>\") == 1\n",
    "        assert tokenizer.token_to_id(\"</s>\") == 2\n",
    "        assert tokenizer.token_to_id(\"<|im_start|>\") == 3\n",
    "        assert tokenizer.token_to_id(\"<|im_end|>\") == 4\n",
    "    except AssertionError as e:\n",
    "        print(\"Special tokens mapping error:\", e)\n",
    "        raise\n",
    "\n",
    "    # 保存tokenizer文件\n",
    "    tokenizer.save(os.path.join(save_dir, \"tokenizer.json\"))\n",
    "    \n",
    "    # 创建配置文件\n",
    "    create_tokenizer_config(save_dir)\n",
    "    print(f\"Tokenizer saved to {save_dir}\")\n",
    "\n",
    "def eval_tokenizer(tokenizer_path: str) -> None:\n",
    "    \"\"\"评估tokenizer功能\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tokenizer: {e}\")\n",
    "        return\n",
    "\n",
    "    # 测试基本属性\n",
    "    print(\"\\n=== Tokenizer基本信息 ===\")\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "    print(f\"Special tokens: {tokenizer.all_special_tokens}\")\n",
    "    print(f\"Special token IDs: {tokenizer.all_special_ids}\")\n",
    "\n",
    "    # 测试聊天模板\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个AI助手。\"},\n",
    "        {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'm fine, thank you. and you?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I'm good too.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"That's great to hear!\"},\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== 聊天模板测试 ===\")\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        # add_generation_prompt=True\n",
    "    )\n",
    "    print(\"Generated prompt:\\n\", prompt, sep=\"\")\n",
    "\n",
    "    # 测试编码解码\n",
    "    print(\"\\n=== 编码解码测试 ===\")\n",
    "    encoded = tokenizer(prompt, truncation=True, max_length=256)\n",
    "    decoded = tokenizer.decode(encoded[\"input_ids\"], skip_special_tokens=False)\n",
    "    print(\"Decoded text matches original:\", decoded == prompt)\n",
    "\n",
    "    # 测试特殊token处理\n",
    "    print(\"\\n=== 特殊token处理 ===\")\n",
    "    test_text = \"<|im_start|>user\\nHello<|im_end|>\"\n",
    "    encoded = tokenizer(test_text).input_ids\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"Original: {test_text}\")\n",
    "    print(f\"Decoded:  {decoded}\")\n",
    "    print(\"Special tokens preserved:\", decoded == test_text)\n",
    "\n",
    "def main():\n",
    "    # 配置路径\n",
    "    data_path = \"your data path\"\n",
    "    save_dir = \"tokenizer_k\"\n",
    "\n",
    "    # 训练tokenizer\n",
    "    train_tokenizer(\n",
    "        data_path=data_path,\n",
    "        save_dir=save_dir,\n",
    "        vocab_size=6144\n",
    "    )\n",
    "\n",
    "    # 评估tokenizer\n",
    "    eval_tokenizer(save_dir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a184583",
   "metadata": {},
   "source": [
    "\n",
    "训练完成之后可以可以使用 `eval_tokenizer()` 测试 Tokenizer 的功能，确保 Tokenizer 正常工作。在这个函数中，我们首先加载训练好的 Tokenizer，然后测试了 Tokenizer 的基本属性、聊天模板、编码解码等功能。这些测试可以帮助我们验证 Tokenizer 的正确性，确保它能够正常工作。正确的输出为：\n",
    "\n",
    "OUT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "=== Tokenizer基本信息 ===\n",
    "Vocab size: 6144\n",
    "Special tokens: ['<|im_start|>', '<|im_end|>', '<unk>', '<s>', '</s>']\n",
    "Special token IDs: [3, 4, 0, 1, 2]\n",
    "\n",
    "=== 聊天模板测试 ===\n",
    "Generated prompt:\n",
    "<|im_start|>system\n",
    "你是一个AI助手。<|im_end|>\n",
    "<|im_start|>user\n",
    "How are you?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "I'm fine, thank you. and you?<|im_end|>\n",
    "<|im_start|>user\n",
    "I'm good too.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "That's great to hear!<|im_end|>\n",
    "\n",
    "\n",
    "=== 编码解码测试 ===\n",
    "Decoded text matches original: False\n",
    "\n",
    "=== 特殊token处理 ===\n",
    "Original: <|im_start|>user\n",
    "Hello<|im_end|>\n",
    "Decoded:  <|im_start|> user\n",
    "Hello<|im_end|>\n",
    "Special tokens preserved: False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3804625",
   "metadata": {},
   "source": [
    "\n",
    "### 5.3.3 Dataset\n",
    "\n",
    "#### PretrainDataset\n",
    "\n",
    "在将数据送入到模型之前，我们还需要进行一些处理用于将文本数据转化为模型能够理解的Token。在这里我们使用的是Pytorch的Dataset类，用于加载数据集。我们定义了一个`PretrainDataset`类，用于加载已预处理好的数据集。我们继承了`torch.utils.data.IterableDataset`来定义该数据集，这使得我们可以更灵活、高效地处理数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9bf09",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=512):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.padding = 0\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = f.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        sample = json.loads(self.data[index])\n",
    "        text = f\"{self.tokenizer.bos_token}{sample['text']}\"\n",
    "        input_id = self.tokenizer(text).data['input_ids'][:self.max_length]\n",
    "        text_len = len(input_id)\n",
    "        # 没满最大长度的剩余部分\n",
    "        padding_len = self.max_length - text_len\n",
    "        input_id = input_id + [self.padding] * padding_len\n",
    "        # 0表示不计算损失\n",
    "        loss_mask = [1] * text_len + [0] * padding_len\n",
    "\n",
    "        input_id = np.array(input_id)\n",
    "        X = np.array(input_id[:-1]).astype(np.int64)\n",
    "        Y = np.array(input_id[1:]).astype(np.int64)\n",
    "        loss_mask = np.array(loss_mask[1:]).astype(np.int64)\n",
    "        return torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(loss_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b23456",
   "metadata": {},
   "source": [
    "\n",
    "在以上代码和图5.3可以看出，`Pretrain Dataset` 主要是将 `text` 通过 `tokenizer` 转换成 `input_id`，然后将 `input_id` 拆分成 `X` 和 `Y`，其中 `X` 为 `input_id` 的前 n-1 个元素，`Y` 为 `input_id` 的后 n-1 `个元素。loss_mask` 主要是用来标记哪些位置需要计算损失，哪些位置不需要计算损失。\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/5-images/pretrain_dataset.png\" alt=\"alt text\" width=\"100%\">\n",
    "    <p>图5.3 预训练损失函数计算</p>\n",
    "</div>\n",
    "\n",
    "图中示例展示了当`max_length=9`时的处理过程：\n",
    "- **输入序列**：`[BOS, T1, T2, T3, T4, T5, T6, T7, EOS]`\n",
    "- **样本拆分**：\n",
    "  - X：`[BOS, T1, T2, T3, T4, T5, T6, T7]` → 模型输入上下文\n",
    "  - Y：`[T1, T2, T3, T4, T5, T6, T7, EOS]` → 模型预测目标\n",
    "- **损失掩码**：\n",
    "  - 有效位置：`[0, 1, 1, 1, 1, 1, 1, 1, 1]` → 仅对T1-EOS计算损失\n",
    "\n",
    "#### SFTDataset\n",
    "\n",
    "`SFTDataset` 其实是一个多轮对话数据集，我们的目标是让模型学会如何进行多轮对话。在这个阶段我们的输入是上一轮的对话内容，输出是当前轮的对话内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74563a9b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, max_length=512):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.padding = 0\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = f.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def generate_loss_mask(self, input_ids):\n",
    "        # 生成 loss mask, 0 表示不计算损失, 1 表示计算损失\n",
    "        mask = [0] * len(input_ids)\n",
    "        a_sequence = [3, 1074, 537, 500, 203]  # <|im_start|>assistant\\n\n",
    "        a_length = len(a_sequence)\n",
    "        n = len(input_ids)\n",
    "        i = 0\n",
    "        \n",
    "        while i <= n - a_length:\n",
    "            # 检查当前位置是否匹配目标子序列\n",
    "            match = True\n",
    "            for k in range(a_length):\n",
    "                if input_ids[i + k] != a_sequence[k]:\n",
    "                    match = False\n",
    "                    break\n",
    "            if match:\n",
    "                # 从子序列结束的位置开始查找第一个4, 4 为 <|im_end|> EOS id\n",
    "                j = None\n",
    "                for idx in range(i + a_length, n):\n",
    "                    if input_ids[idx] == 4:\n",
    "                        j = idx\n",
    "                        break\n",
    "                if j is not None:\n",
    "                    start = i + a_length\n",
    "                    end = j  # 结束位置设为j（包含4）\n",
    "                    # 标记区间为1（包括start到end）\n",
    "                    if start <= end:\n",
    "                        for pos in range(start, end + 1):\n",
    "                            if pos < len(mask):\n",
    "                                mask[pos] = 1\n",
    "                # 跳过当前子序列，避免重叠匹配\n",
    "                i += a_length\n",
    "            else:\n",
    "                i += 1\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        sample = json.loads(self.data[index])\n",
    "        text = self.tokenizer.apply_chat_template(sample, tokenize=False, add_generation_prompt=False)\n",
    "        input_id = self.tokenizer(text).data['input_ids'][:self.max_length]\n",
    "        text_len = len(input_id)\n",
    "        # 没满最大长度的剩余部分\n",
    "        padding_len = self.max_length - text_len\n",
    "        input_id = input_id + [self.padding] * padding_len\n",
    "        # 0表示不计算损失\n",
    "        loss_mask = self.generate_loss_mask(input_id)\n",
    "\n",
    "        input_id = np.array(input_id)\n",
    "        X = np.array(input_id[:-1]).astype(np.int64)\n",
    "        Y = np.array(input_id[1:]).astype(np.int64)\n",
    "        loss_mask = np.array(loss_mask[1:]).astype(np.int64)\n",
    "        return torch.from_numpy(X), torch.from_numpy(Y), torch.from_numpy(loss_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b74615",
   "metadata": {},
   "source": [
    "\n",
    "在 SFT 阶段，这里使用的是多轮对话数据集，所以就需要区分哪些位置需要计算损失，哪些位置不需要计算损失。在上面的代码中，我使用了一个 `generate_loss_mask` 函数来生成 `loss_mask`。这个函数主要是用来生成 `loss_mask`，其中 `loss_mask` 的生成规则是：当遇到 `|<im_start|>assistant\\n` 时，就开始计算损失，直到遇到 `|<im_end|>` 为止。这样就可以保证我们的模型在 SFT 阶段只计算当前轮的对话内容，如图5.4所示。\n",
    "\n",
    "<div align='center'>\n",
    "    <img src=\"https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/5-images/sftdataset.png\" alt=\"alt text\" width=\"90%\">\n",
    "    <p>图5.4 SFT 损失函数计算</p>\n",
    "</div>\n",
    "\n",
    "可以看到，其实 SFT Dataset 和 Pretrain Dataset 的 `X` 和 `Y` 是一样的，只是在 SFT Dataset 中我们需要生成一个 `loss_mask` 来标记哪些位置需要计算损失，哪些位置不需要计算损失。 图中 `Input ids` 中的蓝色小方格就是AI的回答，所以是需要模型学习的地方。所以在 `loss_mask` 中，蓝色小方格对应的位置是黄色，其他位置是灰色。在代码 `loss_mask` 中的 1 对应的位置计算损失，0 对应的位置不计算损失。\n",
    "\n",
    "\n",
    "### 5.3.4 预训练\n",
    "\n",
    "在数据预处理完成后，我们就可以开始训练模型了。我们使用的模型是一个和LLama2结构一样的 Decoder only Transformer模型，使用Pytorch实现。相关代码在`code/k_model.py`文件中。此处不再赘述，源码中有详细的中文注释，且我们在之前的文章中也有详细的介绍。\n",
    "\n",
    "在模型这一部分可以重点看一下生成式模型是如何实现生成token的，可以查看`k_model.py`文件中的`Transforerm`类中的`generate`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857684d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "    def generate(self, idx, stop_id=None, max_new_tokens=256, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        给定输入序列 idx（形状为 (bz,seq_len) 的长整型张量），通过多次生成新 token 来完成序列。\n",
    "        在 model.eval() 模式下运行。效率较低的采样版本，没有使用键k/v cache。\n",
    "        \"\"\"\n",
    "        index = idx.shape[1]\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 如果序列上下文过长，截断它到最大长度\n",
    "            idx_cond = idx if idx.size(1) <= self.args.max_seq_len else idx[:, -self.args.max_seq_len:]\n",
    "            \n",
    "            # 前向传播获取序列中最后一个位置的 logits\n",
    "            logits = self(idx_cond).logits\n",
    "            logits = logits[:, -1, :] # 只保留最后一个时间步的输出\n",
    "            \n",
    "            if temperature == 0.0:\n",
    "                # 选择最有可能的索引\n",
    "                _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "            else:\n",
    "                # 缩放 logits 并应用 softmax\n",
    "                logits = logits / temperature\n",
    "                if top_k is not None:\n",
    "                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "\n",
    "            if idx_next == stop_id:\n",
    "                break\n",
    "\n",
    "            # 将采样的索引添加到序列中并继续\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx[:, index:] # 只返回生成的token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26f9ba",
   "metadata": {},
   "source": [
    "\n",
    "在 `generate` 方法中，我们首先获取序列中最后一个位置的 `logits`，然后基于这些 `logits` 生成新的 `token`。接着，生成的新 `token` 会被添加到序列中，模型随后会继续生成下一个 `token`。通过这种迭代过程，我们能够生成完整的文本。\n",
    "\n",
    "接下来就是最重要的部分，训练模型!\n",
    "\n",
    "> 注：在使用下面代码进行模型训练时，需要指定 `--data_path` 参数为预处理好的数据集路径，例如 `--data_path seq_monkey_datawhale.jsonl`，也需要指定要用哪几张GPU进行训练，例如 `--gpus 0,1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b6d84e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def get_lr(it, all):\n",
    "    \"\"\"\n",
    "    计算当前迭代的学习率，使用余弦退火调度策略\n",
    "    \n",
    "    学习率调度策略：\n",
    "    1. Warmup阶段：学习率从0线性增长到目标学习率\n",
    "    2. 余弦退火阶段：学习率按余弦函数衰减到最小学习率\n",
    "    3. 超出训练步数后：保持最小学习率\n",
    "    \n",
    "    Args:\n",
    "        it (int): 当前迭代步数\n",
    "        all (int): 总迭代步数\n",
    "        \n",
    "    Returns:\n",
    "        float: 当前步数对应的学习率\n",
    "    \"\"\"\n",
    "    warmup_iters = args.warmup_iters  # 预热迭代次数\n",
    "    lr_decay_iters = all  # 学习率衰减的总迭代次数\n",
    "    min_lr = args.learning_rate / 10  # 最小学习率，为初始学习率的1/10\n",
    "\n",
    "    # Warmup阶段：线性增长\n",
    "    if it < warmup_iters:\n",
    "        return args.learning_rate * it / warmup_iters\n",
    "    \n",
    "    # 超出训练步数：保持最小学习率\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    \n",
    "    # 余弦退火阶段\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # 余弦系数\n",
    "    return min_lr + coeff * (args.learning_rate - min_lr)\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    \"\"\"\n",
    "    训练一个epoch的函数\n",
    "    \n",
    "    实现了完整的训练循环，包括：\n",
    "    1. 数据加载和设备转移\n",
    "    2. 动态学习率调整\n",
    "    3. 前向传播和损失计算\n",
    "    4. 梯度累积和反向传播\n",
    "    5. 梯度裁剪和优化器更新\n",
    "    6. 日志记录和模型保存\n",
    "    \n",
    "    Args:\n",
    "        epoch (int): 当前epoch编号\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # 记录开始时间\n",
    "    \n",
    "    # 遍历数据加载器中的每个batch\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        # 将数据转移到指定设备（GPU/CPU）\n",
    "        X = X.to(args.device)  # 输入序列\n",
    "        Y = Y.to(args.device)  # 目标序列\n",
    "        loss_mask = loss_mask.to(args.device)  # 损失掩码，用于忽略padding token\n",
    "\n",
    "        # 计算当前步骤的学习率\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n",
    "        # 更新优化器中所有参数组的学习率\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # 使用混合精度训练上下文\n",
    "        with ctx:\n",
    "            # 前向传播\n",
    "            out = model(X, Y)\n",
    "            # 计算损失并除以累积步数（用于梯度累积）\n",
    "            loss = out.last_loss / args.accumulation_steps\n",
    "            # 将loss_mask展平为一维\n",
    "            loss_mask = loss_mask.view(-1)\n",
    "            # 应用掩码计算有效损失（忽略padding位置）\n",
    "            loss = torch.sum(loss * loss_mask) / loss_mask.sum()\n",
    "\n",
    "        # 使用scaler进行混合精度的反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 每accumulation_steps步执行一次优化器更新\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            # 取消梯度缩放，准备梯度裁剪\n",
    "            scaler.unscale_(optimizer)\n",
    "            # 梯度裁剪，防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            # 执行优化器步骤\n",
    "            scaler.step(optimizer)\n",
    "            # 更新scaler的缩放因子\n",
    "            scaler.update()\n",
    "\n",
    "            # 清零梯度，set_to_none=True可以节省内存\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # 每log_interval步记录一次日志\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            # 打印训练进度信息\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min;'.format(\n",
    "                    epoch + 1,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * args.accumulation_steps,  # 恢复真实的loss值\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "            \n",
    "            # 如果启用SwanLab，记录训练指标\n",
    "            if args.use_swanlab:\n",
    "                swanlab.log({\n",
    "                    \"loss\": loss.item() * args.accumulation_steps,\n",
    "                    \"lr\": optimizer.param_groups[-1]['lr']\n",
    "                })\n",
    "\n",
    "        # 每save_interval步保存一次模型\n",
    "        if (step + 1) % args.save_interval == 0:\n",
    "            model.eval()  # 切换到评估模式\n",
    "            # 构建检查点文件名\n",
    "            ckp = f'{args.save_dir}/pretrain_{lm_config.dim}_{lm_config.n_layers}_{lm_config.vocab_size}.pth'\n",
    "\n",
    "            # 处理多卡保存：如果是DataParallel模型，需要访问.module属性\n",
    "            state_dict = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()  # 切换回训练模式\n",
    "        \n",
    "        # 每20000步保存一个带步数标记的检查点\n",
    "        if (step + 1) % 20000 == 0:\n",
    "            model.eval()\n",
    "            # 构建带步数的检查点文件名\n",
    "            ckp = f'{args.save_dir}/pretrain_{lm_config.dim}_{lm_config.n_layers}_{lm_config.vocab_size}_step{step+1}.pth'\n",
    "\n",
    "            # 保存模型状态字典\n",
    "            state_dict = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    \"\"\"\n",
    "    初始化模型和分词器\n",
    "    \n",
    "    功能包括：\n",
    "    1. 加载预训练的分词器\n",
    "    2. 创建Transformer模型\n",
    "    3. 设置多GPU并行训练（如果可用）\n",
    "    4. 将模型移动到指定设备\n",
    "    5. 统计并打印模型参数量\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, tokenizer) 初始化后的模型和分词器\n",
    "    \"\"\"\n",
    "    def count_parameters(model):\n",
    "        \"\"\"\n",
    "        统计模型中可训练参数的数量\n",
    "        \n",
    "        Args:\n",
    "            model: PyTorch模型\n",
    "            \n",
    "        Returns:\n",
    "            int: 可训练参数总数\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # 从本地路径加载预训练的分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./tokenizer_k/')\n",
    "\n",
    "    # 根据配置创建Transformer模型\n",
    "    model = Transformer(lm_config)\n",
    "    \n",
    "    # 多卡初始化：检查可用GPU数量并设置DataParallel\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 1:\n",
    "        Logger(f\"Using {num_gpus} GPUs with DataParallel!\")\n",
    "        # 使用DataParallel包装模型以支持多GPU训练\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    # 将模型移动到指定设备（GPU或CPU）\n",
    "    model = model.to(args.device)\n",
    "    \n",
    "    # 计算并打印模型参数量（以百万为单位）\n",
    "    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ==================== 命令行参数解析 ====================\n",
    "    parser = argparse.ArgumentParser(description=\"Tiny-LLM Pretraining\")\n",
    "    \n",
    "    # 基础训练参数\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"base_model_215M\", help=\"模型输出目录\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, help=\"训练轮数\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"批次大小\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"学习率\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\", help=\"训练设备\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"数据类型\")\n",
    "    \n",
    "    # 实验跟踪和数据加载参数\n",
    "    parser.add_argument(\"--use_swanlab\", action=\"store_true\", help=\"是否使用SwanLab进行实验跟踪\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8, help=\"数据加载的工作进程数\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"./seq_monkey_datawhale.jsonl\", help=\"训练数据路径\")\n",
    "    \n",
    "    # 训练优化参数\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=8, help=\"梯度累积步数\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"梯度裁剪阈值\")\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"学习率预热迭代次数\")\n",
    "    \n",
    "    # 日志和保存参数\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=100, help=\"日志记录间隔\")\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=1000, help=\"模型保存间隔\")\n",
    "    \n",
    "    # 多GPU训练参数\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0,1,2,3,4,5,6,7', help=\"使用的GPU ID，用逗号分隔 (例如: '0,1,2')\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # ==================== GPU环境设置 ====================\n",
    "    # 设置可见的GPU设备\n",
    "    if args.gpus is not None:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
    "        # 自动设置主设备为第一个可用GPU\n",
    "        if torch.cuda.is_available():\n",
    "            args.device = \"cuda:0\"\n",
    "        else:\n",
    "            args.device = \"cpu\"\n",
    "\n",
    "    # ==================== 实验跟踪初始化 ====================\n",
    "    if args.use_swanlab:\n",
    "        # 注意：使用前需要先登录 swanlab.login(api_key='your key')\n",
    "        run = swanlab.init(\n",
    "            project=\"Happy-LLM\",  # 项目名称\n",
    "            experiment_name=\"Pretrain-215M\",  # 实验名称\n",
    "            config=args,  # 保存所有超参数\n",
    "        )\n",
    "\n",
    "    # ==================== 模型配置 ====================\n",
    "    # 定义语言模型的配置参数\n",
    "    lm_config = ModelConfig(\n",
    "        dim=1024,      # 模型维度\n",
    "        n_layers=18,   # Transformer层数\n",
    "    )\n",
    "\n",
    "    # ==================== 训练环境设置 ====================\n",
    "    max_seq_len = lm_config.max_seq_len  # 最大序列长度\n",
    "    args.save_dir = os.path.join(args.out_dir)  # 模型保存目录\n",
    "    \n",
    "    # 创建必要的目录\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    \n",
    "    # 设置随机种子以确保结果可复现\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 确定设备类型（用于选择合适的上下文管理器）\n",
    "    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "\n",
    "    # 设置混合精度训练的上下文管理器\n",
    "    # CPU训练时使用nullcontext，GPU训练时使用autocast\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "\n",
    "    # ==================== 模型和数据初始化 ====================\n",
    "    # 初始化模型和分词器\n",
    "    model, tokenizer = init_model()\n",
    "    \n",
    "    # 创建训练数据集\n",
    "    train_ds = PretrainDataset(args.data_path, tokenizer, max_length=max_seq_len)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,  # 批次大小\n",
    "        pin_memory=True,             # 将数据加载到固定内存中，加速GPU传输\n",
    "        drop_last=False,             # 不丢弃最后一个不完整的批次\n",
    "        shuffle=True,                # 随机打乱数据\n",
    "        num_workers=args.num_workers # 数据加载的并行工作进程数\n",
    "    )\n",
    "\n",
    "    # ==================== 优化器和训练组件初始化 ====================\n",
    "    # 初始化混合精度训练的梯度缩放器\n",
    "    # 只有在使用float16或bfloat16时才启用\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    \n",
    "    # 初始化Adam优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # ==================== 开始训练 ====================\n",
    "    # 计算每个epoch的迭代次数\n",
    "    iter_per_epoch = len(train_loader)\n",
    "    \n",
    "    # 开始训练循环\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506de3c1",
   "metadata": {},
   "source": [
    "\n",
    "### 5.3.5 SFT 训练\n",
    "\n",
    "SFT 训练和预训练的代码基本一样，只是导入的 Dataset 不一样。在这里我们使用的是 SFTDataset，用于多轮对话的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ff881",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import argparse\n",
    "import time\n",
    "import warnings\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from contextlib import nullcontext\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from k_model import ModelConfig, Transformer\n",
    "from dataset import SFTDataset\n",
    "\n",
    "import swanlab\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def Logger(content):\n",
    "    \"\"\"日志记录器\"\"\"\n",
    "    print(content)\n",
    "\n",
    "def get_lr(it, all):\n",
    "    \"\"\"获取学习率\"\"\"\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    # 1) 预热迭代的线性预热\n",
    "    warmup_iters = args.warmup_iters\n",
    "    lr_decay_iters = all\n",
    "    min_lr = args.learning_rate / 10\n",
    "\n",
    "    if it < warmup_iters:\n",
    "        return args.learning_rate * it / warmup_iters\n",
    "    \n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    # 2) 如果迭代次数超过学习率衰减迭代次数，则返回最小学习率\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    \n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    # 3) 在两者之间，使用余弦衰减至最小学习率\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (args.learning_rate - min_lr)\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    \"\"\"训练一个epoch\"\"\"\n",
    "    start_time = time.time()\n",
    "    for step, (X, Y, loss_mask) in enumerate(train_loader):\n",
    "        X = X.to(args.device)\n",
    "        Y = Y.to(args.device)\n",
    "        loss_mask = loss_mask.to(args.device)\n",
    "\n",
    "        # 获取学习率并更新优化器\n",
    "        lr = get_lr(epoch * iter_per_epoch + step, args.epochs * iter_per_epoch)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        # 前向传播\n",
    "        with ctx:\n",
    "            out = model(X, Y)\n",
    "            loss = out.last_loss / args.accumulation_steps\n",
    "            loss_mask = loss_mask.view(-1)\n",
    "            loss = torch.sum(loss * loss_mask) / loss_mask.sum()\n",
    "\n",
    "        # 反向传播\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # 更新权重\n",
    "        if (step + 1) % args.accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # 打印日志\n",
    "        if step % args.log_interval == 0:\n",
    "            spend_time = time.time() - start_time\n",
    "            Logger(\n",
    "                'Epoch:[{}/{}]({}/{}) loss:{:.3f} lr:{:.7f} epoch_Time:{}min:'.format(\n",
    "                    epoch + 1,\n",
    "                    args.epochs,\n",
    "                    step,\n",
    "                    iter_per_epoch,\n",
    "                    loss.item() * args.accumulation_steps,\n",
    "                    optimizer.param_groups[-1]['lr'],\n",
    "                    spend_time / (step + 1) * iter_per_epoch // 60 - spend_time // 60))\n",
    "            if args.use_swanlab:\n",
    "                swanlab.log({\n",
    "                    \"loss\": loss.item() * args.accumulation_steps,\n",
    "                    \"lr\": optimizer.param_groups[-1]['lr']\n",
    "                })\n",
    "\n",
    "        # 保存模型\n",
    "        if (step + 1) % args.save_interval == 0:\n",
    "            model.eval()\n",
    "            ckp = f'{args.save_dir}/sft_dim{lm_config.dim}_layers{lm_config.n_layers}_vocab_size{lm_config.vocab_size}.pth'\n",
    "\n",
    "            # 处理多卡保存\n",
    "            state_dict = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()\n",
    "        \n",
    "        # 定期保存模型\n",
    "        if (step + 1) % 20000 == 0:\n",
    "            model.eval()\n",
    "            ckp = f'{args.save_dir}/sft_dim{lm_config.dim}_layers{lm_config.n_layers}_vocab_size{lm_config.vocab_size}_step{step+1}.pth'\n",
    "\n",
    "            state_dict = model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict()\n",
    "            torch.save(state_dict, ckp)\n",
    "            model.train()\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    \"\"\"初始化模型\"\"\"\n",
    "    def count_parameters(model):\n",
    "        \"\"\"计算模型参数量\"\"\"\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # 加载分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained('./tokenizer_k/')\n",
    "\n",
    "    # 初始化模型\n",
    "    model = Transformer(lm_config)\n",
    "\n",
    "    # 加载预训练权重\n",
    "    ckp = './base_model_215M/pretrain_1024_18_6144.pth'\n",
    "    state_dict = torch.load(ckp, map_location=args.device)\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    # 多卡初始化\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    if num_gpus > 1:\n",
    "        Logger(f\"Using {num_gpus} GPUs with DataParallel!\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    model = model.to(args.device)\n",
    "    Logger(f'LLM总参数量：{count_parameters(model) / 1e6:.3f} 百万')\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Tiny-LLM Pretraining\")\n",
    "    parser.add_argument(\"--out_dir\", type=str, default=\"sft_model_215M\", help=\"输出目录\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1, help=\"训练轮数\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"批处理大小\")\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4, help=\"学习率\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:0\" if torch.cuda.is_available() else \"cpu\", help=\"使用的设备\")\n",
    "    parser.add_argument(\"--dtype\", type=str, default=\"bfloat16\", help=\"数据类型\")\n",
    "    parser.add_argument(\"--use_swanlab\", action=\"store_true\", help=\"是否使用SwanLab进行实验跟踪\")\n",
    "    parser.add_argument(\"--num_workers\", type=int, default=8, help=\"数据加载的工作进程数\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"./BelleGroup_sft.jsonl\", help=\"训练数据路径\")\n",
    "    parser.add_argument(\"--accumulation_steps\", type=int, default=8, help=\"梯度累积步数\")\n",
    "    parser.add_argument(\"--grad_clip\", type=float, default=1.0, help=\"梯度裁剪阈值\")\n",
    "    parser.add_argument(\"--warmup_iters\", type=int, default=0, help=\"预热迭代次数\")\n",
    "    parser.add_argument(\"--log_interval\", type=int, default=100, help=\"日志记录间隔\")\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=1000, help=\"模型保存间隔\")\n",
    "    # 添加多卡参数\n",
    "    parser.add_argument(\"--gpus\", type=str, default='0,1,2,3,4,5,6,7', help=\"逗号分隔的GPU ID (例如 '0,1,2')\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # 设置可见GPU\n",
    "    if args.gpus is not None:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpus\n",
    "        # 自动设置主设备为第一个GPU\n",
    "        if torch.cuda.is_available():\n",
    "            args.device = \"cuda:0\"\n",
    "        else:\n",
    "            args.device = \"cpu\"\n",
    "\n",
    "    # 初始化swanlab\n",
    "    if args.use_swanlab:\n",
    "        run = swanlab.init(\n",
    "            project=\"Happy-LLM\",\n",
    "            experiment_name=\"SFT-215M\",\n",
    "            config=args,\n",
    "        )\n",
    "\n",
    "    # 模型配置\n",
    "    lm_config = ModelConfig(\n",
    "        dim=1024,\n",
    "        n_layers=18,\n",
    "    )\n",
    "    max_seq_len = lm_config.max_seq_len\n",
    "    args.save_dir = os.path.join(args.out_dir)\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    torch.manual_seed(42)\n",
    "    device_type = \"cuda\" if \"cuda\" in args.device else \"cpu\"\n",
    "\n",
    "    # 上下文管理器\n",
    "    ctx = nullcontext() if device_type == \"cpu\" else torch.cuda.amp.autocast()\n",
    "\n",
    "    # 初始化模型和分词器\n",
    "    model, tokenizer = init_model()\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    train_ds = SFTDataset(args.data_path, tokenizer, max_length=max_seq_len)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers\n",
    "    )\n",
    "\n",
    "    # 缩放器和优化器\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype in ['float16', 'bfloat16']))\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    # 开始训练\n",
    "    iter_per_epoch = len(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        train_epoch(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d6082",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 5.3.6 使用模型生成文本\n",
    "\n",
    "在模型训练完成后，会在`output`目录下生成模型文件，这个文件就是我们训练好的模型。我们可以使用以下命令生成文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5ba27",
   "metadata": {
    "language": "bash"
   },
   "outputs": [],
   "source": [
    "python model_sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ac7b9",
   "metadata": {},
   "source": [
    "\n",
    "我们来看下`model_sample.py`文件中的代码，这个文件中定义了一个`TextGenerator`类，用于生成文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e418ff01",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "from k_model import ModelConfig, Transformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import argparse\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, \n",
    "                 checkpoint='./base_model_215M/pretrain_1024_18_6144.pth',  # 模型检查点路径\n",
    "                 tokenizer_model_path='./tokenizer_k/',  # 分词器模型路径\n",
    "                 seed=42,  # 随机种子，确保可重复性\n",
    "                 device=None,  # 设备，优先使用 CUDA，如果没有可用的 CUDA，则使用 CPU\n",
    "                 dtype=\"bfloat16\"):  # 数据类型，默认为 float32，可以选择 float16 或 bfloat16\n",
    "        \"\"\"\n",
    "        初始化 TextGenerator 类，加载模型、设置设备和分词器等。\n",
    "        \"\"\"\n",
    "        # 模型加载配置\n",
    "        self.checkpoint = checkpoint  # 保存的模型检查点路径\n",
    "        self.tokenizer_model_path = tokenizer_model_path  # 分词器模型文件路径\n",
    "        self.seed = seed  # 随机数种子，用于生成的可重复性\n",
    "        self.device = device or ('cuda:0' if torch.cuda.is_available() else 'cpu')  # 根据硬件条件选择设备\n",
    "        self.dtype = dtype  # 模型的浮点数类型\n",
    "        self.device_type = 'cuda' if 'cuda' in self.device else 'cpu'  # 判断当前设备是否为 CUDA\n",
    "        \n",
    "        # 设置随机种子，确保生成的可重复性\n",
    "        torch.manual_seed(seed)  # 设置 CPU 随机种子\n",
    "        torch.cuda.manual_seed(seed)  # 设置 CUDA 随机种子\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True  # 允许 CUDA 使用 TF32 精度进行矩阵乘法运算\n",
    "        torch.backends.cudnn.allow_tf32 = True  # 允许 cuDNN 使用 TF32 精度加速\n",
    "        \n",
    "        # 根据 dtype 选择适当的自动混合精度上下文\n",
    "        ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[self.dtype]\n",
    "        self.ctx = nullcontext() if self.device_type == 'cpu' else torch.amp.autocast(device_type=self.device_type, dtype=ptdtype)\n",
    "        \n",
    "        # 加载模型检查点文件\n",
    "        checkpoint_dict = torch.load(self.checkpoint, map_location=self.device)  # 加载模型参数 # 初始化模型参数\n",
    "        self.model = Transformer(ModelConfig(dim=1024, n_layers=18))  # 实例化 Transformer 模型\n",
    "        sunwanted_prefix = '_orig_mod.'\n",
    "        for k, v in list(checkpoint_dict.items()):\n",
    "            if k.startswith(sunwanted_prefix):\n",
    "                checkpoint_dict[k[len(sunwanted_prefix):]] = checkpoint_dict.pop(k)\n",
    "        self.model.load_state_dict(checkpoint_dict, strict=False)\n",
    "        \n",
    "        # 计算模型参数量\n",
    "        num_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print(f\"Model has {num_params / 1e6:.3f} M parameters.\")\n",
    "        # 设置模型为评估模式（evaluation mode），防止训练模式下的 dropout 等操作影响结果\n",
    "        self.model.eval()\n",
    "        # 将模型放置到正确的设备上（GPU 或 CPU）\n",
    "        self.model.to(self.device)\n",
    "        # 初始化分词器\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_model_path)  # 根据指定的路径加载分词器\n",
    "\n",
    "    def chat_template(self, prompt):\n",
    "        message = [\n",
    "            {\"role\": \"system\", \"content\": \"你是一个AI助手，你的名字叫小明。\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        return self.tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    def sft_sample(self, \n",
    "               start=\"Hello!\",  # 生成文本的起始提示词，可以是任意字符串\n",
    "               num_samples=3,  # 生成样本的数量，默认生成 3 个样本\n",
    "               max_new_tokens=256,  # 每个样本生成的最大 token 数，默认最多生成 256 个 token\n",
    "               temperature=0.7,  # 控制生成的随机性，1.0 为标准，值越大越随机\n",
    "               top_k=300):  # 保留概率最高的 top_k 个 token，限制生成时的选择范围\n",
    "        \"\"\"\n",
    "        根据给定的起始文本生成样本。\n",
    "        \n",
    "        :param start: 生成文本的起始提示词\n",
    "        :param num_samples: 要生成的文本样本数\n",
    "        :param max_new_tokens: 每个样本生成的最大 token 数\n",
    "        :param temperature: 控制生成的随机性，值越小生成越确定，值越大生成越随机\n",
    "        :param top_k: 限制生成时选择的 token 范围\n",
    "        :return: 生成的文本样本列表\n",
    "        \"\"\"\n",
    "        start = self.chat_template(start)\n",
    "        # 将起始文本编码为 token id 序列\n",
    "        start_ids = self.tokenizer(start).data['input_ids']\n",
    "        # print('start_ids:', start_ids)\n",
    "        x = (torch.tensor(start_ids, dtype=torch.long, device=self.device)[None, ...])  # 将编码后的 token id 转为 PyTorch 张量\n",
    "        generated_texts = []  # 用于保存生成的文本样本\n",
    "        with torch.no_grad():  # 禁用梯度计算，提升效率\n",
    "            with self.ctx:  # 进入自动混合精度的上下文（如果是 GPU 并使用 float16 时）\n",
    "                for k in range(num_samples):  # 循环生成指定数量的样本\n",
    "                    y = self.model.generate(x, self.tokenizer.eos_token_id, max_new_tokens, temperature=temperature, top_k=top_k)  # 生成文本\n",
    "                    generated_texts.append(self.tokenizer.decode(y[0].tolist()))  # 解码生成的 token 序列为可读文本\n",
    "        return generated_texts  # 返回生成的文本样本\n",
    "\n",
    "\n",
    "    def pretrain_sample(self, \n",
    "               start=\"Hello!\",  # 生成文本的起始提示词，可以是任意字符串\n",
    "               num_samples=3,  # 生成样本的数量，默认生成 3 个样本\n",
    "               max_new_tokens=256,  # 每个样本生成的最大 token 数，默认最多生成 256 个 token\n",
    "               temperature=0.7,  # 控制生成的随机性，1.0 为标准，值越大越随机\n",
    "               top_k=300):  # 保留概率最高的 top_k 个 token，限制生成时的选择范围\n",
    "        \"\"\"\n",
    "        根据给定的起始文本生成样本。\n",
    "        \n",
    "        :param start: 生成文本的起始提示词\n",
    "        :param num_samples: 要生成的文本样本数\n",
    "        :param max_new_tokens: 每个样本生成的最大 token 数\n",
    "        :param temperature: 控制生成的随机性，值越小生成越确定，值越大生成越随机\n",
    "        :param top_k: 限制生成时选择的 token 范围\n",
    "        :return: 生成的文本样本列表\n",
    "        \"\"\"\n",
    "        # 如果 start 是以 'FILE:' 开头，表示从文件中读取起始文本\n",
    "        if start.startswith('FILE:'):\n",
    "            with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "                start = f.read()  # 读取文件内容作为起始文本\n",
    "        \n",
    "        # 将起始文本编码为 token id 序列\n",
    "        start_ids = self.tokenizer(start).data['input_ids']\n",
    "        # print('start_ids:', start_ids)\n",
    "        x = (torch.tensor(start_ids, dtype=torch.long, device=self.device)[None, ...])  # 将编码后的 token id 转为 PyTorch 张量\n",
    "        # print(x.shape)\n",
    "        generated_texts = []  # 用于保存生成的文本样本\n",
    "        with torch.no_grad():  # 禁用梯度计算，提升效率\n",
    "            with self.ctx:  # 进入自动混合精度的上下文（如果是 GPU 并使用 float16 时）\n",
    "                for k in range(num_samples):  # 循环生成指定数量的样本\n",
    "                    y = self.model.generate(x, max_new_tokens=max_new_tokens, temperature=temperature, top_k=top_k)  # 生成文本\n",
    "                    generated_texts.append(self.tokenizer.decode(y[0].tolist()))  # 解码生成的 token 序列为可读文本\n",
    "        \n",
    "        return generated_texts  # 返回生成的文本样本\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print(\"------------------- Pretrain Sample ------------------- \\n\")\n",
    "\n",
    "    pretrain_prompt_datas = [\n",
    "        '<|im_start|>北京大学是',\n",
    "        '<|im_start|>中国矿业大学（北京）地球科学与测绘工程学院',\n",
    "    ]\n",
    "\n",
    "    generator = TextGenerator(checkpoint='./base_model_215M/pretrain_1024_18_6144.pth')  # 初始化生成器\n",
    "    for i in range(len(pretrain_prompt_datas)):\n",
    "        samples = generator.pretrain_sample(start=pretrain_prompt_datas[i], num_samples=1, max_new_tokens=120, temperature=0.75)\n",
    "        print(f\"\\nSample {i+1}:\\n{pretrain_prompt_datas[i]}{samples[0]}\\n{'-'*20}\")  # 打印生成的样本并用分隔线分割\n",
    "\n",
    "    print(\"\\n ------------------- SFT Sample ------------------- \\n\")\n",
    "\n",
    "    sft_prompt_datas = [\n",
    "        '你好呀',\n",
    "        \"中国的首都是哪里？\",\n",
    "        \"1+12等于多少？\",\n",
    "        \"你是谁？\"\n",
    "    ]\n",
    "    generator = TextGenerator(checkpoint='./sft_model_215M/sft_dim1024_layers18_vocab_size6144.pth')  # 初始化生成器\n",
    "    for i in range(len(sft_prompt_datas)):\n",
    "        samples = generator.sft_sample(start=sft_prompt_datas[i], num_samples=1, max_new_tokens=128, temperature=0.6)\n",
    "        print(f\"\\nSample {i+1}:\\nQuestion: {sft_prompt_datas[i]} \\nAI answer: {samples[0]}\\n{'-'*20}\")  # 打印生成的样本并用分隔线分割"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228e93a",
   "metadata": {},
   "source": [
    "\n",
    "最后我们来看一下模型输出的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195effe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------- SFT Sample ------------------- \n",
    "\n",
    "Model has 215.127 M parameters.\n",
    "\n",
    "Sample 1:\n",
    "Question: 你好呀 \n",
    "AI answer: 你好!有什么我可以帮你的吗?\n",
    "--------------------\n",
    "\n",
    "Sample 2:\n",
    "Question: 中国的首都是哪里？ \n",
    "AI answer: 中国的首都是北京。\n",
    "--------------------\n",
    "\n",
    "Sample 3:\n",
    "Question: 1+1等于多少？ \n",
    "AI answer: 1+1等于2。\n",
    "--------------------\n",
    "------------------- Pretrain Sample ------------------- \n",
    "\n",
    "Model has 215.127 M parameters.\n",
    "\n",
    "Sample 1:\n",
    "<|im_start|>北京大学是中国最早建立的研究型大学之一,是我国最早设置研究生院的高校之一,是第一、二国教育委员会师资培训基地;北京大学是第一、二所国立大学,其校名与北京大学相同。\n",
    "北京大学录取标准:本科三批1万元,本科一批1万元,本科一批2000元,专科一批2000元,高中起点:非本科一批\n",
    "--------------------\n",
    "\n",
    "Sample 2:\n",
    "<|im_start|>中国矿业大学（北京）地球科学与测绘工程学院副教授黄河流域地质学科带头人古建平教授为大家介绍世界地质变化的概念及工作经验。\n",
    "古建平教授介绍了最近几年的植物学和地质学的基本概念,尤其是树都黄河、松涛、暗河等都有地质学工作者的身影,其中树都黄河以分布面积最大,是树都黄河中华砂岩公园的主景区。\n",
    "黄河内蒙古\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be9eef",
   "metadata": {},
   "source": [
    "\n",
    "到这里，我们的模型就训练完成了，恭喜你训练了一个属于你自己的大模型。\n",
    "\n",
    "> 大家在训练的时候可以将 batch 调的低一些，这样可以减少显存的占用，避免显存不足的问题。当然这样会增加训练时间，可以根据自己的显卡显存大小来调整 batch 的大小。实测 Pretrain batch 为 4 的情况下只需要 7G 显存，训练时长预计 533 小时。作者是在 8卡4090 上进行训练的，预训练一共耗时 46 小时，SFT 阶段在 BelleGroup 350万条中文指令训练 24 小时。\n",
    "\n",
    "作者也在魔搭平台上传了本章节训来的模型，如果大家的设备不足以训练大模型，大家也可以在魔搭平台下载模型进行调试和模型体验。模型下载地址如下：\n",
    "\n",
    "> *ModelScope 模型下载地址：[🤖 ModelScope](https://www.modelscope.cn/collections/Happy-LLM-e98b91b10b684a)*    \n",
    "> *ModelScope 创空间体验地址：[🤖 创空间](https://www.modelscope.cn/studios/kmno4zx/happy_llm_215M_sft)*\n",
    "\n",
    "\n",
    "**参考资料**\n",
    "\n",
    "[1] Andrej Karpathy. (2023). *llama2.c: Fullstack Llama 2 LLM solution in pure C*. GitHub repository. https://github.com/karpathy/llama2.c  \n",
    "\n",
    "[2] Andrej Karpathy. (2023). *llm.c: GPT-2/GPT-3 pretraining in C/CUDA*. GitHub repository. https://github.com/karpathy/llm.c  \n",
    "\n",
    "[3] Hugging Face. (2023). *Tokenizers documentation*. https://huggingface.co/docs/tokenizers/index  \n",
    "\n",
    "[4] Skywork Team. (2023). *SkyPile-150B: A large-scale bilingual dataset*. Hugging Face dataset. https://huggingface.co/datasets/Skywork/SkyPile-150B  \n",
    "\n",
    "[5] BelleGroup. (2022). *train_3.5M_CN: Chinese dialogue dataset*. Hugging Face dataset. https://huggingface.co/datasets/BelleGroup/train_3.5M_CN  \n",
    "\n",
    "[6] Jingyao Gong. (2023). *minimind: Minimalist LLM implementation*. GitHub repository. https://github.com/jingyaogong/minimind  \n",
    "\n",
    "[7] Mobvoi. (2023). *seq-monkey-data: Llama2 training/inference data*. GitHub repository. https://github.com/mobvoi/seq-monkey-data"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
